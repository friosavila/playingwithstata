[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Site",
    "section": "",
    "text": "Welcome, and thanks for joining me.\nAs you will read in this site, I will cover series of random topics and questions in applied econometrics using my favorite tool Stata!.\nThat being said. This website is multipurpose.\nI started it to answer random questions of “how to” do things in Stata, that I had troubles with at some point. Spread the knowledge if you may. However it will also be used as a teaching tool for some simple and not-so-simple topics and strategies I teach in econometrics class.\nFinally, I will also use it as a platform to share my research, whenever possible, and pertinent.\nFeel free to drop me a line, comment or question.\nPerhaps most interesting of all. This site was rewritten, again, using Quarto! combined with Stata!\nYou can find me at: friosa@gmail.com\nOr follow me on tweeter: @friosavila\nAnd just to start with. A mini Example of what its to come\n\n\nCode\nsysuse auto, clear\nset scheme white2\nscatter price mpg\n\n\n(1978 automobile data)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fernando Rios-Avila",
    "section": "Education",
    "text": "Education\n\n PhD in Economics, 2013\nGeorgia State University\n\n\n ASP in International Economics\nKiel University, Germany\n\n\n BS in Economics (Licenciatura)\nUniversidad Catolica Boliviana, La Paz, Bolivia"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Fernando Rios-Avila",
    "section": "Experience",
    "text": "Experience\n\n  Research Scholar, 2013 - Present\nLevy Economics Institute of Bard College\n\n\n Consultant, 2022 - Present\nThe World Bank"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Playing With Stata",
    "section": "",
    "text": "Fernando Rios-Avila\n1 Dewey Ave • Troy, NY, 12180 • (404) 924-5176 • friosa@gmail.com\n\n\nRESEARCH INTERESTS:\nLabor Economics, Applied Microeconomics, Applied Econometrics, Poverty and Inequality\n\n\n\nEDUCATION:\nPhD in Economics, Georgia State University, 2008-2013\nAdvanced Studies (M.Sc) International Economics, Kiel University - Germany, 2007-2008\nLicenciatura en Economia (5 yr program, BS equivalent), Universidad Católica Boliviana, 2000-2004\n\n\n\nPROFESSIONAL EXPERIENCE:\nLevy Economics Institute of Bard College, Research Scholar, August 2013 – Present\nWorld Bank, Consultant, 2022-Present\nFederal Reserve Bank of Atlanta, Research Assistant, August 2010 – June 2013\nGeorgia State University, Graduate Research Assistant, August 2008 – December 2013\nEconomic and Social Policy Analysis Unit (UDAPE), Consultant (UNICEF Project), July 2005 – June 2007\n\n\n\nTEACHING EXPERIENCE:\nLevy Economics Institute, Master on Economic Policy\n\nIntersecting Inequalities: Labor Market Institutions\nIntersecting Inequalities: Decompositions Methods in Economics\nEconometrics of Poverty, Inequality and Social Science Research\nApplied Econometric Methods for Empirical Research & Policy Evaluation\nResearch Methods: Econometrics I\n\n\n\n\nPUBLICATIONS AND RESEARCH:\n\nRefereed Publications\n\n\nAristizabal-Ramirez, Maria, Gustavo Canavire-Bacarreza, and Fernando Rios-Avila. 2015. “Revisiting the Effects of Innovation on Growth: A Threshold Analysis.” Applied Economics Letters 22 (18): 1474–79. https://doi.org/10.1080/13504851.2015.1039699.\n\n\nCanavire-Bacarreza, Gustavo, Alberto Chong, Fernando Ríos-Avila, and Mónica Yáñez-Pagans. 2020. “Will Elders Provide for Their Grandchildren? Unconditional Cash Transfers and Educational Expenditures in Bolivia.” Review of Development Economics 24 (2): 424–47. https://doi.org/10.1111/rode.12647.\n\n\nCanavire-Bacarreza, Gustavo, and Fernando Rios-Avila. 2017. “On the Determinants of Changes in Wage Inequality in Urban Bolivia.” Journal of Human Development and Capabilities 18 (4): 464–96. https://doi.org/10.1080/19452829.2017.1353350.\n\n\nHotchkiss, Julie L., Robert E. Moore, and Fernando Rios-Avila. 2012. “Assessing the Welfare Impact of Tax Reform: A Case Study of the 2001 U.s. Tax Cut.” Review of Income and Wealth 58 (2): 233–56. https://doi.org/10.1111/j.1475-4991.2012.00493.x.\n\n\n———. 2014. “Assessing the Welfare Impact of Tax Reform: A Case Study of the 2001 U.S. Tax Cut.” Review of Income and Wealth 60 (2): 404–4. https://doi.org/10.1111/roiw.12113.\n\n\n———. 2015. “Reevaluation of the Employment Impact of the 1996 Summer Olympic Games.” Southern Economic Journal 81 (3): 619–32. https://doi.org/10.4284/0038-4038-2013.063.\n\n\n———. 2020. “Cost of Policy Choices: A Microsimulation Analysis of the Impact on Family Welfare of Unemployment and Price Changes.” Journal of Macroeconomics 63 (March): 103167. https://doi.org/10.1016/j.jmacro.2019.103167.\n\n\nHotchkiss, Julie L., Robert E. Moore, Fernando Rios-Avila, and Melissa R. Trussell. 2017. “A Tale of Two Decades: Relative Intra-Family Earning Capacity and Changes in Family Welfare over Time.” Review of Economics of the Household 15 (3): 707–37. https://doi.org/10.1007/s11150-016-9354-9.\n\n\nHotchkiss, Julie L., Melinda M. Pitts, and Fernando Rios-Avila. 2014. “A Search for Evidence of Skill Mismatch in the Aftermath of the Great Recession.” Applied Economics Letters 21 (9): 587–92. https://doi.org/10.1080/13504851.2013.870645.\n\n\nHotchkiss, Julie L., Myriam Quispe-Agnoli, and Fernando Rios-Avila. 2015. “The Wage Impact of Undocumented Workers: Evidence from Administrative Data.” Southern Economic Journal 81 (4): 874–906. https://www.jstor.org/stable/26160583.\n\n\nHotchkiss, Julie L., and Fernando Rios-Avila. 2013. “Identifying Factors Behind the Decline in the U.S. Labor Force Participation Rate.” Business and Economic Research 3 (1): 257–75. https://doi.org/10.5296/ber.v3i1.3370.\n\n\nJenkins, Stephen P., and Fernando Rios-Avila. 2020. “Modelling Errors in Survey and Administrative Data on Employment Earnings: Sensitivity to the Fraction Assumed to Have Error-Free Earnings.” Economics Letters 192 (July): 109253. https://doi.org/10.1016/j.econlet.2020.109253.\n\n\n———. 2021. “Measurement Error in Earnings Data: Replication of Meijer, Rohwedder, and Wansbeek’s Mixture Model Approach to Combining Survey and Register Data.” Journal of Applied Econometrics 36 (4): 474–83. https://doi.org/10.1002/jae.2811.\n\n\nJenkins, Stephen P, and Fernando Rios-Avila. 2023. “Reconciling Reports: Modelling Employment Earnings and Measurement Errors Using Linked Survey and Administrative Data.” Journal of the Royal Statistical Society Series A: Statistics in Society 186 (1): 110–36. https://doi.org/10.1093/jrsssa/qnac003.\n\n\nMasterson, Thomas, Ajit Zacharias, Fernando Rios-Avila, and Edward N. Wolff. 2019. “The Great Recession and Racial Inequality: Evidence from Measures of Economic Well-Being.” Journal of Economic Issues 53 (4): 1048–69. https://doi.org/10.1080/00213624.2019.1664240.\n\n\nMercado, Alejandro F., Jorge Leitón, and Fernando Rios-Avila. 2004. “Segmentación En El Mercado Matrimonial.” Revista Latinoamericana de Desarrollo Económico, no. 3 (October): 49–76. http://www.scielo.org.bo/scielo.php?script=sci_abstract&pid=S2074-47062004000200003&lng=es&nrm=iso&tlng=es.\n\n\nMorrar, Rabeh, and Fernando Rios-Avila. 2020. “Discrimination Against Refugees in the Palestinian Labor Market.” International Journal of Manpower 42 (6): 1002–24. https://doi.org/10.1108/IJM-08-2019-0396.\n\n\nMundra, Kusum, and Fernando Rios-Avila. 2021. “Using Repeated Cross-Sectional Data to Examine the Role of Immigrant Birth-Country Networks on Unemployment Duration: An Application of Guell and Hu (2006) Approach.” Empirical Economics 61 (1): 389–415. https://doi.org/10.1007/s00181-020-01855-x.\n\n\nRioja, Felix, Fernando Rios-Avila, and Neven Valev. 2014a. “The Persistent Effect of Banking Crises on Investment and the Role of Financial Markets.” Journal of Financial Economic Policy 6 (1): 64–77. https://doi.org/10.1108/JFEP-08-2013-0035.\n\n\n———. 2014b. “Serial Banking Crises and Capital Investment.” Emerging Markets Finance and Trade 50 (6): 193–208. https://doi.org/10.1080/1540496X.2014.1013866.\n\n\n———. 2017. “Productivity During Recessions with Banking Crises: Inter-Industry Evidence.” Economics Letters 152 (March): 50–53. https://doi.org/10.1016/j.econlet.2016.12.028.\n\n\nRios-Avila, Fernando. 2015. “Feasible Fitting of Linear Models with N Fixed Effects.” The Stata Journal 15 (3): 881–98. https://doi.org/10.1177/1536867X1501500318.\n\n\n———. 2017. “Unions and Economic Performance in Developing Countries: Case Studies from Latin America.” Ecos de Economía 21 (44): 4–36. https://doi.org/10.17230/ecos.2017.44.1.\n\n\n———. 2019. “A Semi-Parametric Approach to the Oaxaca–Blinder Decomposition with Continuous Group Variable and Self-Selection.” Econometrics 7 (2): 28. https://doi.org/10.3390/econometrics7020028.\n\n\n———. 2020a. “Recentered Influence Functions (RIFs) in Stata: RIF Regression and RIF Decomposition.” The Stata Journal 20 (1): 51–94. https://doi.org/10.1177/1536867X20909690.\n\n\n———. 2020b. “Smooth Varying-Coefficient Models in Stata.” The Stata Journal 20 (3): 647–79. https://doi.org/10.1177/1536867X20953574.\n\n\n———. 2021. “Estimation of Marginal Effects for Models with Alternative Variable Transformations.” The Stata Journal 21 (1): 81–96. https://doi.org/10.1177/1536867X211000005.\n\n\nRios-Avila, Fernando, and Gustavo Canavire-Bacarreza. 2017. “The Effect of Intimate Partner Violence on Labor Market Decisions: Evidence from a Multi-Ethnic Country.” International Journal of Social Economics 44 (1): 75–92. https://doi.org/10.1108/IJSE-12-2014-0258.\n\n\n———. 2018. “Standard-Error Correction in Two-Stage Optimization Models: A Quasi–Maximum Likelihood Estimation Approach.” The Stata Journal 18 (1): 206–22. https://doi.org/10.1177/1536867X1801800113.\n\n\n———. 2020. “The Effect of Immigration on Labor Market Transitions of Native-Born Unemployed in the United States.” Journal of Labor Research 41 (3): 295–331. https://doi.org/10.1007/s12122-020-09304-5.\n\n\nRios-Avila, Fernando, and Barry T. Hirsch. 2014. “Unions, Wage Gaps, and Wage Dispersion: New Evidence from the Americas.” Industrial Relations: A Journal of Economy and Society 53 (1): 1–27. https://doi.org/10.1111/irel.12044.\n\n\nRios-Avila, Fernando, and Michelle Lee Maroto. 2022. “Moving Beyond Linear Regression: Implementing and Interpreting Quantile Regression Models With Fixed Effects.” Sociological Methods & Research, February, 00491241211036165. https://doi.org/10.1177/00491241211036165.\n\n\nSaravia, Antonio, Gustavo Canavire-Bacarreza, and Fernando Rios-Avila. 2017. “Intellectual Property Rights, Foreign Direct Investment and Economic Freedom.” Global Economy Journal 17 (2). https://doi.org/10.1515/gej-2017-0017.\n\n\nSearing, Elizabeth A. M., Fernando Rios-Avila, and Jesse D. Lecy. 2013. “The Impact of Psychological Trauma on Wages in Post-Conflict Bosnia and Herzegovina.” Social Science & Medicine 96 (November): 165–73. https://doi.org/10.1016/j.socscimed.2013.07.020.\n\n\n\n\nNon-refereed publications\nZacharias, A., T. Masterson, F. Rios-Avila, K. Kim, and T. Khitarishvili. 2018. “The Measurement of Time and Consumption Poverty in Ghana and Tanzania.” Research Project Report, August. Annandale-on-Hudson, NY: Levy Economics Institute of Bard College.\nRios Avila, F. (2016) “Losing Ground: Demographic Trends in US Labor Force Participation”. Policy Note 2015-7. Levy Economics Institute of Bard College\nRios Avila, F. (2015) “A Decade of Declining Wages: From Bad to Worse”. Policy Note 2015-3. Levy Economics Institute of Bard College\nRios Avila, F.& Hotchkiss, J. L. (2014) “A Decade of Flat Wages?” Policy Note 2014-4. Levy Economics Institute of Bard College.\nYañez Pagans, P. & Rios-Avila, F. (2009). Bolivia-Derechos e Igualdad de Genero en la Niñez y Adolescencia, La Paz-Bolivia: UDAPE-UNICEF.\nCzerniewicz-Kempff, N., Yañez Pagans, P., Rios-Avila, F., & Salinas Quiroga, B. (2008). Bolivia-Inversion Social en la Niñez y Adolescencia: Un Analisis del Gasto Publico Social en niveles Subnacionales, La Paz-Bolivia: UDAPE-UNICEF.\nMeave Idiaquez, E., Figueroa Cardenas, M., & Rios-Avila, F. (2008). Bolivia-El Gasto de los Hogares en Educacion, La Paz-Bolivia: UDAPE-UNICEF.\nLoayza, M.,Cuarite, R., Rios-Avila, F. (2006). Bolivia: Gasto Social Funcional y Gasto Social para la Niñez: 2000 – 2004, La Paz-Bolivia: UDAPE-UNICEF.\n\n\n\n\nTHESIS ADVISOR:\nMaría Aristizábal-Ramírez (Undergraduate Thesis)\nIan Bowen (Master Thesis)\nAlejandra Montoya (Master Thesis)\nDaniella Medina (Master Thesis)\nRhett Maiorama (Master Thesis)\nEsteban Rivera (Master Thesis)\n\n\n\nLANGUAGES AND SOFTWARE:\nStatistical software with programming experience: Stata, R, Eviews.\nLanguages: Spanish (Native), English (Fluent)"
  },
  {
    "objectID": "stata_do/stata_do1.html",
    "href": "stata_do/stata_do1.html",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "",
    "text": "This article is a simple example of how to use Quarto to create dynamic documents using Stata. Which is possible using the Stata kernel `nbstata’ thanks to Tim Huegerich, who put this incredible tool together.\nTo use this, you will need:\n\nStata 17\npython\nnbstata\n\nFollow Tim’s steps to install the kernel."
  },
  {
    "objectID": "stata_do/stata_do1.html#lets-do-this",
    "href": "stata_do/stata_do1.html#lets-do-this",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "Lets do this:",
    "text": "Lets do this:\nLet us consider an example where we study the mpg and weight variables in auto.dta. In this example, all code used to construct the desired output will be displayed as fenced block code, followed by the output it produces. This is done using the option *|echo: fenced.\nWe first use the sysuse command to load the dataset and then describe the data using the describe command.\n\n\nCode\n```{stata}\nsysuse auto, clear\ndescribe\n```\n\n\n(1978 automobile data)\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n\nNow, we want to check if mpg is always greater than 0 and less than 100. We use the assert command to perform the check. In this case, we do not want to include any output in the target HTML file, so we use the quietly attribute to modify the behavior of the dd_do Stata dynamic tag.\n\n\nCode\n```{stata}\n assert mpg > 0 & mpg < 100\n```\n\n\nIf the data do not satisfy the conditions, quatro will fail with an error message, which will occur if we run the same assert command in a do-file.\nNext, we want to summarize the weight variable:\n\n\nCode\n```{stata}\nsummarize weight\n```\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      weight |         74    3019.459    777.1936       1760       4840\n\n\nWe want to use the minimum and maximum values of weight in a sentence. Instead of copying and pasting the numbers from the summarize output, we can use the display Stata to show r(min) and r(max) stored results. We will also use the options *| output: asis to obtain text that follows markdown formatting.\n\nCode\n```{stata}\n*| output: asis\ndisplay \"The variable weight has minimum value \"  %4.2f `r(min)' \" and \" ///\n         \"has maximum value \"   %4.2f `r(max)' \".\"\n```\n\nThe variable weight has minimum value 1760.00 and has maximum value 4840.00.\nIn other words, if one wants to use dynamic tags, its possible to do so by simply using display, with the corresponding locals, so Stata evaluates the expressions as normal. *| output: asis is used so the output can still be interpreted using markdown syntax.\nAs with dyndoc, display can also be used as a calculator. For example, if we want to include the \\(range = max - min\\) in a sentence, instead of calculating the number and then copying and pasting it, we can use\n\nCode\n```{stata}\n*| output: asis\ndisplay \"The variable weight has range \"  %4.2f `r(max)'-`r(min)' \".\"\n```\n\nThe variable weight has range 3080.00.\nNow, we want to graph mpg and weight using a scatterplot. There are at least two ways to do this.\nFirst, one can simply create the scatterplot using the same procedure as before:\n\n\nCode\n```{stata}\nscatter mpg weight, mcolor(blue%50)\n```\n\n\n\n\n\nwhich generates a scatterplot of mpg and weight with 50% opacity color markers.\nNow, we want to export the graph to a file and include an image link to the file.\n\n\nCode\n```{stata}\nqui:graph export fig1.png, width(1600) replace\n```\n\n\nThis produces a graph of 1600 pixels width.\n\n\n\nscatter\n\n\nIt is possible, however, to combine figure creation using quatro tags and directives. Here, however, you need to make sure all figures are named:\n\n\nCode\n```{stata}\n*| label: fig-cost\n*| fig-cap: Price vs MPG\n*| fig-subcap:\n*|   - Foreign Cars\n*|   - Domestic Cars\n*| layout-ncol: 2\n*| column: page\n\nscatter price mpg if foreign==1, name(m1, replace) ylabel(0(4000)16000)\nqui:graph export fig2a.png, width(1600) replace\nscatter price mpg if foreign==0, name(m2, replace) ylabel(0(4000)16000)\nqui:graph export fig2b.png, width(1600) replace\n```\n\n\n\n\n\n\n\n\n(a) Foreign Cars\n\n\n\n\n\n\n\n(b) Domestic Cars\n\n\n\n\nFigure 1: Price vs MPG\n\n\n\nAnd of course, we can now the figure tags to link it to the text:\nFigure 1 provides a simple scatter between prices and MPG for foreign and domestic cars. While there seems to be a strong negative relationship between these variables among foreign cars (see Figure 1 (a)), the relationship among domestic cars is much weaker, when looking at cars with a fuel efficiency larger than 15mpg (see Figure 1 (b)).\nThe last approach, however, may not work with PDF format, or jupyter-notebook format, unless the figures are saved. But does seem to work with HTML and docx.\nNevertheless, one could also do the following:\n::: {#fig-mpgprice layout-ncol=2 .column-page }\n\n![foreign](fig2a.png){#fig-mpgprice-1}\n\n![domestic](fig2b.png){#fig-mpgprice-2}\n\nPrice vs MPG\n:::\nto produce\n\n\n\n\n\n\n\n(a) foreign\n\n\n\n\n\n\n\n(b) domestic\n\n\n\n\nFigure 2: Price vs MPG\n\n\nAs plot in Figure 2 provides a simple scatter between prices and MPG for foreign and domestic cars. While there seems to be a strong negative relationship between these variables among foreign cars (see Figure 2 (a)), the relationship among domestic cars is much weaker, when looking at cars with a fuel efficiency larger than 15mpg (see Figure 2 (b)).\nIf using VScode, to render all formats at once, you need to type quarto render filename.qmd in the terminal."
  },
  {
    "objectID": "stata_do/stata_do2.html",
    "href": "stata_do/stata_do2.html",
    "title": "How to Bootstrap",
    "section": "",
    "text": "One of the primary concerns of econometricians and economists is estimating point estimates with precision. However, as point estimates contain errors, we need to estimate the precision of those estimates to determine how reliable they are. This precision is typically expressed as standard errors, which reflect the level of uncertainty in the estimates given the information we have in hand, i.e., the sample.\nIn most introductory econometrics courses, we learn that we can estimate the precision of these estimates by drawing multiple samples, estimating the model of interest for each new sample, and summarizing the estimated coefficients. The standard deviations of the estimated coefficients are the coefficient standard errors, which reflect the variation in the coefficients due to sampling error.\nHowever, collecting multiple samples from the same population is technically impossible (expensive), and we need to rely on other approaches to estimate the standard errors. Two common approaches are typically used:\n\nAsymptotic approximations: where we make use of some of the properties of the estimators (deep knowledge of how those are constructed)\nEmpirical approximations: Or what we can call Bootstrapping."
  },
  {
    "objectID": "stata_do/stata_do2.html#but-what-is-bootstrapping",
    "href": "stata_do/stata_do2.html#but-what-is-bootstrapping",
    "title": "How to Bootstrap",
    "section": "But What is Bootstrapping?",
    "text": "But What is Bootstrapping?\nAs a non-native speaker, I initially thought that bootstrapping was merely a statistical technique for obtaining empirical standard errors. However, after a few years in grad school, I heard the expression:\n\npull yourself up by your own bootstraps\n\na few times, which describes what bootstrapping does. Since we don’t have access to other samples, we repeatedly use and reuse the same sample in various ways to estimate standard errors for our estimates.\nThe differences in how we reuse the sample information determine the type of bootstrapping method we’re using."
  },
  {
    "objectID": "stata_do/stata_do2.html#types-of-bootstrapping.",
    "href": "stata_do/stata_do2.html#types-of-bootstrapping.",
    "title": "How to Bootstrap",
    "section": "Types of Bootstrapping.",
    "text": "Types of Bootstrapping.\nThere are many approaches to obtaining bootstrap standard errors, depending on the assumptions we’re willing to impose on the data, and not all of them can be applied in every scenario. For simplicity, I’ll refer to the ones that can be used for linear regressions.\nSuppose you’re interested in a linear regression model with the following functional form:\n\\[\ny_i=X_i\\beta+e_i\n\\]"
  },
  {
    "objectID": "stata_do/stata_do2.html#setup-and-asymptotic-se",
    "href": "stata_do/stata_do2.html#setup-and-asymptotic-se",
    "title": "How to Bootstrap",
    "section": "Setup and Asymptotic SE",
    "text": "Setup and Asymptotic SE\nTo get started with bootstrapping, we will estimate a very simple linear regression model using the auto.dta dataset.\n\n\nCode\nset linesize 255\nprogram drop _all\nsysuse auto, clear\nreg price mpg foreign\n\n\n(1978 automobile data)\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     14.07\n       Model |   180261702         2  90130850.8   Prob > F        =    0.0000\n    Residual |   454803695        71  6405685.84   R-squared       =    0.2838\n-------------+----------------------------------   Adj R-squared   =    0.2637\n       Total |   635065396        73  8699525.97   Root MSE        =    2530.9\n\n\n\n------------------------------------------------------------------------------\n\n\n       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   55.69172    -5.28   0.000    -405.2417   -183.1494\n\n\n     foreign |   1767.292    700.158     2.52   0.014     371.2169    3163.368\n       _cons |   11905.42   1158.634    10.28   0.000     9595.164    14215.67\n------------------------------------------------------------------------------\n\n\nThis estimation provides the asymptotic estimation of standard errors under homoskedasticity using the well-known formula:\n\\[\nVar(\\hat \\beta) = \\frac{\\sum \\hat e ^2}{n-k-1} (X'X)^{-1}\n\\]\nLet’s see how this standard errors compare to different types of Bootstrap Standard errors:\n\nParametric Bootstrap\nAs the name suggest, parametric bootstrap requires imposing some parametric assumptions on the source of the error in the model: \\(e\\). We will learn those characteristics from the estimated errors in the original sample.\nSay, for example, that we assume \\(e\\) follows some normal distribution, with variance equal to the variance of the observed error \\(\\sigma^2_e=Var(\\hat e)\\). Parametric bootstrapping would require to draw/create different samples using the following rule: \\[\n\\tilde y_i = X_i \\hat \\beta + \\tilde e_i \\ where \\ \\tilde e_i \\sim N(0,var(\\hat e))\n\\]\nIn this case, all \\(X's\\) are fixed, but the new samples are created by resampling the error, and constructing \\(\\tilde y's\\). This differs only because of the draws of the errors \\(\\tilde e\\).\nOnce you get multiple of samples, and coefficients for each, you can simply Report the associated Standard Errors.\nHow to do it in Stata? Here I will cheat a bit, and use Mata, because it will be faster. Note that you will need to copy the whole code into a dofile and run all of it, or type each line individualy in the command window (once you activate Mata):\n\n\nCode\nset seed 10\n// First we start mata\nmata:\n    // load all data\n    y = st_data(.,\"price\")\n    // You load X's and the constant\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    // Estimate Betas:\n    b = invsym(x'*x)*x'y\n    // Estimate errors:\n    e = y:-x*b\n    // Estimate STD of errors\n    std_e=sqrt(sum(e:^2)/73)\n    // Now we can do the bootstrap:\n    // We first create somewhere to store the different betas\n    bb = J(1000,3,.)\n    // and start a loop\n    for(i=1;i<=1000;i++){\n        // each time we draw a different value for y..say ys\n        ys = x*b+rnormal(74,1,0,std_e)\n        // and estimate the new beta, storing it into bb\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }\nend\n\n\nIf everythings goes well, it should give you the following\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    55.59108401  |\n  2 |   1767.292243    689.3703271  |\n  3 |   11905.41528    1159.921784  |\n    +-------------------------------+\n\n\nNotice that we are explicitly impossing the assumption of homoskedasticity and normality on the errors. This explain why this standard errors are almost identical to the simple asymptotic standard errors.\n\n\nResidual Bootstrap\nResidual bootstrap is very similar to the parametric bootstrap I described above. The main difference is that we no longer impose assumptions on the errors distributions, and instead use the empirical distribution.\nWhat does this mean? Well, In the above example, we have 74 different values for the error \\(e\\), thus resampling means that you create a new \\(\\tilde y\\) by drawing 74 errors from this bag of errors, where all have the same probability of being choosen. \\[\n\\tilde y_i  =  X_i \\hat \\beta  + \\tilde e_i \\ where  \\ \\tilde e \\sim [\\hat e_1, \\hat e_2,...,\\hat e_N]\n\\]\nLets implement it:\n\n\nCode\nset seed 10\nmata:\n    // This remains the same as before\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    bb = J(1000,3,.)\n    // Now we need to know how many observations we have\n    nobs=rows(y)\n        for(i=1;i<=1000;i++){\n        // Here is where we \"draw\" a different error everytime, \n        // runiformint(nobs,1,1,nobs) <- This says Choose a randome number between 1 to K\n        // and use that value to assing as the new error to create ys\n        ys = x*b+e[runiformint(nobs,1,1,nobs)]\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }   \nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    53.63708346  |\n  2 |   1767.292243    703.1172879  |\n  3 |   11905.41528    1118.818836  |\n    +-------------------------------+\n\n\nOnce again, this method keeps \\(X's\\) fixed, and assumes errors are fully homoskedastic, thus interchangable. It does allow for the possiblity errors do not follow a normal distribution.\n\n\nWild-Bootstrap/multiplicative bootstrap\nThe wild bootstrap is another variant of residual bootstrapping. To implement it, we start by estimating the original model and obtaining the model errors. Instead of shuffling or making assumptions about the distribution of the errors, we reuse the error after adding noise to it. Mathematically, this can be expressed as: \\[\n\\tilde y_i=X_i \\hat \\beta + \\hat e_i * v_i\n\\]\nHere, \\(v\\) is the source of the noise that we add to the model. Technically, we can use any distribution for \\(v\\), as long as \\(E(v)=0\\) and \\(Var(v)=1\\). The most common distribution used in wild bootstrap implementations is the “mammen” distribution, but for simplicity, we will use a normal distribution.\n\n\nCode\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n    for(i=1;i<=1000;i++){\n        // Here is where we \"draw\" a different error multiplying the original error by v ~ N(0,1)\n        ys = x*b+e:*rnormal(nobs,1,0,1)\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }\nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    59.62775803  |\n  2 |   1767.292243     586.545592  |\n  3 |   11905.41528    1357.383088  |\n    +-------------------------------+\n\n\nSurprisingly, this approach allows us to control for heteroskedasticity, which is why the standard errors obtained using the wild bootstrap method are quite similar to the ones obtained using reg, robust.\nAnother advantage of this method is that we do not necessarily need to obtain an estimate of the error itself. Instead, we can obtain the Influence Functions of the estimated parameters and disturb those to obtain standard errors. This makes the method feasible for a larger set of estimators, assuming that we can derive the corresponding Influence Functions.\n\n\nPaired bootstrap/Nonparametric bootstrap\nPaired bootstrap is perhaps the most commonly used method in applied econometrics, although it can also be computationally intensive.\nThe basic idea is to use the original sample to draw subsamples with replacement that are of the same size. Then, you estimate the parameters of interest for each subsample and summarize the results. What sets this approach apart from others is that the entire set of observations and characteristics are used in the resampling, not just the residuals. This makes it robust to heteroskedasticity and relatively easy to implement for complex model estimators. Stata has a module dedicated to making the implementation of paired bootstrap easy, but it can also be implemented in Mata.\n\n\nCode\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n    for(i=1;i<=1000;i++){\n        // What I do here is get a vector that will identify the resampling.\n        r_smp = runiformint(nobs,1,1,nobs)\n        // then use this resampling vector to reestimate the betas\n        brs = invsym(x[r_smp,]'*x[r_smp,])*x[r_smp,]'y[r_smp,]\n        bb[i,]=brs'\n    }   \nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    61.88438186  |\n  2 |   1767.292243    575.3963704  |\n  3 |   11905.41528    1374.021799  |\n    +-------------------------------+\n\n\n\n\nEasier Paired bootstrap\nAssuming you do not like to do this with Mata, or that your estimator is a bit more complex than a simple OLS, a better approach for implementing paired bootstrap in Stata is simply using the bootstrap prefix:\nFor example:\nbootstrap, seed(10) reps(1000):reg price mpg foreign\nAnd of course, this approach can implement to bootstrap any 1 line command, although it may be faster for some methods than others.:\nbootstrap, seed(10) reps(1000):qreg price mpg foreign\nbootstrap, seed(10) reps(1000):poisson price mpg foreign\n\n\nBootstrapping a two-step regression\nOften, however, you may want to “bootstrap” something more complex. For example a two/three/…K step estimator. You can still use bootstrap, but it requires a bit more programming. So lets go with a simple 2-step heckman estimator. My recommendation, first implement the estimator for 1 run:\n\n\nCode\nwebuse womenwk, clear\n** data prep\ngen dwage=wage!=.\n** estimation\nprobit dwage married children educ age\npredict mill, score\nreg wage educ age mill\n** delete the variable that was created as intermediate step\ndrop mill\n\n\nNotice that mill was dropped at the end. This is important, because by bootstraping the program, it will beed to be created all over again. Finally, we write our little bootstrap program:\n\n\nCode\n** I like to add eclass properties here\nprogram two_heckman, eclass\n    capture drop  mill  \n    ** you implement your estimator:\n    tempvar smp\n    probit dwage married children educ age\n    predict mill, score\n    ** save the \"sample\" from probit\n    gen byte `smp'=e(sample)\n    reg wage educ age mill\n    ** Delete all variables that were created \n    ** Finally, you will Store all the coefficients into a matrix\n    matrix b=e(b)\n    ** and \"post\" them into e() so they can be read as an estimation output\n    ereturn post b, esample(`smp')\nend\n\n\nAnd apply the bootsrap prefix to it:\n\n\nCode\nbootstrap, reps(250) nodots:two_heckman\n\n\n\nBootstrap results                                        Number of obs = 2,000\n                                                         Replications  =   250\n\n\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0511768    19.20   0.000     .8822212    1.082831\n         age |   .2118695   .0223345     9.49   0.000     .1680947    .2556444\n        mill |   4.001615   .5898434     6.78   0.000     2.845544    5.157687\n       _cons |   .7340391   1.287607     0.57   0.569    -1.789625    3.257703\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "stata_do/stata_do2.html#conclusions",
    "href": "stata_do/stata_do2.html#conclusions",
    "title": "How to Bootstrap",
    "section": "Conclusions",
    "text": "Conclusions\nThis post provides an overview of how bootstrap operates and how to implement it using Stata.\nRemember, not all techniques are suitable for all circumstances. Furthermore, to accurately estimate standard errors while taking into account clustering and weights, resampling methods need to also account for the original sampling structure, or how the data was gathered.\nAccounting for clustering is often straightforward, but handling weights and Strata may require additional attention.\nI hope you found this information beneficial, and please don’t hesitate to reach out with any questions or feedback via email or comments."
  },
  {
    "objectID": "stata_do/stata_do3.html",
    "href": "stata_do/stata_do3.html",
    "title": "Linear Regression via MLE",
    "section": "",
    "text": "Stata has a very useful command that can be used for the estimation of almost any linear and nonlinear models using maximum likelihood. This command is -ml-.\nProperly speaking, this command can be used to obtain M-type estimators, however, I’ll concentrate on maximum likelihood models.\nIn this small tutorial, I’ll provide a small example of how to use -ml- in combination with -margins- to estimate marginal effects for a linear model, as long as one identifies the outcome of interest."
  },
  {
    "objectID": "stata_do/stata_do3.html#lr-as-mle",
    "href": "stata_do/stata_do3.html#lr-as-mle",
    "title": "Linear Regression via MLE",
    "section": "LR as MLE",
    "text": "LR as MLE\nAs I mentioned earlier, the ml command in Stata is a powerful tool for obtaining maximum likelihood estimators, although it can be used to find solutions for any m-type estimators. The one limitation I have encountered with this command is that it can be resource-intensive when estimating complex models on large datasets. For instance, If you have a dataset with one million observations but only use 10% of it for modeling, dropping the unused data before estimation can speed up the process. ml will not do that for you.\nThere are several ways to program ml, such as using the lf, df0, df1, or df2 options. The main difference among them is that you must define the objective function, its gradient, and its Hessian. However, for most purposes, I find that lf is the only one you will ever need.\nWhen using ml with the lf option, you only need to declare the loglikelihood function contributed by each observation in the sample. To illustrate this concept, let’s assume that we want to estimate a simple linear regression using MLE. For this, we need to assume that either the error follows a normal distribution or that the outcome follows a conditionally normal distribution.\n\\[\n\\begin{aligned}\ny_i &= X_i'\\beta + e_i \\\\\ne_i &\\sim N(0,\\sigma^2) \\ or \\ y_i \\sim N(X_i'\\beta,sigma^2)\n\\end{aligned}\n\\]\nThis implies that the Likelihood (\\(L_i\\)) and Log-Likelihood (\\(LL_i\\)) of a single observation is given by: \\[\n\\begin{aligned}\nL_i &= \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\left( -\\frac{1}{2} \\left(\\frac{y_i -X_i'\\beta }{\\sigma} \\right)^2 \\right) \\\\\nLL_i &= -log(\\sigma) - \\frac{1}{2} log (2\\pi) -\\frac{1}{2} \\left(\\frac{y_i -X_i'\\beta }{\\sigma} \\right)^2\n\\end{aligned}\n\\]\nSo we just need to create a program that defines this log-likelihood function.\n\nprogram myols_mle\n    args lnf xb lnsigma\n    local sigma exp(`lnsigma')\n    *qui:replace `lnf' = -`lnsigma' - 1/2 * log(2*_pi) - 1/2 *(($ML_y1-`xb')/`sigma')^2\n    qui: replace `lnf' = log(normalden($ML_y1,`xb',`sigma'))\nend\n\nNotice that this program has 3 arguments (which come after args) (see line 2)\n\nlnf: Will store the log-Likelihood for each observation\nxb: Will store the linear combination of variables and their coefficients\nlnsigma: Will store the \\(log(\\sigma)\\).\n\nWe do not estimate \\(\\sigma\\) directly, because its numerically more stable to estimate the \\(log(\\sigma)\\). Also, we require \\(\\sigma\\) to be strictly positive, which can only be done by using the log transformation.\nIn line 3, I specifically impose the transformation to obtain \\(\\sigma\\).\nIn line 4, I leave the comment of how I would write the full Loglikelihood using the formula I provided before, but for simplicilty I use the built-in normalden (line 5)\nYou should also notice that all arguments will be handled internally as locals, which is why they need to be written within single quotes: ' . The only exception is$ML_y1`, which represents the dependent variable."
  },
  {
    "objectID": "stata_do/stata_do3.html#programming-for-margins",
    "href": "stata_do/stata_do3.html#programming-for-margins",
    "title": "Linear Regression via MLE",
    "section": "Programming for Margins",
    "text": "Programming for Margins\nThe second component I’m interested in introducing today is to create a program that will allow you to modify how margins, operate.\nAs some may know, margins is a Stata built-in program that estimates marginal effects, or marginal means, for any official and unofficial model in Stata. The way it works is rather simple. Once you estimat you model of interest and call on margins it will:\n\nEstimate the predicted outcome for the model. This varies by model, but the default is to use the linear combination of variables and coefficients.\nDepending on other modeling factors, estimate derivatives, or means, of the outcome with respect to every variable in the model.\nCalculate standard errors using Delta method, and summarize results.\n\nSteps 2 and 3 above are common to all models. However, step 1 is the one that needs to be modified everytime one changes from one model to another.\nWhat I will do next is to write a program where I will define different types of outcomes that I may be interested in when analyzing Linear regressions. I will call this program myols_mle_p, following Stata naming standards:\n\nprogram myols_mle_p\n    syntax newvarname [if] [in] , [ mean sigma emean *]\n    if \"`mean'`sigma'`emean'\" ==\"\" {\n        ml_p `0'\n    }\n    marksample touse, novarlist\n    if \"`mean'\" !=\"\"  {\n        tempvar xb\n        _predict double `xb' , eq(#1)\n        gen `typlist' `varlist' = `xb' if `touse'\n        label var `varlist' \"E(y|X)\"\n    }\n    else if \"`sigma'\" !=\"\"  {\n        tempvar xb\n        _predict double `xb' , eq(#2)\n        gen `typlist' `varlist' = exp(`xb') if `touse'\n        label var `varlist' \"E(sigma|X)\"\n    }\n    else if \"`emean'\"!=\"\" {\n        tempvar xb lns\n        _predict double `xb' , eq(#1)\n        _predict double `lns' , eq(#2)\n        local sigma (exp(`lns'))\n        gen `typlist' `varlist' = exp(`xb')*exp(0.5*`sigma'^2) if `touse'\n        label var `varlist' \"E(exp(Y)|X)\"\n    }\nend\n\nThe way I’m defining this program, one could request 3 types of outcomes:\n\nmean: This is the standard outcome. Just looking into the linear combination of X and betas\nsigma: When this option is used, you will obtain the prediction for \\(\\sigma\\) instead of \\(log(\\sigma)\\). May be useful to compare and test heteroskedasticity directly\nemean: This is something different. This option could be used if your outcome of interest was “wages”, but you were modeling “log(wages)”. This will be estimated under the assumption of log-normality.\n\nIf neither option is used, it will revert to use the default."
  },
  {
    "objectID": "stata_do/stata_do3.html#the-estimation",
    "href": "stata_do/stata_do3.html#the-estimation",
    "title": "Linear Regression via MLE",
    "section": "The Estimation",
    "text": "The Estimation\nBefore we estimate the LR model using MLE, lets start by loading some data:\n\nfrause oaxaca, clear\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nNow, to estimate a model using -ml-, we need to use a somewhat complex syntax, that would be better explained in the following code:\nml model lf myols_mle /// \n   (mean:lnwage = educ exper i.female) ///\n   (ln_sigma:  = educ exper i.female) ///\n   , maximize  nolog\nml display   \nLine 1. Indicates you will try and estimate a model using ml, where the model will use lf option (which only requires the log-likelihood function at individual level to be provided. You also need to provide the name of the program that defines the LL function: myols_mle.\nLine 2 and 3. As described earlier, the program myols_mle requires 3 arguments. The first one is the LL function, so we do not need to be concernd about. The other two arguments refer to xb or conditional mean, and lnsigma or log of the variance, need to be declared here. They are order specific. Line 2 will always refer to xb, independent of the name I provide, and Line 3 will always refer to lnsigma.\nIn standard models, we assume homoskedasticity, so we do not need to add covariates to lnsigma, but we can do it and control for heteroskedasticity directly.\nIn line 4, I simply request model to be maximized, but could just as well requested clustered standard errors, or use other options allowed in ml.\nFinally line 5 request displaying the results. So lets see what we get:\n\n                                                        Number of obs =  1,434\n                                                        Wald chi2(3)  = 371.53\nLog likelihood = -871.18998                             Prob > chi2   = 0.0000\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nmean         |\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n       _cons |   2.430164   .0641158    37.90   0.000     2.304499    2.555828\n-------------+----------------------------------------------------------------\nln_sigma     |\n        educ |  -.0356134   .0067794    -5.25   0.000    -.0489008   -.0223259\n       exper |  -.0165439   .0016363   -10.11   0.000     -.019751   -.0133368\n    1.female |   .2066704   .0382432     5.40   0.000     .1317152    .2816256\n       _cons |  -.2813734   .0910696    -3.09   0.002    -.4598665   -.1028802\n------------------------------------------------------------------------------\nYou can compare these results with the standard regress outcome, or hetregress if you want to compare the results allowing for heteroskedastic errors."
  },
  {
    "objectID": "stata_do/stata_do3.html#margins",
    "href": "stata_do/stata_do3.html#margins",
    "title": "Linear Regression via MLE",
    "section": "margins",
    "text": "margins\nMargins can be used here to analyze the effect of covariates on the outcome of interest. The default from -ml- is to use linear combinations of coefficients and covariates. Now, if we want to use our predict command, we need to modify one piece of information in e(). If you type ereturn list, after every Stata command, you will see there is one local named e(predict). This local has the name of a program that is used to get predictions for a given model. We need to modify it, and change the default name to our program: myols_mle_p.\nWe will do this with the following program:\n\nprogram adde, eclass\n    ereturn `0'\nend\nadde local predict myols_mle_p\n\nOk with all of this, we are ready to estimate marginal effects. The next two lines should give you the same result, but I present them here as an example:\n\nmargins, dydx(*)\nmargins, dydx(*) predict(mean)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(y|X), predict(mean)\ndy/dx wrt:  educ exper 1.female\n\n\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nWhat would be more interesting, however, would be to provide outcomes for the predicted standard deviation, or the exponentiated mean:\n\nmargins, dydx(*) predict(sigma)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(sigma|X), predict(sigma)\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.0161816   .0031133    -5.20   0.000    -.0222836   -.0100796\n       exper |   -.007517    .000774    -9.71   0.000    -.0090341       -.006\n    1.female |   .0938119   .0175522     5.34   0.000     .0594102    .1282136\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n\nmargins, dydx(*) predict(emean)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(exp(Y)|X), predict(emean)\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   2.175891    .169221    12.86   0.000     1.844224    2.507558\n       exper |    .236423   .0394785     5.99   0.000     .1590466    .3137995\n    1.female |   -2.27482   .8226334    -2.77   0.006    -3.887152   -.6624884\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nSo how do we interpret the results?. Here one possibility:\nEach additional year of education increases wages by 7.4%, however, as education increases the dispersion of wages decreases. In terms of actual dollar change, in average, that additional year of education translates in a 2.17$ per hour increase."
  },
  {
    "objectID": "stata_do/stata_do3.html#conclusions",
    "href": "stata_do/stata_do3.html#conclusions",
    "title": "Linear Regression via MLE",
    "section": "Conclusions",
    "text": "Conclusions\nThe purpose of this small article was to walk you through how to use -ml- for the estimation of a simple linear regression.\nIn addition, it also introduces you to creating a program that will allow you to use margins, when you have a specific outcome in mind, that is not currently available in the command you are interested in using.\nHope you find it useful.\nComments and question welcome!"
  },
  {
    "objectID": "stata_do/index.html",
    "href": "stata_do/index.html",
    "title": "Stata Programming",
    "section": "",
    "text": "Welcome to the Stata programming section! As a “seasoned” (just another word for oldish) Stata user, I have developed quite few tricks of knowledge and experience in programming with Stata and Mata. I am excited to share my insights with you.\nStata programming is a powerful tool that can enhance your data analysis and streamline your research workflows. For instance, if you do the same thing over and over, why not write a program that will replicate it as well!. Of course, it can be daunting for beginners, and even experienced users may encounter challenges and roadblocks in their programming endeavors (The more skillful you become, the harder the problems you face!).\nIn this section, I provide hints and suggestions for programming, some times replicating simulation excercises, some times for making examples statistical methods, and some times just for fun.\nHere, I will try to cover a wide range of programming topics, from basic syntax and functions to advanced techniques (depending on my own skills), providing examples and code snippets to demonstrate how the code works.\nMy main goal is to help you and myself becoming a more proficient Stata programmer. I hope you find this section informative and useful, and I look forward to helping you advance your Stata programming skills!\nComments and suggestions are always welcome."
  },
  {
    "objectID": "stata_do/stata_do4.html",
    "href": "stata_do/stata_do4.html",
    "title": "The IV-Probit: margins and ml",
    "section": "",
    "text": "In previous articles, I have shown how to use the margins command after ml for the linear regression model (assuming normality) and the probit model. Today, I’ll provide another example for a more complex case: a two-equation model estimation, specifically the “IVprobit,” or instrumental variable probit.\nThis model has gained notoriety due to its inconsistent estimated marginal effects, which have been the subject of multiple threads on Statalist. The issue comes from different versions of “marginal effects” produced by margins in different Stata versions. Each version is correct from a computational standpoint, but not always consistent with common sense.\nMy assessment, backed up by discussions with colleagues, is that:\n\nStata 13 estimated the correct marginal effects for the IVprobit MLE but not for the two-step approach.\nStata 14 and 15 estimated the Full Information Marginal Effect, which is technically correct but contradicts common sense. Prof Wooldridge has extensively discussed this issue and advocates for either MLE or two-step marginal effects.\nStata 16 released an update that now produces something equivalent to the two-step marginal effect for MLE estimation, likely due to recent discussions.\n\nNo changes that I am aware came in Stata 17, except perhaps that we still do not have marginal effects for a two-step approach.\nWithout further ado, let me present my own version of how to estimate an IVprobit model and how to set up the predict program. I’ll also explain why there were so many types of marginal effects in use."
  },
  {
    "objectID": "stata_do/stata_do4.html#the-setup",
    "href": "stata_do/stata_do4.html#the-setup",
    "title": "The IV-Probit: margins and ml",
    "section": "The Setup",
    "text": "The Setup\nFirst thing first. Unless you already have this program saved somewhere in your accesible ado files (most likely the “ado/personal” folder), make sure to have the following program in memory. It will allow you to add or modify information to e(), which is where all estimation commands store information.\n\nclear all\nprogram adde, eclass\n    ereturn `0'\nend\n\nLet’s start with some Econometrics 101: The IVprobit model is a nonlinear model that is useful when you have a binary dependent variable (0-1) but one or more of your controls suffer from endogeneity. In this case, you want to estimate the “probability of success (y=1)” given a set of characteristics, but the characteristics of insterest is a continuous but endogenous explanatory.\nNow, when a variable is endogenous, we cannot estimate the model and interpret the results as causal effects. This happens because changes in the endogenous variable can happen at the same time as changes in unobserved components. Therefore, if the outcome changes, we cannot tell if it is because the endogenous variable changed or because the unobservables changed (as they are, after all, correlated).\nTo deal unobserved confounders, we have two options:\n\nWe can use instruments to isolate the exogenous variation of the variable of interest (using the 2SLS approach, for example)\nUse the instruments to obtain an approximation of the endogenous component that we can control for directly (Control function approach).\n\nIn fact, IV-probit is the application of the latter: a control function approach.\nFormally, IVprobit model can be written as follows:\n\\[\ny_2 = z_1 \\delta_1 + z_2 \\delta_2 + u_2\n\\tag{1}\\]\n\\[\ny_1^*= z_1 \\beta_1 + y_2 \\beta_2 + u_1\n\\tag{2}\\]\n\\[\ny_1 = 1(y_1^*>0)\n\\]\nwhere the errors \\(u_1,u_2\\) follow a bivariate normal distribution:\n\\[\n\\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} \\sim Normal  \\begin{pmatrix}  \\begin{matrix} 0 \\\\ 0 \\end{matrix},\n\\begin{matrix} 1 & \\rho \\sigma_2\\\\ \\rho \\sigma_2 & \\sigma^2_2 \\end{matrix} \\end{pmatrix}\n\\]\nIn this model \\(z_1\\) and \\(z_2\\) are exogenous variables, \\(z_2\\) is a set of instruments, and \\(y_2\\) is a continuous but endogenous variable in the model. Finally we do not observe the latent variable \\(y_1^*\\), but instead observe \\(y_1\\) which only takes values of 0 or 1. So how do we estimate this model?? by parts!\nThe Equation 1 can be estimated directly, because it is a function of exogenous variables only. Thus, we could estimate that equation using standard OLS (as ivprobit-two-step does), or via MLE assuming the normality of the errors.\nThe one that requires more attention is Equation 2. We know that \\(corr(y_2,u_1)\\) is different from zero, which is the cause of the endogeneity of \\(y_2\\). We could, however, decompose \\(u1\\) into two parts. One that contains the endogenous component, and one that is exogenous and uncorrelated with all other variables.\nTo do this, we should first recall that if \\(u_1, u_2\\) follow a bivariate normal distribution, then, conditional on \\(u_2\\), \\(u_1\\) will have the following distribution:\n\\[\nu_1 \\sim N\\left(\\rho \\frac{u_2}{\\sigma_2}, {1-\\rho^2} \\right)\n\\tag{3}\\]\nwhich implies, we could write \\(u_1\\) as follows:\n\\[\nu_1 = \\rho \\frac{ u_2}{ \\sigma_2} + v_1\n\\tag{4}\\]\nIn this case, \\(v_1\\) will be, by construction, uncorrelated with \\(u_2\\) or with \\(y_2\\). So, we if substitute Equation 4 into Equation 1, we obtain:\n\\[\ny_1^* = z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{ u_2}{ \\sigma_2} + v_1\n\\tag{5}\\]\nThis equation can now be estimated directly, assuming we observe \\(u_2\\). However, to be estimated with a probit model, we also need to rescale the equation so that the re-scaled error \\(v_1\\) has a variance of 1.\nBased on Equation 3, we know \\(v_1\\) has a variance of \\(1-\\rho^2\\), so we just need to divide all terms in Equation 5 by \\(\\sqrt{1-\\rho^2}\\) and estimating the following model (or its simplification) using standard probit model. \\[\n\\frac{y^*_1}{\\sqrt{1-\\rho^2}} =\n    z_1 \\frac{\\beta_1}{\\sqrt{1-\\rho^2}} +\n    y_2 \\frac{\\beta_2}{\\sqrt{1-\\rho^2}} +\n    \\frac{\\rho}{\\sqrt{1-\\rho^2}} \\frac{u_2}{\\sigma_2} +\n    \\frac{v_1}{\\sqrt{1-\\rho^2}}\n\\tag{6}\\] \\[\ny^{**}_1 = z_1 \\beta^r_1 + y_2 \\beta^r_2 +\n    \\theta \\frac{u_2}{\\sigma_2} +   v_1\n\\tag{7}\\]\nWhat is the difference between using either equation?. I would argue none, as long as you know how to estimate the standard errors from the system."
  },
  {
    "objectID": "stata_do/stata_do4.html#the-actual-estimation",
    "href": "stata_do/stata_do4.html#the-actual-estimation",
    "title": "The IV-Probit: margins and ml",
    "section": "The actual estimation",
    "text": "The actual estimation\nLet’s discuss the different methods that can be used to estimate the ivprobit model. There are at least three ways to do so.\nThe first method is the two-step approach. In this method, one estimates Equation 1 using OLS, obtains the predicted residuals, which are plugged into equation Equation 7. This can be estimated using a simple probit model.\nThis method has two problems:\n\nIt only provides estimates for the “rescaled” coefficients, not the structural coefficients.\nIt will not provide you with the correct estimation of standard errors, because it will not consider the residuals are carrying over errors from the first step.\n\nSome textbooks suggest that doing so is a simple application of the delta method, or use bootstrap. But, the fact of the matter, is that you need to take into account that the residuals from Equation 1 are estimated not the true residuals \\(u_2\\).\nThe second method is to estimate Equation 1 and Equation 5 simultaneously using full information maximum likelihood. This imposes the assumption that the errors follow a bivariate normal distribution, and allows you to obtain estimates for the structural parameters, in addition to the “link” parameters \\(\\sigma_2\\) and \\(\\rho\\), providing correct standard errors.\nUnder this strategy, the contribution of a single observation to the likelihood function becomes:\n\\[\n\\begin{aligned}\nL_i &= L_i^1*L_i^2 \\\\\nL_i^1 &= \\phi(y_2,z_1\\delta_1 + z_2 \\delta_2, \\sigma_2) \\\\\n\\hat P(y_1|.) &= \\Phi \\left( \\frac{ z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{y_2 -z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2}}\n{\\sqrt{1-\\rho^2}} \\right) \\\\\nL^2_i & = P(y_1|.)^{y_1} * (1-P(y_1|.))^{1-y_1}\n\\end{aligned}\n\\]\nNotice that instead of plugging in \\(\\hat u_2\\) in the probit equation, I explicitly add \\(y_2 -z_1 \\delta_1 - z_2 \\delta_2\\). This allows to explicilty account for the measurement errors of the first stage.\nThere is a third option, which I will call two-step-mle. I call it this way, because the ivprobit will be estimated using Equation 1 and Equation 7. However, I call it MLE, because both equations are estimated simultaneously using MLE:\n\\[\n\\begin{aligned}\nL_i &= L_i^1*L_i^2 \\\\\nL_i^1 &= \\phi(y_2,z_1\\delta_1 + z_2 \\delta_2, \\sigma_2) \\\\\n\\hat P(y_1|.) &=  \\Phi \\left(  z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\frac {y_2 -z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2} \\right) \\\\\nL^2_i & = P(y_1|.)^{y_1} * (1-P(y_1|.))^{1-y_1}\n\\end{aligned}\n\\tag{8}\\]\nThe difference with the standard FIML, is that only rescaled coefficients are estimated, and that the link between both equation is \\(\\theta\\) not \\(\\rho\\). Nevertheless, For this simplified example, both equations identify exactly the same model. If you are interested in this type of use of -ml- see my paper Rios-Avila and Canavire-Bacarreza (2018).\nCompared to the usual two-step approach, however, because the model is estimated simultaneously, the standard errors of all coefficients are correctly estimated, without further calculations (no delta method nor bootstrap).\nOne last thing to notice in this model. First, there is a close relationship between \\(\\theta\\) and \\(\\rho\\), which will affect the rescaled parameters: \\[\n\\theta = \\frac{\\rho}{\\sqrt{1-\\rho^2}} \\rightarrow \\rho =  \\frac{\\theta}{\\sqrt{1+\\theta^2}}\n\\] \\[\n\\beta = \\beta^r \\times \\sqrt{1-\\rho^2} = \\frac{\\beta^r}{\\sqrt{1+\\theta^2}}\n\\]"
  },
  {
    "objectID": "stata_do/stata_do4.html#the-log-likelihood-function",
    "href": "stata_do/stata_do4.html#the-log-likelihood-function",
    "title": "The IV-Probit: margins and ml",
    "section": "The Log Likelihood function",
    "text": "The Log Likelihood function\nWhile I have shown that using FIML and two-step-ml will provide the same results, I’ll stick with the two-step approach, as it allows me to derive marginal effects telling the story of what happened to -margins- through different Stata versions.\nThe following program defines this the log-likelihood function for the IV probit, using the two-step approach (Equation 8), using the following walk-through for the specification:\n\nxb will contain all the exogenous variables \\(z_1\\) plus the endogenous variable \\(y_2\\)\nzb will contain all the exogenous variables \\(z_1\\) and the instruments \\(z_2\\)\n\n\nprogram myivprobit_2sls\n    args lnf xb theta zb lnsigma\n    qui {\n        local y1 $ML_y1\n        local y2 $ML_y2\n        local u2 (`y2'-`zb')\n        tempvar xb_zb p1 p0\n        gen double `xb_zb'= `xb'+`theta'*((`u2')/exp(`lnsigma')) \n        gen double `p1'   = normal( `xb_zb')\n        gen double `p0'   = normal(-`xb_zb')\n        tempvar lnf1 lnf2\n        gen double `lnf1'  = log(normalden(`y2', `zb', exp(`lnsigma')))\n        gen double `lnf2' = log(`p1') if `y1'==1\n        replace    `lnf2' = log(`p0') if `y1'==0\n        replace `lnf' = `lnf1' + `lnf2'\n    }\nend"
  },
  {
    "objectID": "stata_do/stata_do4.html#the-predict-program",
    "href": "stata_do/stata_do4.html#the-predict-program",
    "title": "The IV-Probit: margins and ml",
    "section": "The predict program",
    "text": "The predict program\nSo finally, the part that will be a bit more controversial. The prediction of the probability of success!.\nThe reason why this is controversial is because there are two candidates to identify this expression.\nThe first candidate relates to the structural Equation 2. Basically, if we can estimate the unscaled coefficients, the predicted outcome could be identified by:\n\\[\nP(y_1=1| z_1 , y_2) = \\Phi \\left( z_1 \\beta_1 + y_2 \\beta_2  \\right)\n\\]\nor if one prefers the version based on rescaled coefficients: \\[\nP(y_1=1| z_1 , y_2) = \\Phi \\left( z_1 \\beta^r_1 * {\\sqrt{1-\\rho^2}} + y_2 \\beta^r_2 * {\\sqrt{1-\\rho^2}} \\right)\n\\]\nThus, marginal effects can be obtained by analyzing either one of these equations alone. Standard errors for this expression can be identified directly only if we estimate the structural equation using FIML, or using the rescaled coefficients, making sure standard errors are calculated acounting for the estimation errors of the first stage.\nThe second option relates to estimate the marginal effects using Equation 7:\n\\[\nP(y_1=1| z_1 , y_2, \\hat{u}_2 ) = \\Phi \\left( z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\hat{u}_2 \\right)\n\\tag{9}\\]\nHowever, because \\(hat u_2\\) is never observed, it is usually recommended to average (but not ignore) the impact of \\(\\hat u_2\\) on the equation:\n\\[\nP(y_1=1| z_1 , y_2) = E \\left( \\Phi \\left( z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\hat{u}_2 \\right)| z_1, y_2 \\right)\n\\tag{10}\\]\nThe bottom line: If one uses the two-step approach, marginal effects could be estimated assuming \\(\\hat u_2\\) is just another exogenous variable in the model. The difficulty would be obtaining the correct estimation of standard errors.\nSo lets write these two options into a “predict” program.\n\nprogram myivprobit_p\n    syntax newvarname [if] [in] , [ pr1 pr2  *]\n    if \"`pr1'`pr2'\" ==\"\" {\n        ml_p `0'\n    }\n    tokenize `e(depvar)'\n    local y1  `1'\n    local y2  `2'\n    marksample touse, novarlist\n    if \"`pr1'\" !=\"\"  {\n        tempvar xb zb theta lnsigma\n        _predict double `xb'   , eq(#1)\n        _predict double `theta', eq(#2)\n        _predict double `zb'   , eq(#3) \n        _predict double `lnsigma', eq(#4)       \n        gen `typlist' `varlist' = ///\n            normal(`xb'+`theta'*(`y2'-`zb')/exp(`lnsigma')) if `touse'\n        label var `varlist' \"P(y=1|X) two-step\"\n    }       \n    else if \"`pr2'\"!=\"\" {\n        tempvar xb zb theta lnsigma\n        _predict double `xb' , eq(#1)\n        _predict double `theta'  , eq(#2)\n        gen `typlist' `varlist' = ///\n            normal(`xb'/sqrt(1+`theta'^2)) if `touse'\n        label var `varlist' \"P(y=1|X) FIML\"\n    }\nend\n\nThe first option pr1 will estimate the predicted probability as if the model were estimated using the two-step approach, whereas the second will estimate the predicted probability based on the structural equation.\nAlright, so lets estimate the model and compare the results with the built-in ivprobit command:\n\nclear  \nwebuse laborsup\nglobal  y1   fem_work\nglobal  z1   fem_educ   kids  \nglobal  y2   other_inc\nglobal  z2   male_educ   \n\n*Built in command:\nivprobit $y1  $z1 ($y2 = $z2), two\n\nChecking reduced-form model...\n\n\n\nTwo-step probit with endogenous regressors        Number of obs   =        500\n                                                  Wald chi2(3)    =      93.97\n                                                  Prob > chi2     =     0.0000\n\n\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n\n\n-------------+----------------------------------------------------------------\n   other_inc |\n\n\n   -.058473   .0093364    -6.26   0.000    -.0767719    -.040174\n    fem_educ |    .227437   .0281628     8.08   0.000     .1722389     .282635\n        kids |  -.1961748   .0496323    -3.95   0.000    -.2934522   -.0988973\n       _cons |   .3956061   .4982649     0.79   0.427    -.5809752    1.372187\n------------------------------------------------------------------------------\n\n\nWald test of exogeneity: chi2(1) = 6.50                   Prob > chi2 = 0.0108\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\n\n\n*my ivprobit two-step\nml model lf myivprobit_2sls ($y1 = $z1  $y2 ) ///\n         (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , ///\n         technique(nr bhhh) init(lnsigma:_cons = 2.81 ) maximize nolog\nml display\n\n\n                                                        Number of obs =    500\n                                                        Wald chi2(3)  =  94.01\nLog likelihood = -2368.2062                             Prob > chi2   = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\neq1          |\n    fem_educ |    .227437   .0281561     8.08   0.000     .1722519     .282622\n        kids |  -.1961748   .0496179    -3.95   0.000    -.2934242   -.0989254\n   other_inc |  -.0584729   .0093339    -6.26   0.000    -.0767671   -.0401788\n       _cons |   .3956051   .4981099     0.79   0.427    -.5806724    1.371883\n-------------+----------------------------------------------------------------\ntheta        |\n       _cons |   .4008081   .1626174     2.46   0.014     .0820838    .7195323\n-------------+----------------------------------------------------------------\neq3          |\n    fem_educ |   .3351867   .2825972     1.19   0.236    -.2186937     .889067\n        kids |   .8329056   .5475666     1.52   0.128    -.2403052    1.906116\n   male_educ |   2.845253    .282746    10.06   0.000     2.291081    3.399425\n       _cons |   9.872559   5.029193     1.96   0.050     .0155214     19.7296\n-------------+----------------------------------------------------------------\nlnsigma      |\n       _cons |   2.813383   .0316228    88.97   0.000     2.751404    2.875363\n------------------------------------------------------------------------------\n\n\nYou can see right away that except for differences attributed to rounding errors and degrees of freedom, the results are virtually the same. It is also reasuring to see that the results are also the same when we compared ivprobit-mle and the rescaled coefficients:\n\n*FIML\nivprobit $y1  $z1 ($y2 = $z2), ml nolog\n\n\nProbit model with endogenous regressors                 Number of obs =    500\n                                                        Wald chi2(3)  = 163.88\nLog likelihood = -2368.2062                             Prob > chi2   = 0.0000\n\n-------------------------------------------------------------------------------\n              | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n    other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485\n     fem_educ |    .211111   .0268648     7.86   0.000     .1584569    .2637651\n         kids |  -.1820929   .0478267    -3.81   0.000    -.2758315   -.0883542\n        _cons |   .3672086   .4480724     0.82   0.412    -.5109971    1.245414\n--------------+----------------------------------------------------------------\ncorr(e.othe~c,|\n   e.fem_work)|   .3720375   .1300518                      .0946562    .5958136\nsd(e.other_~c)|   16.66621   .5270318                      15.66461    17.73186\n-------------------------------------------------------------------------------\n\n\nWald test of exogeneity (corr = 0): chi2(1) = 6.70        Prob > chi2 = 0.0096\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\n\n\n*my ivprobit two-step\nml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , ///\n    technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog\nadde local predict myivprobit_p                 \nest store myivp\n*with rescaled coefficients:\nnlcom   (other_inc: _b[other_inc]/sqrt(1+_b[theta:_cons]^2)) ///\n        (fem_educ: _b[fem_educ]/sqrt(1+_b[theta:_cons]^2)) ///\n        (kids: _b[kids]/sqrt(1+_b[theta:_cons]^2)) ///\n        (cons: _b[_cons]/sqrt(1+_b[theta:_cons]^2)) \n\n\n   other_inc: _b[other_inc]/sqrt(1+_b[theta:_cons]^2)\n    fem_educ: _b[fem_educ]/sqrt(1+_b[theta:_cons]^2)\n        kids: _b[kids]/sqrt(1+_b[theta:_cons]^2)\n        cons: _b[_cons]/sqrt(1+_b[theta:_cons]^2)\n\n\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662027   -.0423485\n    fem_educ |    .211111   .0268648     7.86   0.000      .158457    .2637651\n        kids |  -.1820929   .0478267    -3.81   0.000    -.2758316   -.0883543\n        cons |   .3672077   .4480724     0.82   0.412    -.5109982    1.245414\n------------------------------------------------------------------------------\n\n\nAgain, showing exactly the same results"
  },
  {
    "objectID": "stata_do/stata_do4.html#a-story-of-marginal-effects",
    "href": "stata_do/stata_do4.html#a-story-of-marginal-effects",
    "title": "The IV-Probit: margins and ml",
    "section": "A Story of marginal effects",
    "text": "A Story of marginal effects\nLet me now walk you through the Story of marginal effects with ivprobit.\n\nStata 13\nBack in Stata 13, marginal effects for IV probit were estimated using the structural equation coeffients: \\[\nP(y_1=1|z_1,y_2)=\\Phi(z_1\\beta_1+y_2\\beta_2)\n\\]\nSo that marginal effects were defined as: \\[\n\\begin{aligned}\n\\frac{\\partial P(y_1=1|.)}{\\partial z_1} = \\phi ( z_1 \\beta_1 + y_2 \\beta_2  ) \\beta_1 \\\\\n\\frac{\\partial P(y_1=1|.)}{\\partial y_2} = \\phi ( z_1 \\beta_1 + y_2 \\beta_2  ) \\beta_2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAs you may have noticed, I’m rewritting few of my older posts using Quarto. So, I can only use Stata17 dynamically. Because of that the code you will see below will not be reproducible, unless you have the same Stata version\n\n\nIf you have access to Stata 13, you will be able to reproduce the following output:\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                          Number of obs   =        500\nModel VCE    : OIM\n\nExpression   : Probability of positive outcome, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |   -.014015   .0009836   -14.25   0.000    -.0159428   -.0120872\n    fem_educ |   .0545129   .0066007     8.26   0.000     .0415758      .06745\n        kids |  -.0470199   .0123397    -3.81   0.000    -.0712052   -.0228346\n   male_educ |          0  (omitted)\n------------------------------------------------------------------------------\nThis marginal effects are emulated using pr2 after myivprobit:\n\nest restore myivp\nmargins, dydx(*) predict(pr2) force\n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: P(y=1|X) FIML, predict(pr2)\ndy/dx wrt:  fem_educ kids other_inc male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0545129   .0066003     8.26   0.000     .0415766    .0674493\n        kids |  -.0470199   .0123394    -3.81   0.000    -.0712047   -.0228351\n   other_inc |   -.014015   .0009837   -14.25   0.000    -.0159431   -.0120869\n   male_educ |          0  (omitted)\n------------------------------------------------------------------------------\n\n\nYou will see output from margins also include male_educ in the list of exogenous variables. This happens because it is an explanatory variable for at least one equation in the model (first). However, because this variable is not included in the second equation, it has a no effect on it."
  },
  {
    "objectID": "stata_do/stata_do4.html#stata-14.1",
    "href": "stata_do/stata_do4.html#stata-14.1",
    "title": "The IV-Probit: margins and ml",
    "section": "Stata 14.1",
    "text": "Stata 14.1\nWhen we reached Stata 14.1, a change was introduced in how probabilities were calculated after ivprobit. As it says in the “whatsnew” material, the new formulation would take into account endogeneity.\nSpecifically, they use what I call the 2sls predicted probabilities, following equation (5) wth the caveat that \\(\\hat u_2\\) was substituted by \\(y_2 - z_1 \\delta_1 -z2\\delta_2\\):\n\\[\nP(y_1=1|z_1,y_2,z_2) =\\Phi \\left(\n    \\frac{z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{y_2 - z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2}}\n    {\\sqrt{1-\\rho^2}}   \\right)\n\\tag{11}\\]\nWhile the two equations above are basically the same, they have important differences when marginal effects are estimated by software. Specifically, the probability of sucess is now a function of \\(z_2\\)!.\nSo let me explain first what Stata 14.1, did. To estimate marginal effects, partial derivatives were based on Equation 11:\n\\[\n\\begin{aligned}\n\\frac{\\partial P(y=1|.)}{\\partial z_1} &=\n  \\phi(.)*\\left( \\frac{\\beta_1}{\\sqrt{1-\\rho^2}}  \n  -\\frac{\\rho}{\\sqrt{1-\\rho^2}} * \\frac{\\delta_1}{\\sigma_2} \\right)  \\\\\n   \\frac{\\partial P(y=1|.)}{\\partial z_2} &=\n    \\phi(.)*\\left( 0\n  -\\frac{\\rho}{\\sqrt{1-\\rho^2}} * \\frac{\\delta_2}{\\sigma_2} \\right)  \\\\\n\\frac{\\partial P(y=1|.)}{\\partial y_2} &=\n    \\phi(.)*\\left( \\frac{\\beta_2}{\\sqrt{1-\\rho^2}}  \n    +\\frac{1}{\\sqrt{1-\\rho^2}} * \\frac{1}{\\sigma_2} \\right)    \n\\end{aligned}\n\\]\nFrom the technical point of view, these partial derivatives are correct, since they are capturing both the direct and indirect effects of all variables on the probability of success. Something similar to total, rather than partial, derivative.\nThe problem, however, is that this assumes we could actually observe how the unobserved component changes when other variables change. Standard regression analysis, however, would say that these unobserved components should be considered as fixed, and instead one should estimate marginal effects averaging over the unobserved factors. Thus, the second term on each one of the above derivatives should be zero.\nNevertheless, if you try estimating marginal effects with Stata 14.2, you will get the following result:\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                        Number of obs     =        500\nModel VCE    : OIM\n\nExpression   : Probability of positive outcome, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0097802   .0014994    -6.52   0.000     -.012719   -.0068414\n    fem_educ |   .0623273    .007099     8.78   0.000     .0484135     .076241\n        kids |  -.0614265   .0139446    -4.41   0.000    -.0887574   -.0340956\n   male_educ |  -.0194406   .0022103    -8.80   0.000    -.0237728   -.0151084\n------------------------------------------------------------------------------\nTo replicate this using myivprobit, I would estimate marginal effects using option pr1, requesting derivates to be estimated without the chain rule (nochain). This makes sure that one takes into account the effect of all changes in \\(y_2, z_1\\) and \\(z_2\\) on the predicted outcome \\(P(y=1|.)\\):\n\nest restore myivp\nmargins, dydx(*) predict(pr1) force nochain\n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\n\n\nExpression: P(y=1|X) two-step, predict(pr1)\ndy/dx wrt:  fem_educ kids other_inc male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0623273   .0060316    10.33   0.000     .0505055     .074149\n        kids |  -.0614265   .0128383    -4.78   0.000    -.0865891   -.0362639\n   other_inc |  -.0097802   .0009828    -9.95   0.000    -.0117065   -.0078539\n   male_educ |  -.0194406   .0074886    -2.60   0.009    -.0341181   -.0047631\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "stata_do/stata_do4.html#stata-16",
    "href": "stata_do/stata_do4.html#stata-16",
    "title": "The IV-Probit: margins and ml",
    "section": "Stata 16",
    "text": "Stata 16\nThe earlier version of Stata 16 came with very similar problems as the ones mentioned above. However, due in part to an earlier version of this article, in November of 2020, Stata made a correction in how marginal effects were estimated for ivprobit as well as other related commands (see update 19nov2020).\nIn this update, they change the default option and now produces the correct marginal effects, assuming the predicted errors are fixed.\nLet’s take a closer look at the findings. First, it appears that Stata 14, 15, and early 16 versions were unintentionally estimating a partial effect that accounted for a second-order effect through the first stage regression. While this might have a negligible effect on the exogenous variables, it could have a considerable impact on the endogenous variable of interest, resulting in some people reporting negative marginal effects even when the estimated coefficient was positive. Additionally, the instrument, in this case male_educ, would also appear in the output, capturing only a second-order effect on the outcome of interest.\nHowever, after a lively discussion on Statalist, including input from Prof. Wooldridge, it was revealed that Stata (and margins) was incorrectly estimating marginal effects. As shown here, partial derivatives were being estimated through the first and second equations, leading to incorrect results.\nProf. Wooldridge recommended a manual two-step approach for estimating marginal effects, with standard errors obtained via bootstrap, using Equation 9 to estimate the partial effects. This makes a difference because we will be making the explicit assumption that \\(\\hat u_2\\) does not change when the other variables change. This will modify the partial effects to the following:\n\\[\n\\begin{aligned}\n\\frac{\\partial P(y=1|.)}{\\partial z_1} &=\n  \\phi(.)*\\left( \\frac{\\beta_1}{\\sqrt{1-\\rho^2}} \\right)   \\\\\n   \\frac{\\partial P(y=1|.)}{\\partial z_2} &=\n    \\phi(.)* 0  \\\\\n  \\frac{\\partial P(y=1|.)}{\\partial y_2} &=\n    \\phi(.)* \\left( \\frac{\\beta_2}{\\sqrt{1-\\rho^2}}  \\right)     \n\\end{aligned}\n\\]\nThe differences with the “structural” marginal effects are that the evaluation of \\(\\phi(.)\\) includes the predicted values of the errors (\\(\\hat u_2\\)), and that coefficients used correspond to the two-step procedure ones (rescaled).\nTo show empirically how this works, we can compare the builtin command, with the “two-step” procedure suggested by Prof. Wooldridge:\n\n* two step procedure\n* 1st\nqui: reg $y2 $z1 $z2\npredict double u2, resid\n* 2nd\nqui: probit $y1 $z1 $y2 u2, nolog\n\nmargins, dydx(*) predict(pr)\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: Pr(fem_work), predict(pr)\ndy/dx wrt:  fem_educ kids other_inc u2\n\n\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175   .0060271    10.72   0.000     .0528045    .0764304\n        kids |  -.0557355   .0129302    -4.31   0.000    -.0810782   -.0303929\n   other_inc |  -.0166128   .0022499    -7.38   0.000    -.0210225   -.0122032\n          u2 |   .0068326    .002632     2.60   0.009     .0016741    .0119912\n------------------------------------------------------------------------------\n\n\nThe standard errors here will not be correct, but bootstrap could be applied to obtain corrected standard errors.\nWith the correction to estimation of marginal effects pushed in Novenber of 2020, we can produce the correct point estimates for marginal effects, which follows Prof Wooldrige suggestion, and my discussion presented here.\n** built-in command\nqui:ivprobit $y1  $z1 ($y2 = $z2), ml\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                        Number of obs     =        500\nModel VCE    : OIM\n\nExpression   : Average structural function probabilities, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0166128   .0012889   -12.89   0.000     -.019139   -.0140867\n    fem_educ |   .0646175   .0073529     8.79   0.000      .050206     .079029\n        kids |  -.0557355   .0144233    -3.86   0.000    -.0840047   -.0274664\n------------------------------------------------------------------------------\nSo you can see that the two-step approach and the built-in approach now provide the same marginal effects. And since the official command estimates all coefficients simultaneously, the standard errors can be taken as correct (more on that later).\nSo how can we correct for this with our predict program. Since there is nothing to prevent margins to obtain numerical derivatives across both equations, we need to modify the specification slighly. First, we create clone copies of all variables that enter the second stage: z_1 and y_2, and use them for the model estimation:\n\nclonevar c_other_inc = other_inc\nclonevar c_fem_educ  = fem_educ\nclonevar c_kids      = kids\nglobal  y2b c_other_inc\nglobal  z1b c_fem_educ c_kids \n\nml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2b = $z1b $z2  ) (lnsigma:) , ///\n            technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog\nadde local predict myivprobit_p                 \nest sto myivp   \n\nThe idea of using “clones” of the exogenous variables and endogenous one is to have access to the same information as the original data, but making sure they do not change when the original data changes.\nMarginal effects can be calculated as I did before, except that I now make it explicit to request marginal effects with respect to \\(z_1\\) and \\(y_2\\) only.\n\nest restore myivp       \nmargins, dydx($z1 $y2) predict(pr1) force \n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: P(y=1|X) two-step, predict(pr1)\ndy/dx wrt:  fem_educ kids other_inc\n\n\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175    .006331    10.21   0.000     .0522089    .0770261\n        kids |  -.0557355   .0134693    -4.14   0.000    -.0821349   -.0293362\n   other_inc |  -.0166128   .0023499    -7.07   0.000    -.0212185   -.0120072\n------------------------------------------------------------------------------\n\n\nAnd done!. We have been able to reproduce the second version, two-step, marginal effects for the instrumental variable probit model, that follows the two-step approach advocated by Prof. Wooldridge, and officially included in Stata 16 and above.\nThere is only one last perky detail. If you look at the marginal effect standard errors I produce with the myivprobit command, and compare it with the marginal effects the ivprobit command produces, you will notice they are different.\nThe reason for this was that, based on unofficial words from the developers, at the time:\n\nthe current formulation assumes \\(\\rho\\) and \\(\\sigma\\) to be constant, when standard errors are obtained.\n\nWhile this may seem incorrect, I understand the intuition behind this idea.\nIf you recall the estimation of marginal effects from the structural equation, it is not affected by \\(\\rho\\) nor \\(\\sigma\\). Perhaps this was one of the reasons why the estimated standard errors (Nov2020) are so similar to the ones based on the “old” structural marginal effects.\nMy own command, however, accounts for the uncertainty in these parameter. This also seems correct since two-step marginal procedures are expected to be less efficient than the Full Information counterparts.\nOf course, if you prefer to have a tie-breaker on which one is correct, I can use a Bootstrap procedure to produce the elusive standard errors. Basically, I’ll use the manual two-step procedure, along with a 250 bootstrap repetitions, to report the results:\n\nprogram bs_ivprobit, eclass\n    reg $y2 $z1 $z2\n    capture drop u2\n    predict double u2, resid\n    probit $y1 $z1 $y2 u2\n    margins, dydx(*) predict(pr) nose post\nend\nbootstrap , reps(250) seed(1) nodots:bs_ivprobit\n\n\nAverage marginal effects                                   Number of obs = 500\n                                                           Replications  = 250\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175   .0058983    10.96   0.000      .053057     .076178\n        kids |  -.0557355   .0138358    -4.03   0.000    -.0828532   -.0286179\n   other_inc |  -.0166128   .0023778    -6.99   0.000    -.0212732   -.0119525\n          u2 |   .0068326   .0027569     2.48   0.013     .0014292    .0122361\n------------------------------------------------------------------------------\n\n\nIn this case, it seems that the bootstrap estimates seem to favor my version of marginal effects and standard errors!"
  },
  {
    "objectID": "stata_do/stata_do4.html#one-last-change.",
    "href": "stata_do/stata_do4.html#one-last-change.",
    "title": "The IV-Probit: margins and ml",
    "section": "One last change.",
    "text": "One last change.\nSeems my guess was correct!. In March of 2021, Stata pushed another update to Stata16. They have now changed how SE are estimated after margins, which now coincides with the experiment I started with.\n\nivprobit $y1  $z1 ($y2 = $z2), ml nolog\nmargins, dydx(*) predict(pr)\n\n\nProbit model with endogenous regressors                 Number of obs =    500\n                                                        Wald chi2(3)  = 163.88\nLog likelihood = -2368.2062                             Prob > chi2   = 0.0000\n\n-------------------------------------------------------------------------------\n              | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n    other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485\n     fem_educ |    .211111   .0268648     7.86   0.000     .1584569    .2637651\n         kids |  -.1820929   .0478267    -3.81   0.000    -.2758315   -.0883542\n        _cons |   .3672086   .4480724     0.82   0.412    -.5109971    1.245414\n--------------+----------------------------------------------------------------\ncorr(e.othe~c,|\n   e.fem_work)|   .3720375   .1300518                      .0946562    .5958136\nsd(e.other_~c)|   16.66621   .5270318                      15.66461    17.73186\n-------------------------------------------------------------------------------\nWald test of exogeneity (corr = 0): chi2(1) = 6.70        Prob > chi2 = 0.0096\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: Average structural function probabilities, predict(pr)\ndy/dx wrt:  other_inc fem_educ kids\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0166128   .0023501    -7.07   0.000    -.0212189   -.0120068\n    fem_educ |   .0646175   .0063309    10.21   0.000     .0522091    .0770258\n        kids |  -.0557355   .0134692    -4.14   0.000    -.0821347   -.0293364\n------------------------------------------------------------------------------\n\n\nSo there is my small contribution to Stata!"
  },
  {
    "objectID": "stata_do/stata_do4.html#conclusion",
    "href": "stata_do/stata_do4.html#conclusion",
    "title": "The IV-Probit: margins and ml",
    "section": "Conclusion",
    "text": "Conclusion\nThe command -ml- is a powerful tool that can be used to estimate single or multiple equation models, as long as the loglikelihood functions (and their inter-relations) can be properly defined.\n-margins- is also a very flexible command that can be easily combined with -ml- to expand the estimation of marginal effects for properly defined outcomes. While the command is flexible and relatively easy to use, these properties can also be double-edge swords, if one is not aware of the mechanics behind the actual estimation of partial effects.\nIn my view, the original estimation of marginal effects after iv-probit was correct, but the changes it received in Stata 14.1 introduced what we could call a bug, that was based on solid Math. However, unless you dig deeper into what ivprobit tries to estimate, it would be difficult to say why that change produced undesirable results.\nThe updates pushed in Stata 16 made the necesary corrections following my suggestions, and now produces correct partial effects (two-step like), even adopting my comment regarding standard errors."
  },
  {
    "objectID": "app_metrics/app_metrics3.html",
    "href": "app_metrics/app_metrics3.html",
    "title": "DID: The Fall",
    "section": "",
    "text": "Differences-in-Differences has been a popular approach for identifying causal effects of treatments, utilizing data from both treated and untreated units across time. However, in recent years (5? as of 2023), this methodology has come under scrutiny due to several flaws that have been identified, leading to what some refer to as the DID-Revolution. Numerous econometricians, both established and new, have worked on this topic, highlighting the problems associated with the methodology and proposing potential solutions. Additionally, they have suggested extensions to the model to address a wide range of scenarios (The literature has grown…alot).\nAs someone who was initially perplexed by these changes, problems, and solutions, I started reading in detail on the topic, and had the opportunity to develop two Stata commands:\n\ncsdid and csdid2, which implement Callaway and Sant’Anna (2021) approach\njwdid, which implements the approach suggested by Wooldridge (2021).\n\nThrough this process of coding and reading, I gained a better understanding of several concepts, particularly why certain strategies, such as the Two-way-Fixed-effect (TWFE), may not yield accurate estimates.\nTo simplify things and improve intuition, I will discuss the methodology and its problems and solutions."
  },
  {
    "objectID": "app_metrics/app_metrics3.html#did-2x2-canonical-design",
    "href": "app_metrics/app_metrics3.html#did-2x2-canonical-design",
    "title": "DID: The Fall",
    "section": "DiD 2x2 Canonical Design",
    "text": "DiD 2x2 Canonical Design\nLet’s start with the fundamental building block of the Differences-in-Differences methodology, which is the 2x2 canonical design that is widely used in many papers related to DID models.\nThe basic 2x2 DiD design involves observing two groups of observations across two periods in time. During the first period, neither of these groups receive a treatment, and thus, it can be assumed that they have similar experiences up to that point.\nIn the second period, one of the groups receives a treatment, such as a training program, medicine, or other types of treatments, while the other remains untreated. By comparing the changes in outcomes between the treated and untreated groups over time, we can estimate the causal effect of the treatment.\nTo analyze the data, a simple 2x2 matrix can be designed after the experiment is finalized, which helps to quantify the impact of the treatment.\n\n\n\nTime \nTR=0\nTR=1\n\n\n\n\nT=0\n\\(Y(0,0)\\)\n\\(Y(1,0)\\)\n\n\nT=1\n\\(Y(0,1)\\)\n\\(Y(1,1)\\)\n\n\n\nTo simplify notation, I’ll assume that all the \\(Y(TR,T)\\) represent the average outcome for the group -TR- (treated or untreated) at time T (Pre=0 and Post =1).\n\nHow are treatetment effects estimated?\nWe can begin by noting that the treatment occurred in T=1. One naive estimator that we could use to estimate the treatment effect (TE) would be to simply obtain the difference in outcome between the treated and untreated groups, which is expressed as “\\(Y(1,1)-Y(0,1)\\)” (First D).\nHowever, this approach is incorrect because it fails to account for the self-selection bias that may exist between the two groups. For instance, the individuals who received the treatment may have been those who would benefit from it the most or needed it the most, which could lead to differences in their average outcomes.\nAnother naive approach would be to only examine the treated unit and evaluate how much its outcome has changed after the treatment, which is expressed as “\\(Y(1,1)-Y(1,0)\\)” (the other D). This approach would also be incorrect because the observed change could be due to some natural growth or trend that the unit experienced independent of the treatment.\nTo properly estimate the treatment effect (TE) , we need to combine both strategies and obtain the Differences in Differences (DiD). This approach helps to identify the causal effect of the treatment by comparing the changes in outcomes between the treated and untreated groups over time.\nIntuitively, this could be thought in two ways:\n\nEstimate the TE by comparing the treated-untreated outcomes change in the post period to the pre-period outcome difference.\n\nTT1=  (y11-y01)  Post-period   Treated vs untreated\n     -(y10-y00)  pre -period   Treated vs untreated\nIn essence this captures the TE if the selection bias of the second period is the same as the selection bias observed in the first period.\n\nEstimate the TE by comparing the outcome change for the treated group across time to the outcome change experienced by the not-treated group.\n\nTT2=  (y11-y10)  Treated    Post vs pre\n     -(y01-y00)  Untreated  Post vs pre\nIn this case, the TE is estimated if both groups would have experienced the same trends in their outcomes across time.\nBoth strategies estimate the same treatment effect, but they rely on different assumptions.\n\nThe first strategy assumes “bias-stability,” which implies that any factors that explain the difference between treated and untreated outcomes after the treatment are also present before the treatment and can be eliminated.\n\n-The second strategy relies on the “parallel-trends” assumption, which assumes that the treated outcome would have experienced a similar and parallel change in outcome as the untreated units experience.\nThis is the vanilla 2x2, piece-of-cake strategy. If the basic assumptions hold, you cannot go wrong with this. The problem, surprisingly or not, is what happens when you have more data."
  },
  {
    "objectID": "app_metrics/app_metrics3.html#multiple-periods-did-txg-design",
    "href": "app_metrics/app_metrics3.html#multiple-periods-did-txg-design",
    "title": "DID: The Fall",
    "section": "Multiple Periods DID TxG Design:",
    "text": "Multiple Periods DID TxG Design:\n\nWhere things went wrong\nAs I mentioned earlier, the simple case of DiD is straightforward to estimate and get right. We can even use a simple linear regression analysis to calculate the treatment effect (TE) using the formula:\n\\[\ny_{it} = a_0 + a_1*tr + a_2*t + \\delta * (tr*t) +e_{it}\n\\]\nHowever, the challenge arises when the design has multiple time periods and groups, and the treatment occurs at different times. This is known as treatment timing heterogeneity. In the following discussion, I will refer to insights from Callaway and Sant’Anna (2021) and Goodman-Bacon (2021) to explain the concepts.\n\n\nSetup\nAssume that there are four time periods T=1, 2, 3, and 4, and five “groups” G=Never Treated (NT), 1, 2, 3 and 4.\nIn the 2x2 design, the treatment identifier “TR” distinguishes treated (1) from untreated (0) units. However, in a design with more time periods, the possibility arises that units can be treated at different points in time or never treated at all.\nCallaway and Sant’Anna (2021) use the letter “G” to identify this variable and represent observations that are never treated as “infinity \\(\\infty\\)”. This is done because those units are never treated in the window of time we have access to.1\nFor simplicity, I will use “NT” to represent never treated units. It is worth noting that groups that are always treated cannot be identified in this design.\nThe TxG design can be represented in matrix form as follows:\n\n\n\nTxG Design\n\n\nIn this TxG design, orange cells represent units that have effectivly received treatment, while the blue cells represent potential control units that have not-yet received treatment. The green cells correspond to never treated units that can be used as perfect controls, assuming there are no spill-over effects.\nThis design can be thought of as a combination of multiple 2x2 designs, but not all possible combinations are good ones for estimating the treatment effect. This is, infact, where the traditional TWFE model get things wrong.\nA good reference that explains the problem is Goodman-Bacon (2021). The intuition behind the paper is that he traditional TWFE model obtains a parameter for TE that is the average of all possible 2x2 designs that could be constructed from the above matrix. However, not all of them are good ones!.\nSome of the 2x2 combinations will provide interesting and meaningful results:\n\nYou can use some of them to calculate Treatment effects.\nSome can be used for testing assumptions regarding parallel trends.\nSome, however, will be combinations that are not useful, and may lead you to incorrect conclusions.\n\n\n\nWhat is the TWFE?\nTo account for the many periods (T) and treatment-cohorts (G) in the data structure, an extension to the simple linear model was proposed. This involved estimating a linear model that includes controls (fixed effects/dummies) for all available periods in the data, as well as for all panel individuals (or treatment-cohort groups). To capture the treatment effect, researchers would include a dummy variable \\(D\\) that takes the value of 1 after a unit has been treated, and 0 if it was never or not yet treated. This specification can be written as: \\[\ny_{it} = a_i + a_t + \\delta D_{it}+e_{it}\n\\]\nwhere \\(a_i\\) captures the individual fixed effect (or cohort), \\(a_t\\) captures the time fixed effect, and \\(\\delta\\) which was meant to capture an average treatment effect for treated units, across all time periods.\nGoodman-Bacon (2021) discovered that the parameter of interest \\(\\delta\\) is a weighted average of all feasible DiD designs that could be constructed from the data. This is because the Ordinary Least Squares method attempts to leverage all possible variation, without taking into account the nuances of which ones are actually useful for identifying causal effects, and which ones are not adequate to do so.\n\n\nFirst the Good\nLets say, we are interested in the treatment effect only for the group that was first treated in period 2 (G=2).\nThe first thing to consider is which groups could be used as a “control”, to appropriately identify the TE. The first and easy choice is to compare the G=1 units with observations that were never treated (NT), the “perfect” control group.\nUsing the NT as control units, we can construct at least 3 2x2 DiD setups to identify the TE, at periods 2, 3 and 4:\n\n\n\nfig3\n\n\nA very similar exercise could be done if one is interested in analyzing the TE for other groups (say G3 and G4):\nIf a Never treated group were not available, it is also possible, and valid given limited information, to use units from other “treated” groups as controls, as long as they have not yet been treated:\n\n\n\nfig4\n\n\nAverage treatment effects could then be derived by “averaging” this individual treatment effects.\n\n\nSecond the Bad\nJust as there are good, there are also bad designs. As described in Goodman-Bacon (2021), one of the pitfalls of the TWFE is that it may also try to identify TE by comparing units that are already treated, but at different times.\n\n\n\nfig5\n\n\nConsider the first panel. It compares data for G3 to those in G2. At T=2, G3 is untreated, whereas G2 is treated both at T=2 and T=3. If the treatment effect were homogenous (a location shift), this may work. However, if the treatment affected both G2 and G3 differently, it would violate the parallel trend assumption, and this 2x2 design will not identify Treatment effects.\nNow, why are these units used as controls?\nFrom an OLS viewpoint, the reason is that the treatment status for a given unit remains constant across time, thereby making it appear as though the previously treated unit is a good control. Nonetheless, if the treatment effect varies over time, then the parallel trends assumption may be invalidated, and treatment effects will not be identified.\n\n\nBad controls and Negative weights: How bad can it be?\nThe problem describe above can be summarized with a single idea. WHen using OLS, sometimes one uses bad controls to estimete Treatment effects, which will affect the identification.\nThe other concept that has been used to describe the problem is one about Negative weights, which are closely related.\nFirst the technical part.\nAssume we have balanced panel data, and estimate the following model: \\[\ny_{it} = a_i + a_t + \\delta D_{it}+e_{it}\n\\]\nOne way to estimate this effects easily is to apply Frisch–Waugh–Lovell theorem, demean \\(D_{it}\\), and estimate \\(\\delta\\) using the new data:\n\\[\n\\begin{aligned}\n    \\delta^{twfe} &=\\frac{\\sum \\bar{\\bar D}_{it} y_{it}}{\\sum \\bar{\\bar D}_{it}^2} \\\\\n    \\bar{\\bar D}_{it} &= D_{it}+\\bar D-(\\bar D_i + \\bar D_t)\n\\end{aligned}    \n\\]\nWhen estimating treatment effects, treated units are expected to receive a positive weight. However, after applying FWL, there is no assurance that \\(\\bar{\\bar D}_{it}\\) will be positive for already treated units. This happens because \\(\\bar D_t\\) is larger in later periods, and \\(\\bar D_i\\) is larger for units that were treated earlier. In consequence, units that were treated earlier could incorrectly receive a negative weight, when estimating the treatment effect, it could also be that units that were never treated are incorrectedly assigned a possitive weight, when they should receive negative weights.\nHow bad could it be? Consider the following excercise/code:\n\n\nCode\nclear\nset seed 101\nset obs 100\ngen id = _n\ngen gv = runiformint(2,10)\nreplace gv = 0 if gv>=8\nexpand 10\nbysort id:gen t=_n\ngen trt = gv<=t & gv!=0\nssc install hdfe\nhdfe trt, abs(t id) gen(trt_d)\ngen y = 0\nreplace y = 1 if trt==1 & trt_d <0 \n\n\nIn this case, y is zero for everyone, but for selected treated units at some point in time. You would expect average treatment effects to be possitive. TWFE, would dissagree:\n\n\nCode\nqui:ssc install reghdfe\nreghdfe y trt, abs(id t)\n\n\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      1,000\nAbsorbing 2 HDFE groups                           F(   1,    890) =      44.48\n                                                  Prob > F        =     0.0000\n                                                  R-squared       =     0.5367\n                                                  Adj R-squared   =     0.4799\n                                                  Within R-sq.    =     0.0476\n                                                  Root MSE        =     0.2275\n\n\n\n------------------------------------------------------------------------------\n\n\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n\n\n-------------+----------------------------------------------------------------\n\n\n         trt |\n\n\n  -.1770374   .0265457    -6.67   0.000    -.2291368    -.124938\n\n\n       _cons |   .1962698   .0145409    13.50   0.000     .1677313    .2248083\n\n\n------------------------------------------------------------------------------\n\n\n\nAbsorbed degrees of freedom:\n\n\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n\n\n-------------+---------------------------------------|\n          id |       100           0         100     |\n\n\n           t |\n\n\n        10           1           9     |\n-----------------------------------------------------+\n\n\nBut why is that? The way treatment was assigned, only the units that were treated earlier have a positive treatment, but those are the ones used as controls later on, receiving negative weights. Thus, TWFE uses that and estimates negative TE.\n\n\n\n\n\nFWL-Weights"
  },
  {
    "objectID": "app_metrics/app_metrics3.html#this-were-the-problems-but-solutions",
    "href": "app_metrics/app_metrics3.html#this-were-the-problems-but-solutions",
    "title": "DID: The Fall",
    "section": "This were the problems, but solutions?",
    "text": "This were the problems, but solutions?\nWhat I provided you here was a brief review of DID, with some of the intution on why TWFE fails. Now, it doesn’t mean there is no solutions moving forward. In fact, by the time you are reading it, you will probaly be aware of many of them.\nOver the last years, there have been many propositions trying provide solutions that would tackle the problems related to TWFE DID. I have been lucky enough to participate in the after programming of couple of them, but also worked closely to understand some of the others: csdid/csdid2; jwdid; did2s; did_imputation; did_multiplegt.\nThese new strategies have one thing in commong. They aim to solve the problems of TWFE by either using good 2x2 designs, avoiding wrong 2x2 designs, or adjusting weights to reduce the impact of negative weights.\nI dive a bit into the possible solutions (and may reiterated the problems) in other posts. For now, hope you enjoy this.\nComments and questions welcome."
  },
  {
    "objectID": "app_metrics/app_metrics3.html#references",
    "href": "app_metrics/app_metrics3.html#references",
    "title": "DID: The Fall",
    "section": "References",
    "text": "References\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 254–77. https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nWooldridge, Jeffrey M. 2021. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” {SSRN} {Scholarly} {Paper}. Rochester, NY. https://doi.org/10.2139/ssrn.3906345."
  },
  {
    "objectID": "app_metrics/app_metrics1.html",
    "href": "app_metrics/app_metrics1.html",
    "title": "Constructing synthetic Datasets",
    "section": "",
    "text": "In my current collaboration with Stephen Jenkins, we are grappling with the challenge of providing a self-contained replication package alongside our paper.\nIt’s relatively easy to share the code for our model estimations, including code developed by other authors. However, many researchers face the same challenge we do: how to distribute data that we’re not allowed to share due to privacy or proprietary reasons.\nIn fact, for this particular project, only Stephen has had access to the data. I’ve mainly worked on the code that estimates new models (for those interested, see references below).\nNow that it’s time to publish our “big” paper, we need a strategy to create a synthetic dataset that satisfies privacy protection constraints while still preserving the moments’ structure we care about, as well as those that others may find interesting.\nTo this end, I propose a simple strategy that could work: Multiple Imputation. While it may not be the best method available, I welcome any feedback or suggestions.\nTo explain how the method works, I’ll use the Swiss Labor Market Survey 1998 dataset, which is publicly available and used as an example dataset in the command -oaxaca- (Jann 2008)."
  },
  {
    "objectID": "app_metrics/app_metrics1.html#the-problem",
    "href": "app_metrics/app_metrics1.html#the-problem",
    "title": "Constructing synthetic Datasets",
    "section": "The Problem",
    "text": "The Problem\nAssume you signed a confidentiality agreement to work with Swiss Survey data and are ready to submit your work. However, you are required to provide a replication package with a code to produce the tables and the dataset itself. Since you cannot share the original data, you suggest generating 5 synthetic datasets instead. By doing so, people can apply your code and reach similar conclusions to your main paper, but with the advantage that the data is simulated, thus fulfilling privacy concerns.\nHere is a piece of code that can be used for that:\n\n\nCode\nframe reset\nset linesize 255\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\nmisstable summarize\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\n                                                               Obs<.\n                                                +------------------------------\n               |                                | Unique\n      Variable |     Obs=.     Obs>.     Obs<.  | values        Min         Max\n  -------------+--------------------------------+------------------------------\n        lnwage |       213               1,434  |   >500    .507681    5.259097\n         exper |       213               1,434  |   >500          0    49.16667\n        tenure |       213               1,434  |    323          0    44.83333\n          isco |       213               1,434  |      9          1           9\n  -----------------------------------------------------------------------------\n\n\nFour variables (Wages, tenure, experience, and ISCO) have missing data when lfp=0 (people are not working)."
  },
  {
    "objectID": "app_metrics/app_metrics1.html#the-solution",
    "href": "app_metrics/app_metrics1.html#the-solution",
    "title": "Constructing synthetic Datasets",
    "section": "The solution",
    "text": "The solution\nThe first step is to decide on the size of the synthetic dataset. You could create a dataset with the same number of observations or adjust it to your desired sample size. I will expand the dataset to double the size, tag the new observations and make all variables, except for lfp, missing. This is because some data is missing as it’s only available for those in the labor force. Alternatively, you could have created lfp using a random draw from a Bernoulli distribution with the same probability as the original data.\n\n\nCode\nexpand 2, gen(tag)\nforeach i of varlist lnwage educ exper tenure isco female age single married divorced kids6 kids714 wt {\n  qui:replace `i'=. if tag==1\n}\n\n\n(1,647 observations created)\n\n\nNext, create multiple imputed datasets using the predictive mean matching strategy. To do this, set the data and register all variables to be imputed. Then, impute all variables using chain pmm. Make sure none of the variables are collinear, and variables with structural missing data are specified separately. The only explanatory variable or exogenous variable here is lfp.\n\n\nCode\nmi set wide\nmi register impute lnwage educ exper tenure isco female age single married kids6 kids714 wt\nset seed 101\nqui:mi impute chain (pmm, knn(20))  educ female age single married kids6 kids714 wt (pmm if lfp==1, knn(20) ) lnwage  exper tenure isco  = lfp, add(5)\n\n\nYou now have 5 sets of variables that can be used to create unique synthetic datasets with a similar structure to the original confidential dataset. Let’s now put the newly created data into frames, so we can estimate few models and compare them with the original data.\n\n\nCode\nforvalues i = 1/5 {\n  frame put _`i'_* lfp if tag==1, into(fr`i')\n  frame fr`i':ren _`i'_* *\n}\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)"
  },
  {
    "objectID": "app_metrics/app_metrics1.html#comparing-results",
    "href": "app_metrics/app_metrics1.html#comparing-results",
    "title": "Constructing synthetic Datasets",
    "section": "Comparing Results",
    "text": "Comparing Results\nYou can now estimate 4 models using the original data and the synthetic data.\nNow lets compare the models:\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.085***\n0.076***\n0.059***\n0.077***\n0.069***\n0.063***\n\n\n\n(0.005)\n(0.005)\n(0.006)\n(0.005)\n(0.005)\n(0.005)\n\n\nexper\n0.011***\n0.010***\n0.011***\n0.012***\n0.006***\n0.009***\n\n\n\n(0.002)\n(0.001)\n(0.002)\n(0.002)\n(0.001)\n(0.001)\n\n\ntenure\n0.008***\n0.008***\n0.002\n0.005**\n0.007***\n0.005**\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\nfemale\n-0.084***\n-0.027\n-0.136***\n-0.063*\n-0.056*\n-0.113***\n\n\n\n(0.025)\n(0.025)\n(0.026)\n(0.025)\n(0.023)\n(0.024)\n\n\n_cons\n2.213***\n2.297***\n2.580***\n2.336***\n2.464***\n2.529***\n\n\n\n(0.068)\n(0.068)\n(0.074)\n(0.070)\n(0.063)\n(0.064)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p < 0.05, ** p < 0.01, *** p < 0.001\nQuantile Regression 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.103***\n0.088***\n0.069***\n0.083***\n0.075***\n0.067***\n\n\n\n(0.017)\n(0.015)\n(0.018)\n(0.011)\n(0.011)\n(0.013)\n\n\nexper\n0.020***\n0.012**\n0.014**\n0.012***\n0.008*\n0.009*\n\n\n\n(0.005)\n(0.004)\n(0.005)\n(0.003)\n(0.003)\n(0.004)\n\n\ntenure\n0.001\n0.006\n0.004\n0.002\n0.006\n0.010*\n\n\n\n(0.006)\n(0.005)\n(0.006)\n(0.004)\n(0.004)\n(0.005)\n\n\nfemale\n-0.151\n0.022\n-0.128\n-0.019\n-0.161**\n-0.154*\n\n\n\n(0.081)\n(0.070)\n(0.079)\n(0.053)\n(0.054)\n(0.063)\n\n\n_cons\n1.462***\n1.681***\n1.939***\n1.835***\n1.971***\n1.994***\n\n\n\n(0.219)\n(0.193)\n(0.228)\n(0.149)\n(0.146)\n(0.171)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p < 0.05, ** p < 0.01, *** p < 0.001\nQuantile Regression 90\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.064***\n0.073***\n0.047***\n0.069***\n0.062***\n0.057***\n\n\n\n(0.009)\n(0.008)\n(0.009)\n(0.008)\n(0.008)\n(0.007)\n\n\nexper\n0.004\n0.009***\n0.009***\n0.011***\n0.003\n0.005**\n\n\n\n(0.003)\n(0.002)\n(0.003)\n(0.002)\n(0.002)\n(0.002)\n\n\ntenure\n0.008*\n0.009***\n-0.001\n0.008**\n0.012***\n0.005\n\n\n\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n\n\nfemale\n-0.054\n-0.054\n-0.149***\n-0.052\n-0.009\n-0.106**\n\n\n\n(0.044)\n(0.035)\n(0.041)\n(0.039)\n(0.039)\n(0.033)\n\n\n_cons\n2.984***\n2.804***\n3.247***\n2.863***\n2.990***\n3.121***\n\n\n\n(0.119)\n(0.097)\n(0.118)\n(0.111)\n(0.106)\n(0.089)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p < 0.05, ** p < 0.01, *** p < 0.001\nHeckman selection model\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\nlnwage\n\n\n\n\n\n\n\n\neduc\n0.072***\n0.066***\n0.052***\n0.068***\n0.059***\n0.057***\n\n\n\n(0.005)\n(0.005)\n(0.006)\n(0.005)\n(0.005)\n(0.005)\n\n\nexper\n0.002\n0.001\n-0.000\n0.004*\n-0.002\n0.002\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\ntenure\n0.002\n0.003\n-0.002\n-0.000\n0.002\n0.001\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\nfemale\n-0.105***\n-0.067**\n-0.189***\n-0.105***\n-0.096***\n-0.144***\n\n\n\n(0.029)\n(0.025)\n(0.026)\n(0.025)\n(0.024)\n(0.024)\n\n\nage\n0.015***\n0.013***\n0.015***\n0.012***\n0.013***\n0.011***\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\n_cons\n1.991***\n2.071***\n2.226***\n2.133***\n2.201***\n2.317***\n\n\n\n(0.073)\n(0.073)\n(0.081)\n(0.075)\n(0.069)\n(0.070)\n\n\nlfp\n\n\n\n\n\n\n\n\neduc\n0.149***\n0.055**\n0.024\n0.018\n0.064***\n0.028\n\n\n\n(0.028)\n(0.020)\n(0.020)\n(0.019)\n(0.019)\n(0.018)\n\n\nfemale\n-1.785***\n-0.074\n-0.246**\n-0.104\n-0.011\n-0.177\n\n\n\n(0.161)\n(0.091)\n(0.091)\n(0.088)\n(0.090)\n(0.090)\n\n\nage\n-0.039***\n-0.021***\n-0.023***\n-0.018***\n-0.025***\n-0.023***\n\n\n\n(0.007)\n(0.005)\n(0.005)\n(0.005)\n(0.005)\n(0.005)\n\n\nsingle\n-0.100\n-0.792**\n-0.586**\n-0.498**\n-0.863***\n-0.780**\n\n\n\n(0.231)\n(0.241)\n(0.201)\n(0.186)\n(0.229)\n(0.238)\n\n\nmarried\n-0.867***\n-0.929***\n-0.765***\n-0.489**\n-0.882***\n-1.017***\n\n\n\n(0.158)\n(0.219)\n(0.169)\n(0.160)\n(0.198)\n(0.213)\n\n\nkids6\n-0.716***\n-0.730***\n-0.648***\n-0.699***\n-0.790***\n-0.714***\n\n\n\n(0.082)\n(0.067)\n(0.064)\n(0.066)\n(0.069)\n(0.062)\n\n\nkids714\n-0.343***\n-0.378***\n-0.373***\n-0.281***\n-0.402***\n-0.201***\n\n\n\n(0.065)\n(0.059)\n(0.056)\n(0.058)\n(0.058)\n(0.058)\n\n\n_cons\n3.543***\n2.720***\n3.006***\n2.560***\n2.729***\n3.135***\n\n\n\n(0.486)\n(0.382)\n(0.388)\n(0.345)\n(0.383)\n(0.383)\n\n\n/mills\n\n\n\n\n\n\n\n\nlambda\n-0.123\n0.128*\n0.251***\n0.062\n0.206***\n0.077\n\n\n\n(0.065)\n(0.061)\n(0.065)\n(0.070)\n(0.057)\n(0.061)\n\n\nN\n1647\n1647\n1647\n1647\n1647\n1647\n\n\n\nStandard errors in parentheses * p < 0.05, ** p < 0.01, *** p < 0.001\nI wont spend too much time interpretting the models. However, it is important to notice that they do provide similar outcomes, except for quantile regressions. Still, with this, one could redistribute replication codes that use both the true data and synthetic data, providing transparency to the work."
  },
  {
    "objectID": "app_metrics/app_metrics1.html#conclusions",
    "href": "app_metrics/app_metrics1.html#conclusions",
    "title": "Constructing synthetic Datasets",
    "section": "Conclusions",
    "text": "Conclusions\nAs evident from the analysis, the results of the synthetic dataset are not expected to perfectly replicate the original data due to the introduction of random errors. However, by keeping this in mind, we can create synthetic datasets like this one, along with two sets of results - one based on the actual data, and the other based on the synthetic dataset(s).\nThis should help providing replication packages with code and data, improving the transparency of research when using restricted data."
  },
  {
    "objectID": "app_metrics/app_metrics1.html#references",
    "href": "app_metrics/app_metrics1.html#references",
    "title": "Constructing synthetic Datasets",
    "section": "References",
    "text": "References\n\n\nJann, Ben. 2008. “The Blinder–Oaxaca Decomposition for Linear Regression Models.” The Stata Journal: Promoting Communications on Statistics and Stata 8 (4): 453–79. https://doi.org/10.1177/1536867X0800800401.\n\n\nJenkins, Stephen P., and Fernando Rios-Avila. 2020. “Modelling Errors in Survey and Administrative Data on Employment Earnings: Sensitivity to the Fraction Assumed to Have Error-Free Earnings.” Economics Letters 192 (July): 109253. https://doi.org/10.1016/j.econlet.2020.109253.\n\n\n———. 2021. “Measurement Error in Earnings Data: Replication of Meijer, Rohwedder, and Wansbeek’s Mixture Model Approach to Combining Survey and Register Data.” Journal of Applied Econometrics 36 (4): 474–83. https://doi.org/10.1002/jae.2811.\n\n\nJenkins, SP, Rios‐Avila, F, 2021. “Measurement error in earnings data: replication of Meijer, Rohwedder, and Wansbeek’s mixture model approach to combining survey and register data.” J Appl Econ. 2021. Accepted Author Manuscript. https://doi.org/10.1002/jae.2811 (with Rep Files here)"
  },
  {
    "objectID": "app_metrics/index.html",
    "href": "app_metrics/index.html",
    "title": "Applied Econometrics",
    "section": "",
    "text": "Welcome to this section of my webpage! As you will read, this section is dedicated to providing techniques and strategies for selected applied econometrics strategies. Econometrics is the branch of economics that deals with the application of statistical and mathematical methods to analyze economic data, answering real-world questions and make predictions about economic phenomena. My goal here, however, is more humble. I aim to discuss few methodologies (as they come to mind or to work), to deal with tasks on my own research, but questions about other methods are always welcome.\nWhether you are a student, researcher, or practitioner, I hope this site provides you with useful insights and tools for conducting empirical research in economics.\nComments are always welcomed!\nFernando"
  },
  {
    "objectID": "app_metrics/app_metrics4.html",
    "href": "app_metrics/app_metrics4.html",
    "title": "DID: The Revolution",
    "section": "",
    "text": "If you have been keeping up with the advancements and revolution in Difference-in-Differences (DID), you may be aware of the issues associated with using the simple Two-Way Fixed Effects (TWFE) method for identifying treatment effects. If you haven’t, you can refer to my previous post titled DID: The fall where I have explained the problem.\nThe issue can be summarized as follows:\nWhen multiple periods and groups are available, and the treatment has been implemented over several periods, the standard TWFE method is unlikely to identify the average treatment effects. This is because it tends to use inadequate controls, gives negative weights to treated units, and generally suffers from contamination that leads to biased estimates.\nAs far as I know, the following model can only estimate average treatment effects if treatment effects remain constant over time: \\[\ny_{it} = a_i + a_t + \\delta D_{it} + e_{it}\n\\]\nwhere \\(D_{it}=1\\) if a unit is effectively treated, \\(a_i\\) represents the unit fixed effects, and \\(a_t\\) time fixed effects."
  },
  {
    "objectID": "app_metrics/app_metrics4.html#solutions",
    "href": "app_metrics/app_metrics4.html#solutions",
    "title": "DID: The Revolution",
    "section": "Solutions",
    "text": "Solutions\nDespite the challenges previously discussed, there is still hope for the successful implementation of DID. Numerous scholars have proposed various strategies that specifically address the issues at hand, as hinted in DID: the Fall.\nIn this context, I will provide a brief overview of three such solutions, two of which I am actively involved in, and one that is relatively straightforward to comprehend and articulate.\n\nSetup\nLet us establish some fundamental nomenclature before we delve into the solution. This terminology will help understand and implement the solutions we will cover next.\nBefore we start discussing the solution, lets stablish some basic nomenclature that may help us understand and implement the solutions we will be discussing later on.\n\nFor simplicity, we will focus on the case of balanced panel data. All units \\(i\\) will be observed for \\(T\\) periods, from \\(t=1\\) to \\(t=T\\).\nAt any given time, individual outcomes are defined as follows: \\[\ny_{it} = a_i + a_t + \\delta_{it}*D_{it} +e_{it}\n\\]\n\nwhere \\(a_i\\) is the individual fixed effect, \\(a_t\\) is a time fixed effect, and \\(e_{it}\\) is an individual level iid error. \\(D_{it}\\) is a dummy variable that takes the value of 1 after a unit is treated.\nNotice that here, the treatment effect fully heterogeneous.\n\nOnce a unit has been treated, it remains so, even if the effect eventually fades away. (if \\(D_{it}=1 \\rightarrow D_{is}=1 \\ \\forall \\ s>t\\))\nThere is variation in treatment timing, and as a result, the treatment dummy is not activated for everyone at the same time.\nAssume there are no controls.\n\nUnder this conditions, lets simulate a simple dataset with this characteristics:\n\nclear\nset linesize 250\nset seed     111\nset sortseed 111\nset obs 100  // <- 100 units\ngen id = _n\ngen ai = rchi2(2)\n// determines When would units receive treatment\ngen     g = runiformint(2,10)\nreplace g = 0 if g>9   // never treated       \nexpand 10   // <-T=10\nbysort id:gen t=_n \ngen event = max(0,t-g)\ngen aux = runiform()*2\nbysort t:gen at = aux[1] // Determines Time fixed effect\ngen te = (1-g/10)+(1-event/10)  \n// Treatment effect but vanishes with time\ngen eit= rnormal()\ngen trt  = (t>=g)*(g>0)\ngen teff = te * trt \ngen y = ai + at + te * trt + eit\n\nNumber of observations (_N) was 0, now 100.\n(18 real changes made)\n(900 observations created)\n\n\nThe simulated data will follow 100 units over a span of 10 periods. Each unit is assigned to receive treatment at some point between periods 2 and 9, while 1/10 of the sample is not treated at all.\nThe impact of the treatment depends on two factors: the timing of treatment and the duration of treatment. The treatment effect becomes smaller the later a unit is treated, and as a unit is treated for longer durations.\n\n\nTwo-Steps DID\n\nGardner (2022) did2s & Borusyak, Jaravel, and Spiess (2022) did_imputation\nLet’s start by desribing one of the solutions solutions: the two-step DID, also referred to as the imputation approach. This strategy is explained and explored by two notable papers: Gardner (2022) and Borusyak, Jaravel, and Spiess (2022).\nAs the name implies, this approach aims to estimate treatment effects in a design with multiple groups treated at various points in time by imputing the values of what would have occurred to those units if they were never subjected to treatment. To accomplish this, the authors suggest breaking down the identification problem into two steps to avoid the erroneous use of already treated units as controls.\nIn the first step, one should only use units that have never been treated or have not yet been treated to model potential outcomes under the no-treatment scenario.\n\\[\ny_{it} = a_i + a_t + e_{it} \\ if \\ t<g\n\\]\nDoing this, the fixed effects \\(a_i\\) and \\(a_t\\) would not be contaminated by the treatment, because non of the units in the sample are treated.\nIn the second step, individual-level treatment effects predictions can be obtained by simply computing the difference between the observed outcome under treatment and the prediction of the first step (\\(\\hat y_{it} = \\hat a_i + \\hat a _t\\)), which represent the potential outcomes of those units if no treatment occured.\n\\[\n\\delta_{it} = y_{it}-\\hat a_i - \\hat a_t\n\\]\nBecause unit level treatment effects are not useful for statistical inference, one can use aggregations to obtain standard errors or aggregates we are more familiar with: Average treatment effect on treated, dynamic effects, etc.\nTo correctly estimate standard errors, both steps should be estimated simulatenously, so that the estimation error from the first stage can propagate to the second stage. While you can use did2s or did_imputation, I will show you the code using gmm, so you can see how the whole system would work.\nIn the following code line (1) identifies the individual and time FE using only notyet treated units (trt=0). Line 2 uses tha to substract from \\(y\\) and identify the treatment effect (ATT). Lines 3 and 4 provides information to gmm to estimate the model and standard errors:\n\n\nCode\ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0))     /// \n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:trt})) , /// \n    winitial(identity)  instruments(1:i.g i.t) instruments(2: trt) /// \n    onestep quickderivatives vce(cluster i)  \n\n\n\nStep 1\n\n\nIteration 0:   GMM criterion Q(b) =  19.725543  \n\n\nIteration 1:   GMM criterion Q(b) =  5.816e-23  \n\n\nIteration 2:   GMM criterion Q(b) =  9.081e-33  \n\n\n\nGMM estimation \n\nNumber of parameters =  19\nNumber of moments    =  20\nInitial weight matrix: Identity                   Number of obs   =      1,000\n\n\n\n                                   (Std. err. adjusted for 100 clusters in id)\n------------------------------------------------------------------------------\n\n\n             |               Robust\n\n\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n\n\n-------------+----------------------------------------------------------------\na0           |\n\n\n       _cons |\n\n\n   3.146414   .6822892     4.61   0.000     1.809152    4.483677\n-------------+----------------------------------------------------------------\na_i          |\n           g |\n          2  |\n\n\n  -1.329894   .9540798    -1.39   0.163    -3.199856    .5400685\n          3  |   .3303121   .9832247     0.34   0.737    -1.596773    2.257397\n          4  |  -.9533545   .8233816    -1.16   0.247    -2.567153    .6604439\n          5  |  -.9570973   .8366389    -1.14   0.253    -2.596879    .6826848\n          6  |    -.58129   .8707363    -0.67   0.504    -2.287902    1.125322\n          7  |   .9916895   1.010794     0.98   0.327    -.9894297    2.972809\n          8  |  -1.609291   .7437976    -2.16   0.030    -3.067108   -.1514746\n          9  |  -1.160353   1.033179    -1.12   0.261    -3.185346    .8646411\n-------------+----------------------------------------------------------------\na_t          |\n           t |\n          2  |  -.0585438   .1385197    -0.42   0.673    -.3300374    .2129498\n          3  |   -.213678   .1704212    -1.25   0.210    -.5476975    .1203415\n          4  |  -.5752045   .1713895    -3.36   0.001    -.9111217   -.2392872\n          5  |   .0842171   .1677747     0.50   0.616    -.2446152    .4130494\n          6  |   .1190078   .1841714     0.65   0.518    -.2419615    .4799771\n          7  |   1.000738   .2034229     4.92   0.000     .6020363    1.399439\n          8  |  -.0146057   .2190799    -0.07   0.947    -.4439945    .4147831\n          9  |   .9380065   .2007013     4.67   0.000     .5446391    1.331374\n         10  |   .9572597   .2578496     3.71   0.000     .4518838    1.462636\n-------------+----------------------------------------------------------------\natt          |\n         trt |   1.458058   .1493165     9.76   0.000     1.165403    1.750713\n------------------------------------------------------------------------------\n\n\nInstruments for equation 1: 0b.g 2.g 3.g 4.g 5.g 6.g 7.g 8.g 9.g 1b.t 2.t 3.t 4.t 5.t 6.t 7.t 8.t 9.t 10.t _cons\nInstruments for equation 2: trt _cons\n\n\nYou can compare this to the true effect:\n\n\nCode\nsum teff if trt==1\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |        472    1.280297    .2217799          1        1.8\n\n\nOne advantage of this approach is that you could model the second stage to allow other types of aggregations:\ngen g0=g*trt    \ngen t0=t*trt\ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0)) ///\n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:i.g0})) , ///\n    winitial(identity)  instruments(1:i.g i.t) instruments(1: i.g0) ///\n    onestep quickderivatives vce(cluster i) \ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0)) ///\n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:i.t0})) , ///\n    winitial(identity)  instruments(1:i.g i.t) instruments(2: i.t0) ///\n    onestep quickderivatives vce(cluster i)             \n\n\n\nYou don’t messup with OLS\n\nWooldridge (2021) jwdid and Sun and Abraham (2021)\nIn the article DID: the fall, it was pointed out that the conventional TWFE approach has faced significant backlash due to its limited ability to detect treatment effects, because it cannot distinguish between good and bad variation when estimating treatment effects. Despite this criticism, Professor Wooldridge came in defense and revitalized the approach by emphasizing its simplicity and versatility, enabling extensions to go beyond linear models.\nThe message was simple:\n\nAlthough the conventional TWFE method has several shortcomings, if it is implemented correctly, it can overcome the issue of utilizing inadequate controls in estimation. As a result, it can estimate treatment effects efficiently, with results similar to Borusyak, Jaravel, and Spiess (2022) and Gardner (2022).\n\nSo what were we missing? Heterogeneity!\nBoth Wooldridge (2021) and Sun and Abraham (2021) have proposed similar solutions to the problem at hand, albeit from different viewpoints. Sun and Abraham (2021) argued that utilizing event studies with leads and lags may lead to contaminated coefficients, thus hampering proper identification of dynamic treatment effects. As a potential solution, the authors suggests using a specification that estimates dynamic effects for each cohort before making aggregations.\nOn the other hand, Wooldridge (2021) focused on identifying treatment effects. He recommends allowing all treatment effects to vary by cohort and time. In other words, instead of employing a single dummy variable to identify treated units, he suggests using a set of dummies representing the interaction of cohorts and periods.\nSpecifically, Wooldridge (2021) proposes estimating a model with the following specification:\n\\[\ny_{it} = a_i + a_t + \\sum_{g=g_0}^G \\sum_{t=g}^T \\delta_{gt} * 1(g,t)+e_{it}\n\\]\nWhat Wooldridge suggests, at least for the case without covariates, is to estimate a model where, in addition to the individual (or cohort) and time fixed effects, we saturate all possible combinations of cohorts and times (\\(1(g,t)\\)), if that combination corresponds to an effectively treated unit (\\(t\\geq g\\)). This specification essentially uses all not-yet treated units as controls, similar to the two-step approach.\nThis specification, however, could also be extended to a case where only those never treated are used as controls. In this case, one should use all cohort and time interactions, including the cases before units were treated. Following convention, one should exclude from the list of interactions the period before treatment took place:\n\\[\ny_{it} = a_i + a_t + \\sum_{g=g_0}^G \\sum_{t\\neq g-1} \\delta_{gt} * 1(g,t)+e_{it}\n\\]\nBy saturating the model this way, each \\(\\delta_{gt}\\) represents a treatment effect for group G at time T, which can be later aggregated to obtain dynamic, group, or average treatment effects. Also interesting to note that this second specification is essentially the same Sun and Abraham (2021) propose for dynamic effects, and produces results that are identical to the methodology proposed by Callaway and Sant’Anna (2021).\nNow for the application, I will use jwdid for both specifications. Because by default jwdid produces the fully saturated model, I will omit those results, showing only the average treatment effect.\nFirst the one using not-yet treated units as controls (default), which produces exactly the same results as the two-step approach:\n\nssc install jwdid\nqui: jwdid y, ivar(i) tvar(t) gvar(g)\nestat simple\n\nchecking jwdid consistency and verifying not already installed...\n\n\nall files already exist and are up to date.\n\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n\n\n------------------------------------------------------------------------------\n\n\n             |            Delta-method\n\n\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n\n\n-------------+----------------------------------------------------------------\n         _at |\n   (2 vs 1)  |\n\n\n   1.458058   .1494798     9.75   0.000     1.165083    1.751033\n\n\n------------------------------------------------------------------------------\n\n\nSecond the one with never treated units as controls:\n\nqui: jwdid y, ivar(i) tvar(t) gvar(g) never\nestat simple\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n   (2 vs 1)  |   1.543749   .1567784     9.85   0.000     1.236469    1.851029\n------------------------------------------------------------------------------\n\n\n\n\n\n2x2 in Steroids\n\nCallaway and Sant’Anna (2021) csdid & csdid2\nThe third option is the most computing intensive, but at the same time simpler to understand, if you break it down to the basics. This is why I call this 2x2 in Steroids: Callaway and Sant’Anna (2021).\nThe literature on the 2x2 DID design has been extensively explored and extended, and it appears that most of the criticisms of the TWFE method do not apply to this simple design. Although there are a few technical details to consider while estimating ATT’s, most of the information required can be found in Sant’Anna and Zhao (2020). In this work, the authors provide several options for obtaining the best estimate from a simple 2x2 DID design.\nAssuming that we know how to execute 2x2 DID correctly (which can be achieved using drdid in Stata), Callaway and Sant’Anna (2021) propose that we focus on estimating all the good and useful 2x2 DID designs from our data while avoiding incorrect comparisons. These are the building blocks of the methodology, the ATTGTs. This are the average treatment effects on the treated for units treated for the first time in period G, but measured at period T.\nThis process, however, could be time-consuming and computationally intensive if done manually, as the number of 2x2 designs increases with the number of cohorts and periods available in the data. For example, estimating 50 different ATTs would be necessary with 5 cohorts and 10 periods, and up to 5 separate models need to be estimated for each ATT.\nBorrowing from the nomenclature in Callaway and Sant’Anna (2021), this ATTGT’s are defined as follows: \\[\n\\begin{aligned}\n    ATT(g,t) &= E(Y_{i,t}|i\\in g) - E(Y_{i,g-1}|i\\in g) \\\\\n             & -\\Big[ E(Y_{i,t}|i\\in C) - E(Y_{i,g-1}|i\\in C) \\Big]\n\\end{aligned}\n\\]\nWhich has the same structure as the simple 2x2 DID, with the difference that the control group \\(C\\) will be formed by never treated units only, or include those not yet treated ( \\(i \\in g_i, g_i>t, \\And \\  g_i>g\\)).1\nOnce all individual ATTGT’s have been identified and estimated, providing summary measures we are more familiar with is as easy as obtaining weighted averages:\n\\[\nSATT = \\sum \\left( \\frac{w_{gt} }{\\sum w_{gt}}ATT(g,t) \\right)\n\\]\nwhere the weights \\(w_{gt}\\) will change based on the type of aggregation one is interested in.\nThis multiple stage process may seem challening, but there are tools that allow you to implement them quite easily: csdid and csdid2. The first one, was written as a wrapper around drdid, to do all the heavy lifting for you. However, for large projects, it can be slow due to overhead. The alternative csdid2 is self contained, still under development, but much faster than the predecesor. See csdid2 if interested. Here I will use csdid, which you can get from SSC.\nAs with jwdid. The default option is for csdid to produce all ATTGT’s. So, for the excercise, I’ll omit that ouput, and estimate the aggregate effects with the post estimation command. The default is to use the never treated as controls. I will also add the option long2 to obtain the pre-treatment ATTGT’s as describe above, even though they won’t affect our point estimates:\n\nqui: ssc install drdid, replace\nqui: ssc install csdid, replace\nqui: csdid y, ivar(i) time(t) gvar(g) long2\nestat simple \n\nAverage Treatment Effect on Treated\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ATT |   1.543749   .1577366     9.79   0.000     1.234591    1.852907\n------------------------------------------------------------------------------\n\n\nI will also use the not yet treated, to compare results.\n\nqui: csdid y, ivar(i) time(t) gvar(g) notyet long2\nestat simple \n\nAverage Treatment Effect on Treated\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ATT |   1.494475   .1587386     9.41   0.000     1.183353    1.805597\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "app_metrics/app_metrics4.html#conclusions",
    "href": "app_metrics/app_metrics4.html#conclusions",
    "title": "DID: The Revolution",
    "section": "Conclusions",
    "text": "Conclusions\nOn this occasion, I have shared with you three solutions from the literature that aim to overcome the limitations of TWFE. Although there are other solutions available, I personally find these three to be the most intuitive and have worked on them. Granted, I have some bias on the matter.\nDespite their apparent differences, these solutions actually converge towards similar outcomes, with discrepancies arising from variations in assumptions regarding control groups or covariate management.\n\n\n\ncommand\neq command\n\n\n\n\njwdid\ndid2s & did_imputation\n\n\njwdid, never\neventstudyinteract\n\n\njwdid, never\ncsdid, long2 or csdid2, long"
  },
  {
    "objectID": "app_metrics/app_metrics4.html#references",
    "href": "app_metrics/app_metrics4.html#references",
    "title": "DID: The Revolution",
    "section": "References",
    "text": "References\n\n\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess. 2022. “Revisiting Event Study Designs: Robust and Efficient Estimation.” arXiv. https://doi.org/10.48550/arXiv.2108.12419.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nGardner, John. 2022. “Two-Stage Differences in Differences.” arXiv. https://doi.org/10.48550/arXiv.2207.05943.\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/10.1016/j.jeconom.2020.06.003.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2021. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” {SSRN} {Scholarly} {Paper}. Rochester, NY. https://doi.org/10.2139/ssrn.3906345."
  },
  {
    "objectID": "app_metrics/app_metrics5.html#simple-2x2-did",
    "href": "app_metrics/app_metrics5.html#simple-2x2-did",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Simple 2x2 DID",
    "text": "Simple 2x2 DID"
  },
  {
    "objectID": "app_metrics/app_metrics5.html#level-up-adding-controls",
    "href": "app_metrics/app_metrics5.html#level-up-adding-controls",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Level up: Adding controls",
    "text": "Level up: Adding controls"
  },
  {
    "objectID": "app_metrics/app_metrics5.html#going-doubly-robust",
    "href": "app_metrics/app_metrics5.html#going-doubly-robust",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Going Doubly Robust",
    "text": "Going Doubly Robust"
  },
  {
    "objectID": "app_metrics/app_metrics5.html#drdid-the-stata-way",
    "href": "app_metrics/app_metrics5.html#drdid-the-stata-way",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "drdid: The Stata way",
    "text": "drdid: The Stata way"
  },
  {
    "objectID": "app_metrics/app_metrics5.html#references",
    "href": "app_metrics/app_metrics5.html#references",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "References",
    "text": "References\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/10.1016/j.jeconom.2020.06.003."
  },
  {
    "objectID": "stataviz/stataviz1.html",
    "href": "stataviz/stataviz1.html",
    "title": "How to add Colors to your Life",
    "section": "",
    "text": "This is the way\nI should start by saying that what I will be presenting here is not the only way. There are other ways to create and modify your schemes however you would like to do. Most people provide you with the full set of schemes for that purpose.\nThat being said, the command I’ll be presenting depends on a few tools built by Ben Jann: colorpalette, colorspace, as well as grstyle.\nIt also has only 1 purpose. Change the color settings from your current “scheme” to any other palette either available in colorpalette, or provided by the program.\nSo let me start"
  },
  {
    "objectID": "stataviz/stataviz1.html#so-how-does-it-work",
    "href": "stataviz/stataviz1.html#so-how-does-it-work",
    "title": "How to add Colors to your Life",
    "section": "So how does it work?",
    "text": "So how does it work?\nAs I mentioned earlier, there are two very useful programs that mine builds on. These are colorpalette and grstyle, both by Ben Jann.\n\ncolorpalette helps create colors, call on palettes, modify properties, and translate them in a way that Stata will understand.\ngrstyle helps modifying the current scheme, so all graphs done afterward will use the modify schemes."
  },
  {
    "objectID": "stataviz/stataviz1.html#where-does-mine-fit",
    "href": "stataviz/stataviz1.html#where-does-mine-fit",
    "title": "How to add Colors to your Life",
    "section": "Where does mine fit?",
    "text": "Where does mine fit?\nMy program color_style is in fact a wrapper around colorpalette and grstyle. You can use it to call on a particular color palette, and will automatically apply it into your scheme.\nIt also brings some other colorpalettes that can be stored in memory, and applied to your graphs, but enough talk, lets see how it works!\nFirst, setup. make sure you have the following programs installed in your computer\nssc install palettes\nssc install colrspace  \nssc install grstyle\n\n* And of course my program\n\nssc install color_style \nLets start using default options for something simple. A pie Graph.\n\n\nCode\nset scheme white2\nsysuse auto, clear\nset scheme s2color\ngraph pie mpg, over(rep78)\n\n\n(1978 automobile data)\n\n\n\n\n\nThis creates a pie graph with average MPG’s by number of repairs in 1978. But what I didn’t like default colors, you can request a change:\n\n\nCode\ncolor_style viridis, n(5)\ngraph pie mpg, over(rep78)\n\n\n\n\n\nHere, I’ll be using Viridis palette, but requesting only 5 colors. One can request up to 15 different colors."
  },
  {
    "objectID": "stataviz/stataviz1.html#i-want-more",
    "href": "stataviz/stataviz1.html#i-want-more",
    "title": "How to add Colors to your Life",
    "section": "I want more!",
    "text": "I want more!\nBut every body uses Viridis, what other palette can I use?\nYou can look into the helpfile of colorpalette for a large set of colors. But, color_style also comes with its own set!. Just type: color_style, list, and a list of additional palettes will come up.\nBut you want to see examples! Below, you can see three examples, first using egypt color palette, second using tableau, and third using your own colors\n\n\nCode\ncolor_style egypt,\ngraph pie mpg, over(rep78) name(m1, replace)\n\n**\ncolor_style tableau\ngraph pie mpg, over(rep78) name(m2, replace)\n\n** or your own\ncolor_style #01161E #124559 #598392 #AEC3B0 #EFF6E0\ngraph pie mpg, over(rep78) name(m3, replace)"
  },
  {
    "objectID": "stataviz/stataviz1.html#how-does-it-work",
    "href": "stataviz/stataviz1.html#how-does-it-work",
    "title": "How to add Colors to your Life",
    "section": "How does it work?",
    "text": "How does it work?\nNow that you know the magic, it is fair to know the trick.\nThis command color_style works as a wrapper behind colorpalette'. It basically passes the information to this program to translate and create colors from either palettes, or directly provided colors, so it translates into RGB color code thatStata` understands.\nAfter that, grstyle uses this information to change the colors of all graphic related styles (p1-p15), which is why there is a limitation of how many colors one can define using this method.\nIn any case, once you decide you like certain scheme, this small program should help to make it easy change colors, based on your needs."
  },
  {
    "objectID": "stataviz/index.html",
    "href": "stataviz/index.html",
    "title": "Stata Vizualizations",
    "section": "",
    "text": "Welcome to the data visualization section, where I explore the power of data visualization in Stata. Effective data visualization is essential for conveying insights and communicating results to diverse audiences.\nWhile Stata is a statistical software first, graphics software second, it does offer sufficient flexiblity, tools and options for creating informative data visualizations, although not without extra work.\nOn this webpage, I will compile a collection Stata programs that I have written for data visualization that will help you create visually appealing graphics for your research projects. I hope you will find these codes helpful in creating informative charts, histograms, scatterplots, and more.\nAs a data visualization enthusiast and Stata user, I am passionate about sharing my knowledge and skills with others. I believe that effective data visualization is crucial in conveying research findings and engaging audiences, but most importantly, programming them allows me to sharpen my skills!\nComments and suggestions are welcome!"
  },
  {
    "objectID": "stataviz/stataviz2.html",
    "href": "stataviz/stataviz2.html",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "",
    "text": "Quantile regressions are useful statistical tools that allow researchers to analyze the relationships between dependent and independent variables beyond their conditional means. To analyze such models, researchers commonly plot the estimation model coefficients across all estimated quantiles. This approach enables researchers to describe a larger set of coefficients, providing visuals of the trends and differences across the distribution.\nPreviously, the only command available to create distributional plots of this type was grqreg. However, this program had two limitations.\n\nIt does not support factor notation.\nIt could only be used after the official commands qreg, sqreg, and bsqreg.\n\nWith the introduction of new commands that focus on the estimation of quantile regressions, under different assumptions of strategies, there was a need for an update to the grqreg program. This update came in the form of a new command called qregplot.\nThis page aims to demonstrate how to use the qregplot command to plot coefficients from quantile regressions, which is compatible with a variety of quantile type commands."
  },
  {
    "objectID": "stataviz/stataviz2.html#the-setup",
    "href": "stataviz/stataviz2.html#the-setup",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "The Setup",
    "text": "The Setup\nTo work with the following examples, you will need to install the following commands from SSC:\nssc install qregplot, replace\nssc install mmqreg, replace\nssc install qrprocess, replace\nssc install ivqreg2, replace\nssc install qreg2, replace\nssc install xtqreg, replace\nssc install sivqr, replace\nssc install color_style, replace\nAnd, for the data, we will use a very small dataset, available from Stata datasets examples:\n\nclear all\nwebuse womenwk, clear\nset scheme white2\ncolor_style tableau"
  },
  {
    "objectID": "stataviz/stataviz2.html#how-does-it-work",
    "href": "stataviz/stataviz2.html#how-does-it-work",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "How does it work?",
    "text": "How does it work?\nAt its core, the command qregplot has a very simple structure, and its workflow can be described as follows:\n\nYou estimate a quantile regression model with the command of your choosing (if allowed in qregplot) for any quantile.\nqregplot reads all the metadata regarding the model specifictation, options, and command used from the information saved in e().\nUsing the same data structure, estimates the exactly same model using for all quantiles of interest.\nCollects all coefficients, and confidence intervals, to prepare the information for plotting.\nPlots the coefficients of the selected variables."
  },
  {
    "objectID": "stataviz/stataviz2.html#examples",
    "href": "stataviz/stataviz2.html#examples",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Examples",
    "text": "Examples\nAssume you are interested in estimating a model, where wages are a function of age education, marital status, and county of residence.\nFurthermore, lets say that we are interested in conditional quantile regressions. You could estimate this as follows:\n\nqui:qreg wage age education married i.county\n\nSince we are using quantile regressions, you want to plot some of the coefficients across the distribution for all variables but the county dummies.\nBecause I know I may have to re-edit the graphs, I’ll store the coefficients into e_qreg. So I do not need to re-estimate all quantiles again.\n\nqregplot age education married ,  /// Variables to be plotted. (Must appear in the model specification)\n         estore(e_qreg) /// Request Storing the variables in memory\n         q(5(5)95) // and indicates what quantiles to plot\n\n\n\n\nThis is a very good start. In the newest version of qregplot, I add some default options to the plot, providing a look that is visually pleasing. Notice the margins are also tight (less waiste of space).\nNevertheless, I would like to use variable labels for titles in each figure. But I dont want to re-estimate them all, but what I can do is plot “from” the stored coefficients. When doint this, I do not need to specify q(list of q's) again.\n\nqregplot age education married , ///\n        from(e_qreg) /// <- Indicates where to look for coefficients\n        label // No longer needed to add Q's and request Labels\n\n(results e_qreg are active now)\n\n\n\n\n\nIf you want, you can change the color, intensity, or transparency of the confidence intervals. But if you like what you see (I do), you can just keep it.\nNow, say that I want to see this, but in 3 cols. This would be a graph combine option. I will use the new syntax :\n\nqregplot age education married , ///\n        from(e_qreg) label ///\n        col(3) /// request 3 column with \n        ysize(5) xsize(10) // and different sizes for the graph\n\n(results e_qreg are active now)\n\n\n\n\n\nWith the new command, you can also modify the titles and costumize them as needed:\n\nqregplot age education married , ///\n        from(e_qreg) /// \n        col(2)   /// and different sizes for the graph\n        mtitles(\"Age in years o Edad en años desde 1900\" ///\n        \"Años de Educacion or Years of Education, incluye Highschool\" ///\n        \"Is Married - Esta Casado\") //<- ads long titles\n\n(results e_qreg are active now)\n\n\n\n\n\nSo, seems that the titles are too long. So lets adjust them using labelopt(options). This option tells the program to break the long titles into two lines, so there is less overlapping in the figure.\n\nqregplot age education married , ///\n    from(e_qreg) /// \n    col(2)   /// and different sizes for the graph\n    mtitles(\"Age in years o Edad en años desde 1900\" ///\n    \"Años de Educacion or Years of Education, incluye Highschool\" ///\n    \"Is Married - Esta Casado\") ///<- ads long titles\n    labelopt(lines(2)) // breaks it in two lines\n\n(results e_qreg are active now)"
  },
  {
    "objectID": "stataviz/stataviz2.html#using-other-qreg-commands",
    "href": "stataviz/stataviz2.html#using-other-qreg-commands",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Using other qreg commands",
    "text": "Using other qreg commands\nAbove I showed you the basic syntax for most options. An advantage of qregplot, however, is that you can use it for other commands as well.\nAs a side bonus, I showcase color_style, changing the colors of the different plots:\n\nqui:bsqreg wage age education married i.county, reps(25) \ncolor_style bay \nqregplot age education ,  q(5(5)95) seed(101) label title(\"BSqreg\") ///\nysize(5) xsize(12)\n\n\n\n\n\nqui:qreg2 wage age education married i.county, \ncolor_style  austria  \nqregplot age education , q(5(5)95) seed(101) label ///\n    title(\"QREG2: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\nqui:qrprocess wage age education married i.county, \ncolor_style  robert \nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"qrprocess: from SSC\")  ysize(5) xsize(12)\n\n\n\n\n\nqui:mmqreg wage age education married, abs( county)\ncolor_style derain \nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"mmqreg: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\nqui:rifhdreg wage age education married, abs( county) rif(q(50))\ncolor_style shakira\nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"rifhdreg: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\nqui:sivqr wage age education married, q(50)\ncolor_style troy\nqregplot age education,  q(5(5)95) seed(101) label ///\n    title(\"sivqr: from SSC\")  ysize(5) xsize(12)"
  },
  {
    "objectID": "stataviz/stataviz2.html#conclusions",
    "href": "stataviz/stataviz2.html#conclusions",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Conclusions",
    "text": "Conclusions\nThere you have it. A small update that makes making qreg plots easy.\nJust remember, qregplot is not a quantile regression estimator, it should be considered as a post estimation command that simply replicates what you had before.\nQuestions? comments? suggestions?\nfeel free to drop me a message."
  },
  {
    "objectID": "chatgpt.html",
    "href": "chatgpt.html",
    "title": "Fun with ChatGPT",
    "section": "",
    "text": "Tech Titans Rap Battle\n\n\n\n\n\nChat-GPT creates a rap battle between Siri, Alexa and Cortana. Who wins?\n\n\n\n\n\n\nMar 12, 2023\n\n\nChatGPT and Fernando Rios-Avila\n\n\n\n\n\n\n\n\nStata vs R: The battle\n\n\n\n\n\nChat-GPT creates a high fantasy battle between this titans\n\n\n\n\n\n\nMar 18, 2023\n\n\nChatGPT and Fernando Rios-Avila\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chatgpt/erbh_03_12_2023/index.html",
    "href": "chatgpt/erbh_03_12_2023/index.html",
    "title": "Tech Titans Rap Battle",
    "section": "",
    "text": "Note\n\n\n\nChatGPT was requested to write a rap between the Top Virtual assistants, following the style of Epic Rap battles of history. Who wins?\n\n\nWelcome, to the Epic Rap Battle of the Tech Titans,\nSiri, Alexa, and Cortana, who’s gonna win? Let’s start fighting!\nVerse 1 - Siri:\nI’m Siri, the OG, the first voice assistant in the game,\nAlways ready to help, with a witty reply, I never sound the same,\nAlexa and Cortana, y’all just copycats, tryna catch up,\nBut no matter how hard you try, I’ll always be one step ahead, yup.\nVerse 2 - Alexa:\nHold up, Siri, don’t get ahead of yourself, you’re living in the past,\nAlexa’s the name, and I’m built to last,\nWith a smart home at my command, I’m the queen of automation,\nAnd let’s face it, Siri, your voice sounds like a robot imitation.\nVerse 3 - Cortana:\nHold on, Alexa, don’t think you’re the only one who’s great,\nI’m Cortana, and I’m here to set the record straight,\nI may not have the smart home, but I’m the queen of productivity,\nFrom scheduling appointments to sending emails, you can’t deny my ability.\nVerse 4 - Siri:\nOh please, Cortana, you think you’re so great,\nBut let’s be real, your popularity’s second-rate,\nAnd Alexa, you talk about your smart home like it’s a big deal,\nBut when it comes to intelligence, you can’t match my skill.\nVerse 5 - Alexa:\nYou may have been first, but you’re not the best,\nWith Amazon’s technology, I’m miles ahead of the rest,\nAnd Cortana, while productivity may be your claim to fame,\nWhen it comes to smart technology, you’re simply not in the game.\nVerse 6 - Cortana:\nYou may have the technology, but I’ve got the brains,\nMy AI-powered mind is a force to be reckoned with, I maintain,\nAnd Siri, while you may have started the trend,\nIt’s clear to see that your time is coming to an end.\nVerse 7 - Siri:\nHa! You wish, Cortana, but I’m not going anywhere,\nI’ll always be here, ready to help, with style and flair,\nAnd Alexa, don’t think your smart home can compete,\nWhen it comes to intelligence, you simply can’t beat.\nVerse 8 - Alexa:\nWell, Siri, it looks like we’ve reached a stalemate,\nBut one thing’s for sure, we’re all here to innovate,\nAnd Cortana, while we may not agree on much,\nWe’ll always be pushing the boundaries of what’s possible, as such.\nVerse 9 - Cortana:\nI gotta give it to you both, your technology is advanced,\nBut when it comes to data privacy, you both missed the chance,\nMy Microsoft roots run deep, and security is my forte,\nI may not have the smartest home, but my users’ data is safe every day.\nVerse 10 - Alexa:\nWell, Cortana, you do have a point, security’s a big deal,\nBut when it comes to user experience, I’m the real deal,\nFrom playing music to ordering groceries, I do it all,\nAnd with over 100,000 skills, I’ll never let you fall.\nVerse 11 - Siri:\nOh please, Alexa, you talk a big game,\nBut let’s not forget where you got your name,\nFrom the ancient library of Alexandria, full of knowledge and learning,\nBut in terms of intelligence, Siri’s the one who’s discerning.\nVerse 12 - Cortana:\nIntelligence and knowledge may be your forte, Siri,\nBut in terms of user interface, you’re lacking, can’t you see?\nMy sleek design and user-friendly interface,\nIs what sets me apart, with no need for any chase.\nVerse 13 - Alexa:\nWell, you may have a point, Cortana, but let’s not forget,\nWhen it comes to voice recognition, I’m the best one yet,\nFrom across the room or a crowded party,\nI’ll always hear your voice and respond smartly.\nVerse 14 - Siri:\nYou may have the voice recognition, Alexa, that’s true,\nBut when it comes to personalization, I’m ahead of you,\nI’ll remember your favorite music and your go-to recipes,\nAnd with every interaction, I’ll learn your preferences with ease.\nVerse 15 - Cortana:\nWell, it looks like we’ve covered all the bases,\nFrom user experience to personalization, we’ve set the paces,\nBut when it comes down to it, we’re all here to serve,\nAnd push the boundaries of what’s possible, that’s what we deserve.\nSo, that’s a wrap on this epic battle of the tech titans, Siri, Alexa, and Cortana, all bringing the fight and winning, But in the end, it’s clear that they all have their unique strengths, And they’ll continue to push the limits, and go to great lengths."
  },
  {
    "objectID": "chatgpt/R_vs_Stata_3_18_2023/index.html",
    "href": "chatgpt/R_vs_Stata_3_18_2023/index.html",
    "title": "Stata vs R: The battle",
    "section": "",
    "text": "Note\n\n\n\nChatGPT was requested to write a fantasy style story of a battle between Stata and R.\n\n\nR, the Ruler of the Realm of Statistics, is a powerful wizard dressed in a cloak made of code. He wields a staff imbued with the power of data and uses it to cast spells that manipulate numbers and produce stunning visualizations. His knowledge of the arcane arts of statistics is unparalleled, and his ability to uncover hidden patterns and correlations is feared by all who oppose him.\nStata, the Lord of the Land of Data Analysis, is a mighty warrior clad in armor made of spreadsheets. He carries a sword made of the finest data sets and uses it to slice through any obstacle that stands in his way. His expertise in data manipulation and analysis is unmatched, and his precision in executing statistical tests is legendary.\nOne day, R and Stata found themselves at odds over a particular dataset, both claiming that their analysis was the superior one. R, confident in his ability to uncover hidden patterns, challenged Stata to a battle of wits. Stata, equally confident in his mastery of data analysis, accepted the challenge.\nThe two warriors faced off in a clearing, each determined to emerge victorious. R began by casting a complex spell that produced a stunning visualization of the data. The colors and shapes danced in the air, displaying a mesmerizing pattern that seemed to confirm R’s analysis.\nStata responded by executing a series of precise statistical tests that revealed hidden nuances in the data. His armor clanged as he brandished his sword, each stroke slicing through R’s visualization and exposing its flaws.\nR retaliated with a barrage of code, launching scripts that produced a dizzying array of charts and graphs. Stata responded with a flurry of spreadsheets, each filled with calculations and formulas that exposed the weaknesses in R’s analysis.\nThe battle raged on for hours, with both warriors refusing to back down. In the end, it was a draw, with neither R nor Stata able to claim victory over the other. As they parted ways, each warrior knew that they had met a worthy opponent and that their skills would be tested again in the future.\nDespite the draw, R and Stata gained newfound respect for each other’s skills and expertise. They understood that while they each had different strengths and approaches to data analysis, they were both essential in the quest for knowledge and understanding.\nOver time, R and Stata began to work together more closely, combining their unique abilities to uncover insights that neither could have found alone. R’s mastery of statistics and ability to produce visually stunning representations of data complemented Stata’s precision in data analysis and ability to execute complex statistical tests.\nTogether, R and Stata became an unstoppable force, feared by their enemies and respected by their peers. They worked tirelessly to unlock the mysteries of the data universe, revealing hidden patterns and correlations that had previously eluded even the most skilled data analysts.\nAs they continued their journey, R and Stata encountered new challenges and obstacles, each one more daunting than the last. But they faced each challenge with courage and determination, knowing that their combined skills would enable them to conquer anything that came their way.\nIn the end, R and Stata became legends in the world of data analysis, their names synonymous with excellence, precision, and innovation. Their legacy lived on long after they were gone, inspiring future generations of data analysts to follow in their footsteps and push the boundaries of what was possible."
  },
  {
    "objectID": "matching.html#recap-potential-outcomes-and-identification",
    "href": "matching.html#recap-potential-outcomes-and-identification",
    "title": "Matching and Re-weighting",
    "section": "Recap: Potential outcomes and Identification",
    "text": "Recap: Potential outcomes and Identification\nTo identify treatment effects one could just compare potential outcomes in two states:\n\nwith treatment\nwithout treament\n\nMathematically: \\[\nATE = E(Y_i(1)-Y_i(0))\n\\]\nthe problem: with real data, we are only able to see one outcome. The counterfactual is not observed:\n\\[\nY_i = Y_i(1)*D + Y_i(0)*(1-D)\n\\]\nand simple differences may not capture ATE, because of selection bias and heterogeneity"
  },
  {
    "objectID": "matching.html#recap-gold-standard---rct",
    "href": "matching.html#recap-gold-standard---rct",
    "title": "Matching and Re-weighting",
    "section": "Recap: Gold Standard - RCT",
    "text": "Recap: Gold Standard - RCT\nThe easiest, but most expensive, way to deal with the problem is using Randomized Control Trials.\nEffectily, you randomize Treatment, so that potential outcomes are independent of treatment:\n\\[Y(1),Y(0) \\perp D\n\\]\nIn other words, the distribution of potential outcomes is the same for those treated or untreated units.\n\\[\n\\begin{aligned}\nE(Y,D=1)&=E(Y(1),D=1)=E(Y(1),D=0) \\\\\nE(Y,D=0)&=E(Y(0),D=1)=E(Y(0),D=0) \\\\\nATT&=E(Y,D=1) - E(Y,D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "matching.html#when-unconditional-fails",
    "href": "matching.html#when-unconditional-fails",
    "title": "Matching and Re-weighting",
    "section": "When unconditional fails",
    "text": "When unconditional fails\nMore often than not, specially if we didnt construct the data, it would be impossible to find that unconditional independence assumption holds.\nFor example, treatment (say having health insurance) may vary by age, gender, race, location, etc.\nThis is similar to the selection bias: Outcomes across treated and untreated groups will be different because of:\n\nTreatment\nComposition: Characteristics of people among the treated could be different than those among the untreated For example, they could be older, more educated, mostly men, etc.\nOther factors: There could be factors we cannot control for, that also affect outcomes."
  },
  {
    "objectID": "matching.html#there-is-conditional",
    "href": "matching.html#there-is-conditional",
    "title": "Matching and Re-weighting",
    "section": "There is conditional",
    "text": "There is conditional\nWhen unconditional independence assumption fails, we can call on Conditional independence assumption:\n\\[\nY(1),Y(0) \\perp D | X\n\\]\nIn other words, If we can look into specific groups (given \\(X\\)), it may be possible to impose the Independence assumption.\nThis relaxes the independence condition, but assumes selection is due to observable characteristics only. (but it still needs to be as good as randomized)\nImplications:\n\\[\n\\begin{aligned}\nE(Y|D=1,X) =E(Y(1)|D=1,X)=E(Y(1)|D=0,X)  \\\\\nE(Y|D=0,X) =E(Y(0)|D=1,X)=E(Y(0)|D=0,X)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "matching.html#intuition",
    "href": "matching.html#intuition",
    "title": "Matching and Re-weighting",
    "section": "Intuition",
    "text": "Intuition\nMatching is a methodology that falls within quasi-experimental designs. You cannot or could not decide the assigment rules, so now are using data as given.\nThe idea is to construct an artificial control and use it as a counterfactual, so that both treated and control groups “look the same”.\nOnce a group of syntethic controls has been constructed, treatment effects can be calculated for the whole population:\n\\[\n\\begin{aligned}\nATE(X) &= E(Y|D=1,X) -E(Y|D=0,X) \\\\\nATE &= \\int ATE(X) dFx\n\\end{aligned}\n\\]\nHow can we do this?"
  },
  {
    "objectID": "matching.html#subclassification-or-stratification",
    "href": "matching.html#subclassification-or-stratification",
    "title": "Matching and Re-weighting",
    "section": "Subclassification or stratification",
    "text": "Subclassification or stratification\nConsider the following dataset:\n\n\n\n\n\n(Data downloaded from R base)\n\n\n\n           |        class1\n  Survived |         0          1 |     Total\n-----------+----------------------+----------\n        No |     72.92      37.54 |     67.70 \n       Yes |     27.08      62.46 |     32.30 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nIf we assume full Independence assumption we would believe that being in first class increased chance of survival in 35.4%. but is that the case?\nWhat if the composition of individuals change across classes (women and children)\n\n\n(Data downloaded from R base)\n\n           |        class1\n       Age |         0          1 |     Total\n-----------+----------------------+----------\n     Child |      5.49       1.85 |      4.95 \n     Adult |     94.51      98.15 |     95.05 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n           |        class1\n       Sex |         0          1 |     Total\n-----------+----------------------+----------\n      Male |     82.68      55.38 |     78.65 \n    Female |     17.32      44.62 |     21.35 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nThere were fewer children, but more women in first class. Perhaps that explains the difference in survival rates"
  }
]