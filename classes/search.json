[
  {
    "objectID": "01Introduction.html#introduction",
    "href": "01Introduction.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nFirst of all, thank you for joining me this semester, we I expect we all will earn something new.\nWhy? well, while I have learned and implemented many of the methodologies we will see here today, there are a few I have yet to dive into.\nNevertheless, I hope you will enjoy, and learn as much as you can from this course, which has the purpose of:\n\nExpose you to a large set of empirical econometric analysis techniques\nExpose you to the application of some of this techniques to the analysis of Causal effects\n\nBut first, lets lay out the Rules of the game…"
  },
  {
    "objectID": "01Introduction.html#grades",
    "href": "01Introduction.html#grades",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\nYour grade will consist on 4 elements\n\nParticipation 10%: As before, active participation is encourage, so reading the material before class is highly recommended.\nPresentations 15%: Students will have two presentations during the semester (second half), based on suggested material (Syllabus) or other papers the students may be interested in.\nThe main requirement. The paper should implement any of the methodologies we will be covering in class.\nThe presentation should emphasize the Research question, assumptions used, methodology, and results. If possible also provide criticism to the paper.\nHomework 25%: Homeworks will be provided for you to practice and implement the different methodologies discussed in class. They can be carried out individually or in groups (of 2). This will include making a brief description of the results."
  },
  {
    "objectID": "01Introduction.html#grades-1",
    "href": "01Introduction.html#grades-1",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\n\nPaper Project 50:\n\n\nWrite a term paper that can be of two types:\n\n\nPaper Replication: You can choose to write a replication paper on a methodological paper, or applied empirical paper.\nIn either case, the replication will have to extend the analysis of the original paper to a different setup (empirical paper), different software (if replication paper), or other extensions to the original analysis/methodology.\nResearch: A 20-25 pages paper where students answer a research question of their choice, using any of the methodologies presented in class. Standard structure of the paper applies.\n\n\nPresentation of the paper in class"
  },
  {
    "objectID": "01Introduction.html#course-content",
    "href": "01Introduction.html#course-content",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe course content will consist of two parts:\n\nPart I: I will review and cover many of empirical methodologies that expand on the Linear Regression analysis we cover in Econometrics 1. This include:\n\nLinear Regression: OLS (again), but with SE emphasis, and allowing for (too)many variables.\nSemi- and non-parametric regressions: When you need things to be Flexible (but not too flexible)\nCQuantile regressions: When you are interested in people beyond the mean (distributions)\nUQuantile-Regressions, RIF-Regressions: When you are interested in the whole distribution\nNonlinear Models: MLE and GMM: When your models are nonlinear (in coefficients)"
  },
  {
    "objectID": "01Introduction.html#course-content-1",
    "href": "01Introduction.html#course-content-1",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\\[MLE: Y|x \\sim f(\\theta)\\] \\[GMM: E(y-m(x))= 0\\]"
  },
  {
    "objectID": "01Introduction.html#course-content-2",
    "href": "01Introduction.html#course-content-2",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe second part of the course aims to introduce Methodologies that have the goal of identifying Causal effects.\nWhat do we mean with that? We specifically focus on cases when:\n\nA change in T(reatment) (\\(0 \\rightarrow 1\\)) has an impact in Y from \\(y(0) \\rightarrow y(1)\\), because we manage use a design that makes everything else (\\(X's \\ \\& \\ \\varepsilon's\\)) constant.\n\n\nParallel WorldsIdeally you want to observe the same unit under two different Status! (Multiverse!)"
  },
  {
    "objectID": "01Introduction.html#course-content-3",
    "href": "01Introduction.html#course-content-3",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nHowever, as we have discussed last semester, achieving this is hard. There are many factors that we may not be able to control. Thus we need to come-up with “cleaver” strategies to achieve something Similar.\n\nRandomization: When Treatment is generated at “random”. You can’t see All worlds, but its the closest.\nPanel Data and Fixed Effects: Some things are fixed, and you may be able to get “rid” of them if you see them often: Panel data, family effect, twins, etc.\nInstrumental Variables: Searching for Exogenous Variation to “simulate” random assignment. Even tho it may only capture “local” effects\nMatching and Re-weighting: If true twins do not exist, find people who are “observational twins”. Same life, same characteristics, different treatment and outcome. (or instead of people, distributions)"
  },
  {
    "objectID": "01Introduction.html#course-content-4",
    "href": "01Introduction.html#course-content-4",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\nRDD-Regression Discontinuity Design: Use Jumps, and assignment rules. Use the fact that some times treatment is assigned around a threshold.\nDifferences in Differences: Use changes across groups and time. DiD: Accounts for group differences and trends (but be aware of TWFE - when we absorb too much)\nSynthetic Control: Similar to Matching, you can create “synthetic” units, combining the information of multiple units. at the same time. And like DID, you can use that to identify effects."
  },
  {
    "objectID": "01Introduction.html#reading",
    "href": "01Introduction.html#reading",
    "title": "Introduction",
    "section": "Reading",
    "text": "Reading\n\nI have assigned many readings! But you do not need to read them all (but, you may benefit from it).\nAt the very least, read one paper/chapter of the assigned readings.\nThe main books: Casual Inference:The mixtape, The Effect, Mostly Harmless Econometrics, are all available online.\nOtherwise, I ll provide the corresponding -pdfs- on the class website.\n\n\n\nte"
  },
  {
    "objectID": "02OLS.html#introduction",
    "href": "02OLS.html#introduction",
    "title": "Linear Regression Model",
    "section": "Introduction",
    "text": "Introduction\n\nLinear Regression (usually estimated via OLS) is the most basic, and still most useful, tool for analyzing data.\nThe goal is to find what the relationship between the outcome \\(y\\) and explanatory variables \\(X's\\) is.\nSay that we start with a very simple “model” that states tries to describe the population function as the following:\n\n\\[\ny = h(X,\\varepsilon)\n\\]\nHere, \\(X\\) represents a set of observed covariates and \\(\\varepsilon\\) the set of unobserved characteristics, and for now, we assume that there is no pre-define relationship between these components.\n\nFor now, we will make standard exogeneity assumptions for the identification of the model"
  },
  {
    "objectID": "02OLS.html#estimation",
    "href": "02OLS.html#estimation",
    "title": "Linear Regression Model",
    "section": "Estimation",
    "text": "Estimation\n\nThe functional form, however, is unknowable. However, under the small assumption that \\(X\\) and \\(\\varepsilon\\) are unrelated, if we would have access to the population data, we could instead consider the Conditional Expectation function (CEF):\n\n\\[\nE(y_i|X_i=x) = \\int t f_y(t|X_i=x)dx\n\\]\n\nNotice that this implies a fully non-parametric estimation of the Linear function (because it does not impose any functional form).\nWith this, we can “decompose” the outcome \\(y\\) into two components, one that depends on observation characteristics (CEF) and one that depends on the error \\(\\varepsilon\\).\n\n\\[\ny = E(y|X) + \\varepsilon\n\\]\n\nThis has the nice property that the error is unrelated to any functional form of \\(X\\), while providing a summary of the relationship between \\(X\\) and \\(y\\)."
  },
  {
    "objectID": "02OLS.html#section",
    "href": "02OLS.html#section",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The CEF is a convenient abstract, but to estimate it, we require assumptions. (Recall the assumptions for unbiased OLS?)\nNamely, we need to impose a linearity assumption, namely:\n\n\\[\nE(y_i|X_i=x) = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + ... +\n\\beta_k x_k = X_i'\\beta\n\\]\n\nAnd the solution for \\(\\beta\\) is given by:\n\n\\[\n\\beta = \\underset{b}{arg} \\ E(L(y_i-X'_i b))\n\\]\nWhere the loss function \\(L(x)=x^2\\). (Square loss function)\n\nThis implies the following condition:\n\n\\[\nE[X_i (y_i-X_i'b)]=0 \\rightarrow \\beta = E[X_i'X_i]^{-1}E[X_i'y_i]\n\\]\n\nThis population terms must be substituted by the sample equivalent: \\(E(X_i) =\\frac{1}{N} \\sum_i^NX_i\\)"
  },
  {
    "objectID": "02OLS.html#mata-ols-estimator",
    "href": "02OLS.html#mata-ols-estimator",
    "title": "Linear Regression Model",
    "section": "Mata: OLS Estimator",
    "text": "Mata: OLS Estimator\nThe estimator using Sample equivalents become:\n\\[\n\\hat \\beta =\n\\left(\\frac{1}{N} \\sum_i X_i'X_i \\right)^{-1}\n\\frac{1}{N} \\sum_i X_i'y_i=(X'X)^{-1}X'y\n\\]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage !=.\nmata:\n  y = st_data(.,\"lnwage\")\n  n = rows(y)\n  x = st_data(.,\"female age educ\"),J(n,1,1)\n  exx = cross(x,x)/n\n  exy = cross(x,y)/n\n  b   = invsym(exx)*exy\n  b\nend  \n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:   y = st_data(.,\"lnwage\")\n\n:   n = rows(y)\n\n:   x = st_data(.,\"female age educ\"),J(n,1,1)\n\n:   exx = cross(x,x)/n\n\n:   exy = cross(x,y)/n\n\n:   b = invsym(exx)*exy\n\n:   b\n                 1\n    +---------------+\n  1 |  -.145393595  |\n  2 |  .0161424301  |\n  3 |  .0719321873  |\n  4 |  1.970020725  |\n    +---------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "02OLS.html#inference---distribution-of-betas",
    "href": "02OLS.html#inference---distribution-of-betas",
    "title": "Linear Regression Model",
    "section": "Inference - Distribution of \\(\\beta's\\)",
    "text": "Inference - Distribution of \\(\\beta's\\)\nso: \\[\n\\begin{aligned}\ny &= X\\beta + \\varepsilon \\\\\n\\sqrt N (\\hat\\beta - \\beta) &=\\frac{1}{N}\\Big[\\sum (X_iX_i')\\Big]^{-1} \\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon_i)\n\\end{aligned}\n\\]\n\nHere \\(\\varepsilon\\) is the true population error. \\(\\hat\\beta\\) is unbiased if the second term has an expectation of Zero. (the error is independent from \\(X\\)).\nAsymptotically, the first term is assumed fixed \\(E(X_i X_i')\\). And, because \\(E(X_i\\varepsilon)=0\\), and \\(\\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon)\\) is normalized, by CLT we have that:\n\n\\[\n\\sqrt N (\\hat\\beta-\\beta)\\sim N(0,E(X_iX_i')^{-1} \\ E(X_iX_i'\\varepsilon ^2) \\ E(X_iX_i')^{-1})\n\\]\n\nFrom here, the main question is : How do we estimate \\(E(X_iX'\\varepsilon_i^2)\\)?"
  },
  {
    "objectID": "02OLS.html#inference-estimating-se",
    "href": "02OLS.html#inference-estimating-se",
    "title": "Linear Regression Model",
    "section": "Inference: Estimating SE",
    "text": "Inference: Estimating SE\n\nLets First Rewrite the last expression:\n\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} X'\\Omega X (X'X)^{-1}\n\\]\nwhere:\n\\[\n\\Omega=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)\n\\]\nIn other words, the variance of \\(\\hat\\beta\\) allows for arbitrary relationship among the errors, as well as heteroskedasticity. This, however is impossible to estimate!, thus we require assumptions"
  },
  {
    "objectID": "02OLS.html#homoskedasticity-and-independent-samples",
    "href": "02OLS.html#homoskedasticity-and-independent-samples",
    "title": "Linear Regression Model",
    "section": "Homoskedasticity and independent samples",
    "text": "Homoskedasticity and independent samples\nThe easiest route is to assume homoskedastic errors \\(\\sigma^2 = \\sigma_i^2 \\ \\forall i \\in 1,...,N\\) . (the error is spread equally around the mean)\nWith independent samples \\(\\sigma_{ij}=0 \\ \\forall \\ i\\neq j\\) . (A persons unobserved is completely independent from anybody else)\n\\[\n\\Omega_00=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)=I(N)*\\sigma^2\n\\]\nThus \\[\n\\begin{aligned}\nVar(\\hat\\beta)_{00} &=(X'X)^{-1} X'I(N)\\sigma^2 X (X'X)^{-1} \\\\\n&=\\sigma^2 (X'X)^{-1} \\\\\n\\sigma^2 &= E(\\varepsilon^2)\n\\end{aligned}\n\\]\n\n\nCode\nmata: e=err = y:-x*b\nmata: var_b_000 = mean(err:^2) * invsym(x'x)\nmata: b,sqrt(diagonal(var_b_000))\n\n\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243547399  |\n  2 |  .0161424301   .0010962465  |\n  3 |  .0719321873    .005029506  |\n  4 |  1.970020725   .0724744138  |\n    +-----------------------------+"
  },
  {
    "objectID": "02OLS.html#section-1",
    "href": "02OLS.html#section-1",
    "title": "Linear Regression Model",
    "section": "",
    "text": "But, \\(\\sigma^2\\) is not known, so we have to use \\(\\hat\\sigma^2\\) instead, which depends on the sample residuals: \\[\n\\hat\\sigma^2 = \\frac{1}{N-k-1}\\sum \\hat e^2\n\\]\nWhere we account for the fact true errors are not observed, but rather residuals are estimated, adjusting the degrees of freedom.\n\n\nCode\nmata:\n    N = rows(y); k = cols(x)\n    var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n    b,sqrt(diagonal(var_b_00))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     N = rows(y); k = cols(x)\n\n:     var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n\n:     b,sqrt(diagonal(var_b_00))\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243887787  |\n  2 |  .0161424301   .0010977786  |\n  3 |  .0719321873   .0050365354  |\n  4 |  1.970020725   .0725757058  |\n    +-----------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "02OLS.html#lifting-assumptions-heteroscedasticity",
    "href": "02OLS.html#lifting-assumptions-heteroscedasticity",
    "title": "Linear Regression Model",
    "section": "Lifting Assumptions: Heteroscedasticity",
    "text": "Lifting Assumptions: Heteroscedasticity\n\nWe start by lifting this assumption, which implies the following:\n\n\\[\n\\sigma^2_i \\neq \\sigma^2_j \\  \\forall \\ i\\neq j\n\\]\nBut to estimate this, we need an approximation for \\(\\sigma^2_i = E(\\varepsilon_i^2) = \\varepsilon_i^2\\).\n\nWith this, we can obtain what is known as th White or Eicker-White or Heteroskedasiticy Robust Standard errors.\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta)_{0} &= (X'X)^{-1} (X\\hat e)'(\\hat eX) (X'X)^{-1} \\\\\n&=(X'X)^{-1} \\sum(X_iX_i'\\hat e^2) (X'X)^{-1}\n\\end{aligned}\n\\]\nWhich imposes NO penalty to the fact that we are using residuals not errors. If we account for that however, we obtain what is known as HC1, SE, the standard in stata. (when you type robust)\n\\[\nVar(\\hat\\beta)_{1}=\\frac{N}{N-K-1}Var(\\hat\\beta)_{0}\n\\]\n\n\nCode\nmata:\n    ixx = invsym(x'x)\n    var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n    var_b_1 = N/(N-k)*var_b_0\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     ixx = invsym(x'x)\n\n:     var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n\n:     var_b_1 = N/(N-k)*var_b_0\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\n                 1             2             3\n    +-------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986  |\n  2 |  .0161424301   .0013544849   .0013563779  |\n  3 |  .0719321873    .005690214   .0056981668  |\n  4 |  1.970020725   .0875757052   .0876981032  |\n    +-------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "02OLS.html#but-error-is-not-the-same-as-residual",
    "href": "02OLS.html#but-error-is-not-the-same-as-residual",
    "title": "Linear Regression Model",
    "section": "But error is not the same as residual!",
    "text": "But error is not the same as residual!\nA residual is model dependent, and should not be confused with the model error \\(\\hat \\varepsilon \\neq \\varepsilon\\). Because of this, additional corrections are needed to obtained unbiased \\(var(\\hat\\beta)\\) estimates. (Degrees of freedom). But other options exists.\nRedefine the Variance Formula:\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} (\\sum X_iX_i \\psi_i )  (X'X)^{-1}\n\\]\nFrom here Mackinnon and White (1985) suggest few other options: \\[\n\\begin{matrix}\nHC0: \\psi_i = \\hat e^2 &\nHC1: \\psi_i = \\frac{N}{N-K}  \\hat e^2 \\\\\nHC2: \\psi_i =   \\hat e^2 \\frac{1}{1-h_{ii}} &\nHC3: \\psi_i =   \\hat e^2 \\frac{1}{(1-h_{ii})^2}\n\\end{matrix}\n\\]\nWhere \\(h_{ii}\\) is the ith diagonal element of \\(X(X'X)^{-1}X'\\) and allows you to see how dependent a model is to a single observation.\nHC2 and HC3 Standard errors are better than HC1 SE, specially when Samples are small.\n\nNOTE: this \\(h_{ii}\\) element is also used to measure the degrees of freedom of a model. Sum it up, and you will see!."
  },
  {
    "objectID": "02OLS.html#coding-robust-se",
    "href": "02OLS.html#coding-robust-se",
    "title": "Linear Regression Model",
    "section": "Coding Robust SE",
    "text": "Coding Robust SE\n\n\nCode\nmata:\n    // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n    h = rowsum(x*invsym(x'x):*x)\n    psi0 = e:^2           ;   psi1 = e:^2*N/(N-k)\n    psi2 = e:^2:/(1:-h)   ;   psi3 = e:^2:/((1:-h):^2)\n    var_b_0 = ixx * cross(x,psi0,x) * ixx\n    var_b_1 = ixx * cross(x,psi1,x) * ixx\n    var_b_2 = ixx * cross(x,psi2,x) * ixx\n    var_b_3 = ixx * cross(x,psi3,x) * ixx\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n    sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\nend  \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n:     h = rowsum(x*invsym(x'x):*x)\n\n:     psi0 = e:^2 ; psi1 = e:^2*N/(N-k)\n\n:     psi2 = e:^2:/(1:-h) ; psi3 = e:^2:/((1:-h):^2)\n\n:     var_b_0 = ixx * cross(x,psi0,x) * ixx\n\n:     var_b_1 = ixx * cross(x,psi1,x) * ixx\n\n:     var_b_2 = ixx * cross(x,psi2,x) * ixx\n\n:     var_b_3 = ixx * cross(x,psi3,x) * ixx\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n&gt;     sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\n                 1             2             3             4             5\n    +-----------------------------------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986   .0243568124   .0243975204  |\n  2 |  .0161424301   .0013544849   .0013563779   .0013573922   .0013603079  |\n  3 |  .0719321873    .005690214   .0056981668   .0057079191    .005725691  |\n  4 |  1.970020725   .0875757052   .0876981032   .0878131672   .0880514838  |\n    +-----------------------------------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nOr in Stata:\nregress y x1 x2 x3, vce(robust)\nregress y x1 x2 x3, vce(hc2)\nregress y x1 x2 x3, vce(hc3)"
  },
  {
    "objectID": "02OLS.html#lifting-even-more-assumptions-correlation",
    "href": "02OLS.html#lifting-even-more-assumptions-correlation",
    "title": "Linear Regression Model",
    "section": "Lifting Even more Assumptions: Correlation",
    "text": "Lifting Even more Assumptions: Correlation\n\nOne assumption we barely consider last semester was the possibility that errors could be correlated within groups. (except for time series and serial correlation)\nFor example, families may share similar unobserved factors, So would people interviewed from the same classroom, cohort, city, etc. There could be many dimensions to consider possible correlations!\nIn that situation, we may be missmeasuring the magnitude of the errors (probably downward), because the \\(\\Omega\\) is no longer diagonal: \\(\\sigma_{ij} \\neq 0\\) for some \\(i\\neq j\\).\n\nBut, estimate all parameters in an NxN matrix is unfeasible. We need assumptions!\n\nSay we have \\(G\\) groups \\(g=(1…G)\\) . We can rewrite the expression for \\(\\hat\\beta\\) as follows:\n\n\\[\n\\begin{aligned}\n\\hat\\beta-\\beta &= (X'X)^{-1}\\sum_{g=1}^G X'_g \\varepsilon_g \\\\\\\n&=(X'X)^{-1}\\sum_{g=1}^G s_g\n\\end{aligned}\n\\]\n\nWe can assume that individuals are correlated within groups \\(E(s_g's_g) =\\Sigma_g\\) , but they are uncorrelated across groups \\(E(s_g s_g')=0 \\ \\forall \\ g \\neq g'\\) .\nThese groups are typically known as “clusters”"
  },
  {
    "objectID": "02OLS.html#addressing-correlation",
    "href": "02OLS.html#addressing-correlation",
    "title": "Linear Regression Model",
    "section": "Addressing Correlation",
    "text": "Addressing Correlation\n\nThe idea of correcting for clusters is pretty simple. We just need to come up with an estimator for \\(\\Sigma_g\\) for every cluster, so that:\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta) &= (X'X)^{-1} \\left( \\sum_{g=1}^N \\Sigma_g \\right) (X'X)^{-1} \\\\\n\\Sigma_g &= E( X_g' \\Omega_g X_g)\n\\end{aligned}\n\\]\n\nHere \\(\\Omega_g\\) should be an approximation of the variance covariance matrix among the errors of ALL individuals that belong to the same cluster. But how do we approximate it?\nAs with the EW - HC standard errors, there are many ways to estimate Clustered Standard errors. See MacKinnon et al (2023) for reference. We will refer only to the simpler ones CV0 and CV1.\n\n\nStill How?\n\n\nRecall we approximate \\(\\sigma^2_i\\) with \\(\\varepsilon_i^2\\). Then we can approximate \\(\\sigma_{ij}\\) with \\(\\varepsilon_j \\varepsilon_i\\). More specifically:\n\n\\[\n\\Omega_g \\simeq \\varepsilon \\varepsilon' \\ or \\ \\Sigma_g = X'_g \\varepsilon \\varepsilon' X_g = (X'_g \\varepsilon) (\\varepsilon' X_g)\n\\]\n\nChange \\(\\varepsilon\\) with \\(\\hat\\varepsilon\\), do that for every group, and done! (almost)."
  },
  {
    "objectID": "02OLS.html#section-2",
    "href": "02OLS.html#section-2",
    "title": "Linear Regression Model",
    "section": "",
    "text": "As mentioned earlier, there are many CCSE (clustered consistent SE).\n\n\\[\n\\begin{aligned}\nCV_0 &= (X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1} \\\\\nCV_1 &= \\frac{G(N-1)}{(G-1)(N-k-1)}(X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1}\n\\end{aligned}\n\\]\n\nSimilar to HC. CV0 does not correct for degrees of freedom. CV1, however, accounts for Degrees of freedom in the model, and clusters.\n\n\n\nCode\nmata:\n    // 1st Sort Data (easier in Stata rather than Mata) and reload\n    y   = st_data(.,\"lnwage\")\n    x   = st_data(.,\"educ exper female\"),J(1434,1,1) \n    cvar= st_data(.,\"isco\")\n    ixx = invsym(cross(x,x)); xy = cross(x,y)\n    b   = ixx * xy\n    e   = y:-x*b\n    // Set the panel info\n    info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n    // get X_g'e for all groups: \n    s_xg_e = panelsum(x:*e,info)\n    // Sum Sigma_g\n    sigma_g = s_xg_e's_xg_e\n    cv0 = ixx*sigma_g*ixx\n    cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n    b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\nend    \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     // 1st Sort Data (easier in Stata rather than Mata) and reload\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female\"),J(1434,1,1)\n\n:     cvar= st_data(.,\"isco\")\n\n:     ixx = invsym(cross(x,x)); xy = cross(x,y)\n\n:     b = ixx * xy\n\n:     e = y:-x*b\n\n:     // Set the panel info\n:     info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n\n:     // get X_g'e for all groups:\n:     s_xg_e = panelsum(x:*e,info)\n\n:     // Sum Sigma_g\n:     sigma_g = s_xg_e's_xg_e\n\n:     cv0 = ixx*sigma_g*ixx\n\n:     cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n\n:     b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\n                  1              2              3\n    +----------------------------------------------+\n  1 |   .0858251775    .0140570765    .0149254126  |\n  2 |   .0147342796    .0014534593    .0015432426  |\n  3 |  -.0949227416    .0525121234    .0557559112  |\n  4 |   2.218849962    .1947497649    .2067798804  |\n    +----------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nor compare it to\n\n\nCode\nreg lnwage educ exper female, cluster(isco)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(3, 8)           =      59.13\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2217\n                                                Root MSE          =     .46897\n\n                                   (Std. err. adjusted for 9 clusters in isco)\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0858252   .0149254     5.75   0.000     .0514071    .1202432\n       exper |   .0147343   .0015432     9.55   0.000     .0111756     .018293\n      female |  -.0949227   .0557559    -1.70   0.127    -.2234961    .0336506\n       _cons |    2.21885   .2067799    10.73   0.000     1.742015    2.695685\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "02OLS.html#beware-of-over-clustering",
    "href": "02OLS.html#beware-of-over-clustering",
    "title": "Linear Regression Model",
    "section": "Beware of over-clustering",
    "text": "Beware of over-clustering\nWhile clustering helps address a problem of “intragroup” correlation, it can/should be done with care. It is important to be aware about some unintended problems of using Correlation.\n\nCV0 and CV1 work well when you have a large number of Clusters. How many? MHE(2009) says…42 (this is like having large enough samples for Asymptotic variance). If # clusters are small, you would do better with other approaches (including CV2 and CV3).\nWhen you cluster your standard errors, you will “most-likely” generate larger standard errors in your model. Standard recommendation (MHE) is to cluster at the level that makes sense (based on data) and produces largest SE (to be conservative)."
  },
  {
    "objectID": "02OLS.html#role-of-clusters",
    "href": "02OLS.html#role-of-clusters",
    "title": "Linear Regression Model",
    "section": "Role of clusters",
    "text": "Role of clusters\n\nStandard Errors"
  },
  {
    "objectID": "02OLS.html#section-3",
    "href": "02OLS.html#section-3",
    "title": "Linear Regression Model",
    "section": "",
    "text": "You may also consider that clustering does not work well when sample sizes within cluster are to diverse (micro vs macro clusters)\nAnd there is the case where clustering is required among multiple dimensions (see vcemway). Where the unobserved correlation could be present in different dimensions.\n\nSo what to cluster and how?\n\nMackinnon et al (2023) provides a guide on how and when to cluster your standard errors. (some are quite advanced)\nGeneral practice, At least use Robust SE (HC2 or HC3 if sample is small), but use clustered SE for robustness.\nYou may want to cluster SE based on some theoretical expectations. Choose -broader- groups for conservative analysis.\nIn treatment-causal effect analysis, you may want to cluster at the “treatment” level.\n\n\nBut…Beyond hc0/1 and CV0/1 there is not much out there for correcting Standard errors in nonlinear models."
  },
  {
    "objectID": "02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "href": "02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "title": "Linear Regression Model",
    "section": "If you can’t Sandwich 🥪, you can re-Sample",
    "text": "If you can’t Sandwich 🥪, you can re-Sample\n\nThe discussion above refereed to the estimation of SE using \\(Math\\). In other words, it was based on the asymptotic properties of the data. Which may not work in small samples.\nAn alternative, often used by practitioners, is using re-sampling methods to obtain approximations to the coefficient distributions of interest.\n\nBut… How does it work?🤔\nFirst ask yourself, how does Asymptotic theory work (and econometrics)? 😱\n\nNote: I recommend reading the -simulation- chapter in The effect, and simulation methods chapter in CT."
  },
  {
    "objectID": "02OLS.html#a-brief-reviewagain",
    "href": "02OLS.html#a-brief-reviewagain",
    "title": "Linear Regression Model",
    "section": "A Brief Review…again 😇",
    "text": "A Brief Review…again 😇\nIf I were to summarize most of the methodologies (ok all) we used last semester, and this one, the properties that have been derived and proofed are based on the assumption that we “could” always get more data (frequentist approach).\nThere is population (or supper population) from where we can get samples of data.\n\nWe get a sample (\\(y,X\\)) (of size N)\nEstimate our model : method(\\(y,X\\))\\(\\rightarrow\\) \\(\\beta's\\)\nRepeat to infinitum\nCollect all \\(\\beta's\\) and summarize. (Mean and Standard deviations)\n\nDone.\nThe distributions you get from the above exercise should be the same as what your estimation method produces. (if not, there there is something wrong with the estimation method)"
  },
  {
    "objectID": "02OLS.html#but-we-only-get-1-sample",
    "href": "02OLS.html#but-we-only-get-1-sample",
    "title": "Linear Regression Model",
    "section": "But we only get 1 Sample!",
    "text": "But we only get 1 Sample!\nThe truth is we do not have access to multiple samples. Getting more data, is in fact, very expensive. So what to do ?\n\nRely on Asymptotic theory\nlearn Bayesian Econometrics 🥺\nor-resample? and do Bootstrap!\n\nBasic idea of Bootstrapping\n\nIn the ideal scenario, you get multiple samples from your population, Estimate parameters, and done.\nIf not possible you do the next best thing. You get your sample (assume is your mini-population),\n\nDraw subsamples of same size (with replacement) (\\(y_i^s,X_i^s\\))\nestimate your model and obtain parameters \\(\\beta^s_i\\)\nSummarize those parameters…and done, you get \\(Var(\\hat\\beta)\\) for 🆓. (or is it?)"
  },
  {
    "objectID": "02OLS.html#bootstrapping",
    "href": "02OLS.html#bootstrapping",
    "title": "Linear Regression Model",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n👢Bootstrapping is a methodology that allows you to obtain empirical estimations of standard errors making use of the data in hand, and without even knowing about Asymptotic theory (other than how to get means and variances).\n\n\nBootstrap Sample\nAnd of course, it comes in different flavors."
  },
  {
    "objectID": "02OLS.html#bootstrap-types",
    "href": "02OLS.html#bootstrap-types",
    "title": "Linear Regression Model",
    "section": "Bootstrap Types:",
    "text": "Bootstrap Types:\n\nNon-parametric Bootstrap: You draw subsamples from the main sample. Each observation has the same pr of being selected.\n\nEasiest to implement ( see bootstrap:)\nWorks in almost all cases, but you may have situations when some covariates are rare.\nCan be extended to allow “clusters” using “block bootstrapping”. Works best if re-sampling “follows” the same sampling structure as your sample.\n\nParametric Bootstrap: You estimate your model, make assumptions of your model error.\n\nYou need to implement it on your own. \\(y^s=x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim f(\\hat \\theta)\\)\nIt will not work well if the assumptions of the error modeling are wrong.\n\nResidual bootstrap: Estimate your model, obtain residuals. Re-sample residuals\n\nAgain, implement it on your own. \\(y^s = x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim {\\hat e_1 , ... , \\hat e_N}\\)\nIt depends even more on the assumptions of the error modeling."
  },
  {
    "objectID": "02OLS.html#section-4",
    "href": "02OLS.html#section-4",
    "title": "Linear Regression Model",
    "section": "",
    "text": "UWild bootstrap: Estimate your model, obtain residuals, and re-sample residual weights.\n\nAgain…on your own: \\(y^s = x\\hat b +\\hat e * v\\) , where \\(v \\sim ff()\\) where \\(ff()\\) is a “good” distribution function. \\(E(v)=0 \\ \\& \\ Var(v)=1\\)\nActually quite flexible, and works well under heteroskedasticity!\nIt can also allow clustered standard errors. The error \\(v\\) no longer changes by individual, but by group. It also works well with weights.\n\nUWild bootstrap-2 : Estimate your model, obtain Influence functions 😱 , and re-sample residual weights.\n\nThis is an extension to the previous option. But with advantages\n\nyou do not need to -reestimate- the model. Just look into how the the mean of IF’s change.\nit can be applied to linear and nonlinear model (if you know how to build the IF’s)\n\nWorks well with clustered and weights.\n\nCWild bootstrap: Similar UWild Bootstrap, Obtain Influence functions under the Null (imposing restrictions), and use that to test the NULL.\n\nNo, you do not need to do it on your own. see bootest in Stata.\nWorks pretty well with small samples and small # clusters. Probably the way to go if you really care about Standard errors."
  },
  {
    "objectID": "02OLS.html#how-to-bootstrap-in-stata",
    "href": "02OLS.html#how-to-bootstrap-in-stata",
    "title": "Linear Regression Model",
    "section": "How to Bootstrap? in Stata",
    "text": "How to Bootstrap? in Stata\nI have a few notes on Bootstrapping here Bootstrapping in Stata. But let me give you the highlights for the most general case.\n\nMost (if not all commands) in Stata allow you to obtain bootstrap standard errors, by default. see:help [cmd]\nthey usually have the following syntax:\n[cmd] y x1 x2 x3, vce(bootstrap, options)\nregress lnwage educ exper female, vce(bootstrap, reps(100))\nHowever, you can also Bootstrap that commands that do not have their own bootstrap option.\nbootstrap:[cmd] y x1 x2 x3, \nbootstrap, reps(100):regress lnwage educ exper female\nbootstrap, reps(100) cluster(isco):regress lnwage educ exper female"
  },
  {
    "objectID": "02OLS.html#section-5",
    "href": "02OLS.html#section-5",
    "title": "Linear Regression Model",
    "section": "",
    "text": "This last command may allow you to bootstrap multiple models at the same time, although it does require a bit of programming. (and a do file)\n\n\n\nCode\ngen tchild = kids6 + kids714\ncapture program drop bs_wages_children\nprogram bs_wages_children, eclass // eclass is for things like equations\n    ** Estimate first model\n    reg lnwage educ exper female\n    matrix b1 = e(b)\n    matrix coleq b1 = lnwage\n    ** Estimate second model\n    reg tchild educ exper female\n    matrix b2 = e(b)\n    matrix coleq b2 = tchild\n    ** Put things together and post\n    matrix b = b1 , b2\n    ereturn post b\nend\nbootstrap: bs_wages_children\n\n\n\n(running bs_wages_children on estimation sample)\n\nwarning: bs_wages_children does not set e(sample), so no observations will be\n         excluded from the resampling because of missing values or other\n         reasons. To exclude observations, press Break, save the data, drop\n         any observations that are to be excluded, and rerun bootstrap.\n\nBootstrap replications (50): .........10.........20.........30.........40......\n&gt; ...50 done\n\nBootstrap results                                        Number of obs = 1,434\n                                                         Replications  =    50\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlnwage       |\n        educ |   .0858252   .0058256    14.73   0.000     .0744072    .0972431\n       exper |   .0147343   .0011288    13.05   0.000      .012522    .0169466\n      female |  -.0949227   .0283363    -3.35   0.001     -.150461   -.0393845\n       _cons |    2.21885   .0826592    26.84   0.000     2.056841    2.380859\n-------------+----------------------------------------------------------------\ntchild       |\n        educ |   .0177854   .0091641     1.94   0.052     -.000176    .0357468\n       exper |  -.0047747   .0017288    -2.76   0.006     -.008163   -.0013864\n      female |  -.1306332   .0457432    -2.86   0.004    -.2202883   -.0409781\n       _cons |   .4163459   .1156959     3.60   0.000     .1895861    .6431058\n------------------------------------------------------------------------------\n\n\nWhy does it matter? because you may want to test coefficients individually, or across models. This is only possible if the FULL system is estimated jointly"
  },
  {
    "objectID": "02OLS.html#final-words-on-bootstrap",
    "href": "02OLS.html#final-words-on-bootstrap",
    "title": "Linear Regression Model",
    "section": "Final words on Bootstrap:",
    "text": "Final words on Bootstrap:\nSo bootstrap (and its many flavors) are convenient approaches to estimate standard errors and elaborate statistical Inference, but its not infallible.\n\nIf the re-sampling process does not simulate the true sampling design, we may miss important information when constructing SE.\nWhen the parameters are estimated using “hard” cutoffs or restricted distributions, it may not produce good approximations for SE.\nYou usually require MANY repetitions (standard = 50, but you probably want 999 or more). The more the better, but has some computational costs. (specially simple bs)\nSome methods play better with weighted samples, clusters, and other survey designs than others. And some require more know-how than others.\n\nSo choose your 🔫weapon wisely!"
  },
  {
    "objectID": "02OLS.html#variance-of-nonlinear-functions",
    "href": "02OLS.html#variance-of-nonlinear-functions",
    "title": "Linear Regression Model",
    "section": "Variance of nonlinear functions",
    "text": "Variance of nonlinear functions\n\n\nSome times (perhaps not with simple OLS) you many need to estimate Standard errors for transformations of your main coefficient of interest, or combinations of those coefficients.\nSay that you estimated \\(\\theta \\sim N(\\mu_\\theta, \\sigma^2_\\theta)\\) but are interested in the distribution of \\(g(\\theta)\\). How do you do this?\nTwo options:\n\nyou re estimate \\(g(\\theta\\)) instead, or\nyou make an approximation, using the Delta Method\n\nHow does it work?"
  },
  {
    "objectID": "02OLS.html#section-6",
    "href": "02OLS.html#section-6",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The Delta method uses the linear approximations to approximate the distribution of otherwise not known distributions.\nFurther, It relies on the fact that linear transformations a normal distribution, is on itself normal. For example:\n\n\\[\ng(\\hat \\theta) \\simeq g(\\theta) + g'(\\hat\\theta) (\\hat \\theta-\\theta)\n\\]\n\nThis states that the nonlinear function \\(g(\\theta)\\) can be “locally” approximated as a linear function in the neighborhood of \\(g(\\theta)\\).\nPredictions above or below are approximated using the slope of the function. \\(g'(\\theta)\\).\nSo, if we take the variance, we get:\n\n\\[\nVar(g(\\hat \\theta)) \\simeq  Var \\left(g(\\theta)+ g'(\\hat\\theta) (\\hat \\theta-\\theta)\\right)\n=g'(\\hat\\theta)^2 Var(\\theta)\n\\]"
  },
  {
    "objectID": "02OLS.html#delta-method-visualization",
    "href": "02OLS.html#delta-method-visualization",
    "title": "Linear Regression Model",
    "section": "Delta Method: Visualization",
    "text": "Delta Method: Visualization"
  },
  {
    "objectID": "02OLS.html#section-7",
    "href": "02OLS.html#section-7",
    "title": "Linear Regression Model",
    "section": "",
    "text": "It can go multivariate as well:\n\\[\n\\begin{aligned}\ng(\\hat \\theta, \\hat \\gamma)-g(\\theta,\\gamma) &\\simeq N(0,\\nabla g ' \\Sigma \\nabla g) \\\\\n\\nabla g ' &=   [\\begin{matrix}\n    dg/d\\theta & dg/d\\gamma\n  \\end{matrix}]\n\\end{aligned}  \n\\]"
  },
  {
    "objectID": "02OLS.html#so-why-do-we-care",
    "href": "02OLS.html#so-why-do-we-care",
    "title": "Linear Regression Model",
    "section": "So why do we care:",
    "text": "So why do we care:\nTwo reasons:\n\nNonlinear models need this kind of approximations to do statistical inference (probit/logit)\nRecall that when using Robust Standard errors Joint hypothesis Should be done with Care…\n\nConsider a linear set of restrictions imposed by the \\(H_0: R\\beta = r\\).\n\nEstimate the Variance of \\(R\\beta\\)\n\n\\[\nVar(R\\beta)  = \\nabla (R\\beta)' Var(\\beta) R \\nabla (R\\beta)'= R' Var(\\beta) R\n\\]\n\nEstimate the F value for the Linear Hypothesis (Wald Test)\n\n\\[\n(R\\hat \\beta-r)' Var(R\\beta)^{-1} (R\\hat \\beta-r)/Q \\sim F(Q,N-K)\n\\]"
  },
  {
    "objectID": "02OLS.html#what-happens-when-k-is-too-big",
    "href": "02OLS.html#what-happens-when-k-is-too-big",
    "title": "Linear Regression Model",
    "section": "What happens when K is too big?",
    "text": "What happens when K is too big?\n\n\nHow many variables (max) can you use in a model?\n\n\\[max \\ k = rank(X'X)\\]\n\nWhat happens when you add too many variables in a model?\n\nIncrease Multicolinearity and coefficient variance (too much noise)\nR2 overly large (without explaining much)\nFar more difficult to interpret (too many factors)\nMay introduce endogeneity (when it wasnt a problem before)\n\nHow can you solve the problem?\n\nYou select only a few of the variables, based on theory, and contribution to the model\n\nWhat if you can’t choose?"
  },
  {
    "objectID": "02OLS.html#ml-we-let-the-choose-for-you",
    "href": "02OLS.html#ml-we-let-the-choose-for-you",
    "title": "Linear Regression Model",
    "section": "ML: We let the 💻Choose for you",
    "text": "ML: We let the 💻Choose for you\n\nBefore we start. The methodology we will discuss are usually meant to get models with “good” predictive power, and some times better interpretability, not so much stat-inference (although its possible)\n\nWhen you do not know how to choose, you could try select a subset of variables from your model such that you maximize the predictive power of the model.\nThis should go beyond IN sample predictive power, but instead maximize Out of sample predictive power.\nThis is typically achieved using the following:\n\\[\nAR^2 = 1-\\frac{SSR}{SST}\\frac{n-1}{n-k-1} \\\\\nAIC = n^{-1}(SSR + 2k\\hat\\sigma^2) \\\\\nBIC = n^{-1}(SSR + ln(n) k\\hat\\sigma^2)\n\\]\nOr using a method known as cross-validation (Comparing predictive power using data not used for model estimation)\nHowever, we can always try to estimate a model with all variables!"
  },
  {
    "objectID": "02OLS.html#ridge-and-lasso-and-elasticnet",
    "href": "02OLS.html#ridge-and-lasso-and-elasticnet",
    "title": "Linear Regression Model",
    "section": "Ridge and Lasso and ElasticNet",
    "text": "Ridge and Lasso and ElasticNet\n\nRecall that when using OLS to obtain \\(\\beta's\\), we try to minimize the following:\n\n\\[\nSSR = \\sum_i(y_i - X_i \\beta)^2\n\\]\n\nThis has the restrictions of mentioned before (\\(k &lt; N\\)). In addition to letting coefficents vary “too much”\nAn alternative is to use Ridge regression, which instead Minimizes the following:\n\n\\[\nrSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K\\beta_k^2\n\\]\n\nThis essentially aims to find parameters that reduces SSR, but also “controls” for how large \\(\\beta's\\) can be, using a shrinkage penalty that depends on \\(\\lambda\\).\nIf \\(\\lambda = 0\\) you get Standard OLS, and if \\(\\lambda \\rightarrow \\infty\\) , you get a situation where all betas (but the constant) are zero. For intermediate values, you may have better models than OLS, because you can balance Bias (when \\(\\beta's\\) are zero) with increase variance (when all \\(\\beta's\\) vary as they “please”)"
  },
  {
    "objectID": "02OLS.html#section-8",
    "href": "02OLS.html#section-8",
    "title": "Linear Regression Model",
    "section": "",
    "text": "We usually start with Ridge, because is relatively Easy to implement, since it has a close form Solution:\n\n\\[\n\\beta = (X'X + \\lambda I)^{-1}{X'y}\n\\]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage!=.\ngen male = 1-female\nmata:\n    y = st_data(.,\"lnwage\")\n    x = st_data(.,\"educ exper female male\"),J(1434,1,1)\n    i0 = I(5);i0[5,5]=0\n    xx = (cross(x,x)) ; xy = (cross(x,y))\n    bb0 = invsym(xx)*xy \n    bb1 = invsym(xx:+i0*1)*xy \n    bb10 = invsym(xx:+i0*10)*xy \n    bb100 = invsym(xx:+i0*100)*xy \n    bb1000 = invsym(xx:+i0*1000)*xy \n    bb0,bb1,bb10,bb100,bb1000\nend \n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female male\"),J(1434,1,1)\n\n:     i0 = I(5);i0[5,5]=0\n\n:     xx = (cross(x,x)) ; xy = (cross(x,y))\n\n:     bb0 = invsym(xx)*xy\n\n:     bb1 = invsym(xx:+i0*1)*xy\n\n:     bb10 = invsym(xx:+i0*10)*xy\n\n:     bb100 = invsym(xx:+i0*100)*xy\n\n:     bb1000 = invsym(xx:+i0*1000)*xy\n\n:     bb0,bb1,bb10,bb100,bb1000\n                  1              2              3              4\n    +-------------------------------------------------------------\n  1 |   .0858251775    .0858183338    .0857563567    .0851046501\n  2 |   .0147342796    .0147345813    .0147372042    .0147554544\n  3 |  -.0949227416    -.047396817   -.0468240416    -.041806663\n  4 |             0     .047396817    .0468240416     .041806663\n  5 |   2.218849962    2.171466638    2.172174327    2.179690914\n    +-------------------------------------------------------------\n                  5\n     ----------------+\n  1     .0778292498  |\n  2     .0146298058  |\n  3    -.0208062854  |\n  4     .0208062854  |\n  5     2.266275433  |\n     ----------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "02OLS.html#lasso-and-elastic-net",
    "href": "02OLS.html#lasso-and-elastic-net",
    "title": "Linear Regression Model",
    "section": "Lasso and Elastic Net",
    "text": "Lasso and Elastic Net\n\nRidge is a relatively easy model to understand and estimate, since it has a close form solution. It has the slight disadvantage that you still estimate a coefficient for “every” variable (tho some are very small)\nAnother approach, that overcomes this advantage is known as Lasso.\n\n\\[\nLSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K |\\beta_k|\n\\]\n\nand the one known as Elastic net\n\n\\[\neSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda_L \\sum_{k=1}^K |\\beta_k| +\n\\lambda_r \\sum_{k=1}^K \\beta_k^2\n\\]\n\nLasso has the advantage of forcing some coefficients exactly to zero, when \\(\\lambda\\) is sufficiently large.\nElastic net tries to use the benefits from both approaches."
  },
  {
    "objectID": "02OLS.html#lasso-vs-ridge",
    "href": "02OLS.html#lasso-vs-ridge",
    "title": "Linear Regression Model",
    "section": "Lasso vs Ridge",
    "text": "Lasso vs Ridge"
  },
  {
    "objectID": "02OLS.html#considerations",
    "href": "02OLS.html#considerations",
    "title": "Linear Regression Model",
    "section": "Considerations:",
    "text": "Considerations:\nAs with many methodologies, the benefits from this approaches is not free.\n\nYou need to choose tuning parameters “wisely” using approaches such as AIC, BIC, or cross validation.\nThe model you get may improve prediction, but inference is not as straight forward.\nIt also requires working with Standardized coefficients. (so the same penalty can be used for all variables in the model.\n\nNevertheless, they can be used as starting point for model selection.\nif interested, look into Stata introduction to Lasso regression. help Lasso intro"
  },
  {
    "objectID": "02OLS.html#brief-example",
    "href": "02OLS.html#brief-example",
    "title": "Linear Regression Model",
    "section": "Brief Example:",
    "text": "Brief Example:\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage!=.\nqui:reg lnwage i.age\npredict p_ols\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)  alpha(0)\npredict p_ridge\nqui:lasso linear lnwage i.age, selection(cv, alllambdas)  \npredict p_lasso\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)   \npredict p_elastic\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n(options xb penalized assumed; linear prediction with penalized coefficients)"
  },
  {
    "objectID": "02OLS.html#shrinking-coefficients",
    "href": "02OLS.html#shrinking-coefficients",
    "title": "Linear Regression Model",
    "section": "Shrinking Coefficients",
    "text": "Shrinking Coefficients\n\nLasso vs Ridge"
  },
  {
    "objectID": "03par_spar_npar.html#introduction",
    "href": "03par_spar_npar.html#introduction",
    "title": "Semi- and Non- Parametric regression",
    "section": "Introduction",
    "text": "Introduction\nWhat exactly do we mean with non parametric??\n\nFirst of all, everything we have done in the last class, concerned to the analysis of parametric relationships between \\(y\\) and \\(X's\\) .\nWhy parametric? Because we assume that the relationship between those variables is linear, so we just need to estimate the parameters of that relationship. (\\(\\beta's\\)). Even tho the CEF was on itself non-parametric.\nThis was just a matter of convince. Instead of trying to estimate all possible conditional means (impossible task?) we impose functional form conditions, to identify the relationship of interest.\nWhen we covered MLE (last semester) we even imposed functional forms assumptions on relationships and distribution!"
  },
  {
    "objectID": "03par_spar_npar.html#so-what-about-non-parametric",
    "href": "03par_spar_npar.html#so-what-about-non-parametric",
    "title": "Semi- and Non- Parametric regression",
    "section": "So what about non-parametric?",
    "text": "So what about non-parametric?\n\nNon-parametric is on the other side of the spectrum. There are no “single” parameters to estimate, but rather it tries to be as flexible as possible, to identify all possible relationships in the data.\nIn terms of distributions, it no longer assumes data distributes as normal, poisson, exponential, etc. Instead, it simply assumes it distributes, however it does. 🤔 But isnt that a problem?\nYes it can be.\n\nOn the one hand Parametric modeling is very “strict” regarding functional forms. (linear quadratic, logs, etc).\nOn the other, Non-parametric can be too flexible. Making the problem almost impossible to solve.\n\nHowever, the benefits of letting your data “speak” for itself, would allow you to avoid some problems with parametric models. At least is some balance can be set on the “flexibility”"
  },
  {
    "objectID": "03par_spar_npar.html#ok-but-what-about-semi-parametric",
    "href": "03par_spar_npar.html#ok-but-what-about-semi-parametric",
    "title": "Semi- and Non- Parametric regression",
    "section": "Ok but what about Semi-parametric!",
    "text": "Ok but what about Semi-parametric!\n\nSemi-parametric models try to establish a mid point between parametric and non-parametric models, attempting to draw from the benefit of both.\n\nIt also helps that it has a smaller computational burden (we will see what do i mean with this.\n\nWhat about an example? Say we are trying to explain “wages” as a function of age and education. (assume exogeneity)\nTheoretical framework : \\[wage = g(age, education, \\varepsilon)\n\\]\nParametric model: \\[wage = b_0 + b_1 age + b_2 education +\\varepsilon\n\\]\nNon-parametric model: \\[\nwage = g(age,education) +\\varepsilon\n\\]\nSemi-parametric model:\n\n\\[wage = b_0 + g_1(age) + g_2(education) +\\varepsilon \\\\ wage = g_0(age)+b1 education+\\varepsilon \\\\ wage = g_0(age)+g_1 (age)education+\\varepsilon\n\\]"
  },
  {
    "objectID": "03par_spar_npar.html#step1-estimation-of-density-functions",
    "href": "03par_spar_npar.html#step1-estimation-of-density-functions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Step1: Estimation of Density functions",
    "text": "Step1: Estimation of Density functions\n\nThe first step towards learning non-parmetric analysis, is by learning to use the most basic task of all.\nEstimating distributions (PDFs) : why? in economics, and other social sciences, we care about distributions!\nDistribution of income, how many live under poverty, how much is concentrated among the rich, how skew the distribution is, what is the level of inequality, etc, etc\nThe parametric approach to estimating distribution, is by using some predefined functional form (say normal), and use the data to estimate the parameters that define that distribution:\n\n\\[\n\\hat f(x) = \\frac{1}{\\sqrt{2\\pi\\hat\\sigma^2}}exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\hat \\mu}{\\hat \\sigma}\\right)^2 \\right)\n\\]"
  },
  {
    "objectID": "03par_spar_npar.html#section",
    "href": "03par_spar_npar.html#section",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Which can be done rather easy in Stata\n\nfrause oaxaca, clear\ndrop if lnwage==.\nsum lnwage\ngen f = normalden(lnwage, r(mean), r(sd))\nhistogram wages\n\nBut as you can see, it does not fit well."
  },
  {
    "objectID": "03par_spar_npar.html#histogram-and-kernel-density",
    "href": "03par_spar_npar.html#histogram-and-kernel-density",
    "title": "Semi- and Non- Parametric regression",
    "section": "Histogram and Kernel Density",
    "text": "Histogram and Kernel Density\nHistograms and Kernel densities (you probably have used a lot) are a type of non-parametric estimators, because they impose no functional form restrictions to estimate probability density functions (PDFs).\nConstruction histograms, is in fact, a fairly Straight forward task:\n\nYou select the width of bins, \\(h\\) , and starting value \\(x_0\\)\n\n\\[if \\ x_i \\in [x_0 + m * h, x_0 + (m+1)h ) \\rightarrow\nbin(x)=m+1\n\\]\n\nAnd the Histogram estimator for density, is given by:\n\n\\[\\hat f (x) = \\frac{1}{nh} \\sum_i 1(bin(x)=bin(x_i))\n\\]\nSimple yet powerful approach to estimate and visualize distributions. But with lots of room for improvement. It may provide very different pictures based on “h”"
  },
  {
    "objectID": "03par_spar_npar.html#histograms-with-varying-h",
    "href": "03par_spar_npar.html#histograms-with-varying-h",
    "title": "Semi- and Non- Parametric regression",
    "section": "Histograms with Varying h",
    "text": "Histograms with Varying h"
  },
  {
    "objectID": "03par_spar_npar.html#kernel-density",
    "href": "03par_spar_npar.html#kernel-density",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kernel density",
    "text": "Kernel density\nAn alternative that overcomes some of the limitations of the Histogram is known as the kernel density estimator. This is defined as:\n\\[\n\\hat f(x) = \\frac{1}{nh}\\sum_i K\\left(\\frac{X_i-x}{h}\\right)\n\\]\nwhere \\(K\\) is what is known as a kernel function.\nThis function is such that has the following properties:\n\\[\n\\int K(z)dz = 1 ; \\int zK(z)dz = 0 ; \\int z^2K(z)dz &lt; \\infty\n\\]\nIs a well behaved pdf on its own, that is symmetric, with defined second moment.\n\nas with the histogram estimator, the Kden is just an average of functions, that has the advantage of being smooth.\nAlthough it also depends strongly, on the choice of bandwidth."
  },
  {
    "objectID": "03par_spar_npar.html#kernel-density-visualization",
    "href": "03par_spar_npar.html#kernel-density-visualization",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kernel density: Visualization",
    "text": "Kernel density: Visualization"
  },
  {
    "objectID": "03par_spar_npar.html#trade-off-bias-vs-variance",
    "href": "03par_spar_npar.html#trade-off-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Trade off: Bias vs variance",
    "text": "Trade off: Bias vs variance\nWhile this estimators are “flexible” in the sense that we impose very simple assumptions for estimation, there is still one parameter that needs attention.\nThe bandwidth \\(h\\)\nThis does not (or cannot) be estimated, rather, needs to be calibrated to balance two problems in Non-parametric analysis. Bias vs Variance:\n\nwhen \\(h\\rightarrow 0\\) , the bias of your estimator goes to zero ( in average). Intuitively \\(\\hat f(x)\\) is constructed based on information that comes from \\(x\\) alone.\nBut the variance increases! Because things will vary for every \\(x\\).\nwhen \\(h \\rightarrow \\infty\\) , the bias increases, because you start using data that is very different to \\(x\\) to estimate \\(\\hat f(x)\\).\nBut variance decreases. Since the “function” is now very smooth (a line?)\n\nThus, special attention is needed to choose the right h, which minimizes the problems (bias and variance)."
  },
  {
    "objectID": "03par_spar_npar.html#kdensity-bias-vs-variance",
    "href": "03par_spar_npar.html#kdensity-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kdensity, Bias vs Variance",
    "text": "Kdensity, Bias vs Variance"
  },
  {
    "objectID": "03par_spar_npar.html#other-considerations",
    "href": "03par_spar_npar.html#other-considerations",
    "title": "Semi- and Non- Parametric regression",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nAs shown above, one needs to choose the bandwidth \\(h\\) carefully, balancing the bias-variance trade off. Common approach is to simply use rule-of-thumb approaches to select this parameter:\n\n\\[\nh = 1.059 \\sigma n ^ {-1/5} \\\\ h = 1.3643 * d * n ^ {-1/5} * min(\\sigma,iqr\\sigma)\n\\]\nBut other approaches may work better.\n\nA second consideration is the choice of Kernel function! (see help kdensity -&gt; kernel)\n\nAlthough, except in few cases, the choice of bandwidth matters more than the kernel function.\n\nThis method works well when your data is smooth and continuous. But for so much for discrete data.\n\nNevertheless, it is still possible to use it with discrete data, and kernel weights!\n\nCan be “easily” extended to multiple dimensions \\(f(x,y,z,...)\\), including mixture of continuous and discrete data. You just multiple Kernels!\n\nBut, beware of Curse of dimensionality.\nBut still better than just Subsampling!"
  },
  {
    "objectID": "03par_spar_npar.html#kfunctions",
    "href": "03par_spar_npar.html#kfunctions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kfunctions",
    "text": "Kfunctions"
  },
  {
    "objectID": "03par_spar_npar.html#np---regression",
    "href": "03par_spar_npar.html#np---regression",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP - Regression",
    "text": "NP - Regression\n\nAs hinted from the beginning, the idea of non-parametric regressions is related to estimate a model that is as flexible as it can probably be.\nThis relates to the CEF, where we want to estimate a conditional mean for every combination of X’s. In other words, you aim to estimate models that are valid “locally”. A very difficult task.\n\nYou have a limited sample size\nYou may not see all possible X’s combinations\nand for some, you may have micro-samples (n=1) Can you really do something with this?\n\nYes, make your model flexible, but not overly flexible! but how?\n\nKernel regression ; Spline regression\nPolynomial regression; Smoothed Spline regression."
  },
  {
    "objectID": "03par_spar_npar.html#univariate-case",
    "href": "03par_spar_npar.html#univariate-case",
    "title": "Semi- and Non- Parametric regression",
    "section": "Univariate case",
    "text": "Univariate case\n\nConsider a univariate case \\(y,X\\) where you only have 1 indep variable, which are related as follows:\n\n\\[\ny = m(x) + e\n\\]\nwhich imposes the simplifying assumption that error is additive.\n\nIn the parametric case:\n\n\\[\ny =b_0 + b_1 x + b_2 x^2 +b_3 x^3 +...+e\n\\]\n(this is, in fact, starting to become less parametric)\n\nBut in the full (unconstrained) model it would just be (simple conditional mean:\n\n\\[\nE(y|X) = \\hat m(x) = \\frac{\\sum y_i 1(x_i=x)}{\\sum 1(x_i=x)}\n\\]\nProblems? Impossible to do out out sample predictions, and if \\(n&lt;42\\) inference would be extremely unreliable."
  },
  {
    "objectID": "03par_spar_npar.html#local-constant-regression",
    "href": "03par_spar_npar.html#local-constant-regression",
    "title": "Semi- and Non- Parametric regression",
    "section": "Local Constant Regression",
    "text": "Local Constant Regression\nWe can improve over the Unconstrained mean using the following connection:\n\n\\(1(x_i=x)\\) is a non smooth indicator that shows if an observation is included (counted towards the mean).\nIt can be substituted with \\(K_h(x_i,x)\\), which is the standardized kernel function. (ie \\(K_h(x_i,x) = \\frac{1}{h} K(\\frac{x_i-x}{h})\\)\nDepending on \\(h\\) , it gives the most weight the closer \\(x_i\\) is to \\(x\\).\nThis gives what is known as the Nadaraya-Watson or Local constant estimator:\n\n\\[\n\\hat m(x) = \\frac{\\sum y_i K_h(x_i,x)}{\\sum K_h(x_i,x)} = \\sum y_i w_i\n\\]\nWhich, on its core, is simply a weighted regression, with weights given by \\(\\frac{K_h(x_i,x)}{\\sum K_h(x_i,x)}\\)\n\nKernel Regressions “borrows” info from neighboring observations to obtain a smooth estimator."
  },
  {
    "objectID": "03par_spar_npar.html#visuals",
    "href": "03par_spar_npar.html#visuals",
    "title": "Semi- and Non- Parametric regression",
    "section": "Visuals",
    "text": "Visuals"
  },
  {
    "objectID": "03par_spar_npar.html#considerations",
    "href": "03par_spar_npar.html#considerations",
    "title": "Semi- and Non- Parametric regression",
    "section": "Considerations",
    "text": "Considerations\n\nLocal Constant estimator is simple to estimate with a single variable. And so with multiple variables:\n\n\\[\n\\hat m(x,z) = \\frac{\\sum y_i K_h(x_i,x) \\times K_h(z_i,z)}{\\sum K_h(x_i,x) \\times K_h(z_i,z)}\n\\]\nThe problem, however, lies on the curse of dimensionality. - More dimensions, less data per \\((x,z)\\) point, unless you “increase” bandwidths.\n\nAs Before, it all depends on the Bandwidth \\(h\\).It determines the “flexibility” of the model.\nThe local constant tends to have considerable bias (specially near limits of the distribution, or when \\(g\\) has too much curvature)"
  },
  {
    "objectID": "03par_spar_npar.html#choosing-h",
    "href": "03par_spar_npar.html#choosing-h",
    "title": "Semi- and Non- Parametric regression",
    "section": "Choosing h",
    "text": "Choosing h\nThe quality of the NPK regression depends strongly on the choice of \\(h\\). And as with density estimation, the choice translates into a tradeoff between bias and variance of the estimation.\nThere are various approaches to choose \\(h\\). Some which depend strongly on the dimensionality of the model.\nFor Example, Stata command lpoly estimates local constant models, using the following:\n\nBut that is not the only approach.\nAn alternative (used for regularization) is using Cross-Validaton. (a method to evaluate the predictive power of a model)"
  },
  {
    "objectID": "03par_spar_npar.html#cross-validation-intuition",
    "href": "03par_spar_npar.html#cross-validation-intuition",
    "title": "Semi- and Non- Parametric regression",
    "section": "Cross Validation: Intuition",
    "text": "Cross Validation: Intuition\n\nSeparate your data in two parts: Training and testing Sample.\nEstimate your model in the TrainS, and evaluate predictive power in TestS.\nTo obtain a full view of predictive power, Repeat the process rotating the training set\n\n\\[\nmse = \\frac{1}{N}\\sum(y_i - g_{-k}(x))^2\n\\]\nThis should give you a better idea of the predictive power of the model."
  },
  {
    "objectID": "03par_spar_npar.html#cross-validation-in-stata",
    "href": "03par_spar_npar.html#cross-validation-in-stata",
    "title": "Semi- and Non- Parametric regression",
    "section": "Cross-validation in Stata",
    "text": "Cross-validation in Stata\nfrause oaxaca, clear\nssc isntall cv_kfold\n\nqui:reg lnwage educ exper tenure female age\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.45838\n\nqui:reg lnwage c.(educ exper tenure female age)\n               ##c.(educ exper tenure female age)\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.42768\n\n. qui:reg lnwage c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n\n. cv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.43038\n\nssc install cv_regress\n* Does lOOCV for regression\ncv_regress\n\nLeave-One-Out Cross-Validation Results \n-----------------------------------------\n         Method          |    Value\n-------------------------+---------------\nRoot Mean Squared Errors |       0.4244\nLog Mean Squared Errors  |      -1.7144\nMean Absolute Errors     |       0.2895\nPseudo-R2                |      0.36344\n-----------------------------------------"
  },
  {
    "objectID": "03par_spar_npar.html#loocv",
    "href": "03par_spar_npar.html#loocv",
    "title": "Semi- and Non- Parametric regression",
    "section": "LOOCV",
    "text": "LOOCV\nBecause the “choice” of “folds” and Repetitions, and the randomness, may produce different results every-time, one also has the option of using the “leave-one-out” approach.\nThis means, leave one observation out, and use the rest to make the predictions.\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n(y_i - \\hat g_{-i}(x_i))^2\n\\]\nThis is not as bad as it looks, since we can use the shortcut\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n\\left(\\frac{y_i - \\hat g(x_i)}{1-w_i/\\Sigma w_j}\\right)^2\n\\]\nIn Stata, the command npregress kernel uses this type of cross-validation to determine “optimal” \\(h\\)\nlpoly y x, kernel(gaussian) nodraw\ndisplay r(bwidth)\n.23992564\nnpregress kernel y x, estimator(constant) noderiv\n. Bandwidth\n-------------------------\n             |      Mean \n-------------+-----------\n           x |  .4064052 \n-------------------------"
  },
  {
    "objectID": "03par_spar_npar.html#extending-from-constant-to-polynomial",
    "href": "03par_spar_npar.html#extending-from-constant-to-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Extending from constant to Polynomial",
    "text": "Extending from constant to Polynomial\nAn alternative way to understanding the simple NW (local constant) regressions, is to understand it as a local regression model with anything but a constant:\n\\[\n\\hat m(x)=min\\sum(y_i - \\beta_0)^2 w(x,h)_i\n\\]\nThis means that you could extend the analogy and include “centered” polynomials to the model.\n\\[\n\\begin{aligned}\nmin &\\sum(y_i - \\beta_0 - \\beta_1 (x_i -x) -\\beta_2 (x_i - x) ^2 - ...-\\beta_k(x_i-x)^k)^2 w(x,h)_i \\\\\n\\hat m(x) &= \\hat \\beta_0\n\\end{aligned}\n\\]\nThis is called the local polynomial regression.\n\nBecause its more flexible, it shows less bias when the true function shows a lot of variation.\nBecause of added polynomials, it requires more information (larger \\(h\\))\nIt can be used to easily obtain local marginal effects.\nAnd can also be used with multinomial models (local planes)\n\n\\[min \\sum (y_i - \\beta_0 - \\beta_1 (x_i-x) - \\beta_2 (z_i-z))^2 w(x,z,h)\n\\]"
  },
  {
    "objectID": "03par_spar_npar.html#local-constant-to-local-polynomial",
    "href": "03par_spar_npar.html#local-constant-to-local-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Local Constant to Local Polynomial",
    "text": "Local Constant to Local Polynomial\n\n\nwebuse motorcycle\ntwo scatter accel time || ///\nlpoly accel time , degree(0) n(100) || ///\nlpoly accel time , degree(1) n(100) || ///\nlpoly accel time , degree(2) n(100) || ///\nlpoly accel time , degree(3) n(100) , ///\nlegend(order(2 \"LConstant\" 3 \"Local Linear\" ///\n4 \"Local Cubic\" 5 \"Local Quartic\"))"
  },
  {
    "objectID": "03par_spar_npar.html#statistical-inference",
    "href": "03par_spar_npar.html#statistical-inference",
    "title": "Semi- and Non- Parametric regression",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nFor Statistical Inference, since each regression is just a linear model, standard errors can be obtained using the criteria as in Lecture 1. (Robust, Clustered, bootstrapped).\n\nWith perhaps one caveat. Local estimation and standard errors may need to be estimated “globally”, rather than locally.\n\nThe estimation of marginal effects becomes a bit more problematic.\n\nLocal marginal effects are straightforward (when local linear or higher local polynomial is used)\nGlobal marginal effects, can be obtained averaging all local marginal effects.\nHowever, asymptotic standard errors are difficult to obtain (consider the multiple correlated components), but bootstrapping is still possible."
  },
  {
    "objectID": "03par_spar_npar.html#stata-example",
    "href": "03par_spar_npar.html#stata-example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Stata Example",
    "text": "Stata Example\nfrause oaxaca\nnpregress kernel lnwage age exper\n\nComputing mean function\n  \nMinimizing cross-validation function:\n  \nIteration 0:   Cross-validation criterion = -1.5904753  \nIteration 1:   Cross-validation criterion = -1.5906158  \nIteration 2:   Cross-validation criterion = -1.5907243  \nIteration 3:   Cross-validation criterion = -1.5911389  \nIteration 4:   Cross-validation criterion = -1.5911389  \nIteration 5:   Cross-validation criterion = -1.5911855  \nIteration 6:   Cross-validation criterion = -1.5912075  \n  \nComputing optimal derivative bandwidth\n  \nIteration 0:   Cross-validation criterion =  .01378252  \nIteration 1:   Cross-validation criterion =   .0019967  \nIteration 2:   Cross-validation criterion =  .00196967  \nIteration 3:   Cross-validation criterion =  .00196371  \n\nBandwidth\n------------------------------------\n             |      Mean     Effect \n-------------+----------------------\n         age |  2.843778   15.10978 \n       exper |  3.113587   16.54335 \n------------------------------------\n\nLocal-linear regression                    Number of obs      =          1,434\nKernel   : epanechnikov                    E(Kernel obs)      =          1,434\nBandwidth: cross-validation                R-squared          =         0.3099\n------------------------------------------------------------------------------\n      lnwage |   Estimate\n-------------+----------------------------------------------------------------\nMean         |\n      lnwage |   3.339269\n-------------+----------------------------------------------------------------\nEffect       |\n         age |   .0169326\n       exper |  -.0010196\n------------------------------------------------------------------------------\nNote: Effect estimates are averages of derivatives.\nNote: You may compute standard errors using vce(bootstrap) or reps()."
  },
  {
    "objectID": "03par_spar_npar.html#other-types-of-non-parametric-models",
    "href": "03par_spar_npar.html#other-types-of-non-parametric-models",
    "title": "Semi- and Non- Parametric regression",
    "section": "Other types of “non-parametric” models",
    "text": "Other types of “non-parametric” models\nWe have explored the basic version of non-parametric modeling. But its not the only one.\nThere are at least two others that are easy to implement.\n\nNonparametric Series Regression (we will see this)\nSmoothing series/splines: Which borrows from Series regression and Ridge Regression."
  },
  {
    "objectID": "03par_spar_npar.html#non-parametric-series",
    "href": "03par_spar_npar.html#non-parametric-series",
    "title": "Semi- and Non- Parametric regression",
    "section": "Non-parametric series",
    "text": "Non-parametric series\nThis approach assumes that model flexibility can achieve by using “basis” functions in combination with Interactions, but using “global” regressions (OLS)\nBut what are “basis” functions? They are a collection of terms that approximates smooth functions arbitrarily well.\n\\[\n\\begin{aligned}\ny &= m(x,z)+e  \\\\\nm(x,z)  &= B(x)+ B(z)+B(x)*B(z)  \\\\\nB(x)  &= (x, x^2, x^3,...) \\\\\nB(x)  & = fracPoly \\\\\nB(x)  &= (x, max(0,x-c_1), max(0,x-c_2), ... \\\\\nB(x)  &= (x,x^2,max(0,x-c_1)^2, max(0,x-c_2)^2, ... \\\\\nB(x)  &= B-splines\n\\end{aligned}\n\\]\n\nPolynomials can be used, but there may be problems with high order polynomials. (Runge’s phenomenon,multiple-co-linearity). They are “global” estimators.\nFractional polynomials: More flexible than polynomials, without producing “waves” on the predictions\nNatural Splines, are better at capturing smooth transitions (depending on degree). Require choosing Knots appropriately.\nB-splines are similar to N-splines, but with better stat properties. Also require choosing knots\n\nExcept for correctly estimating the Bases functions (fracpoly and Bsplines are not straight forward), estimation requires simple OLS."
  },
  {
    "objectID": "03par_spar_npar.html#np-series---tuning",
    "href": "03par_spar_npar.html#np-series---tuning",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - tuning",
    "text": "NP series - tuning\n\nWhile NP-series are easy to estimate, we also need to address problems of over-fitting.\nWith Polynomial: What degree of polynomial is correct? What about the degree of the interactions?\nFractional Polynomials: How many terms are needed, what would their “degrees” be.\nNsplines, Bsplines: How to choose degree? and where to set the knots?\n\nThese questions are similar to the choosing \\(h\\) in kernel regressions. However, model choice is simple…Cross validation.\n\nEstimate a model under different specifications (cut offs), and compare the out-of-sample predictive power. (see Stata: cv_kfold or cv_regress)\n\nOne more problem left. Statistical Inference"
  },
  {
    "objectID": "03par_spar_npar.html#np-series---se-and-mfx",
    "href": "03par_spar_npar.html#np-series---se-and-mfx",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - SE and Mfx",
    "text": "NP series - SE and Mfx\nLecture 1 applies here. Once the model has been chosen, you can estimate SE using appropriate methods. There is only one caveat\n\nStandard SE estimation ignores the uncertainty of choosing cut-offs or polynomial degrees. In principle, cut-offs uncertainty can be modeled. But requires non-linear estimation.\n\nMarginal effects are somewhat easier for some basis. Just take derivatives:\n\\[\ny = b_0 + b_1 x + b_2 x^2 +b_3 max(0,x-c)^2 + e \\\\\n\\frac{dy}{dx}=b_1 + 2 b_2 x + 2 b_3 (x-c) 1(x&gt;c)\n\\]\n\nBut keeping track of derivatives in a multivariate model is difficult, and often, the functions are hard to track down. so how to implement it?"
  },
  {
    "objectID": "03par_spar_npar.html#np-series-implementation-marginal-effects",
    "href": "03par_spar_npar.html#np-series-implementation-marginal-effects",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series: Implementation marginal effects",
    "text": "NP series: Implementation marginal effects\nAs always, it all depends on how are the models estimated.\n\nStata command npregress series allows you to estimate this type of models using polynomials, splines and B-splines. And also allows estimates marginal effects for you. (can be slow)\nfp estimates fractional polynomials, but does not estimate marginal effects for you.\nR is a bit more flexible in terms of tracking down functions as variables (~x+I(x*x)+ln(x)). So it may be “easy” to estimate those effects.\nIn Stata, you can use the package f_able to estimate those marginal effects, however. see here for details. and SSC for the latest update.\n\nfrause oaxaca, clear\ndrop agesq\nf_spline age = age, nk(1) degree(3)\nf_spline exper = exper, nk(1) degree(3)\nqui:regress lnwage c.(age*)##c.(exper*)\nf_able age? exper?, auto \nmargins, dydx(age exper) noestimcheck \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0360234   .0033909    10.62   0.000     .0293775    .0426694\n       exper |   .0082594   .0050073     1.65   0.099    -.0015547    .0180735\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "03par_spar_npar.html#semiparametric-regressions",
    "href": "03par_spar_npar.html#semiparametric-regressions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Semiparametric Regressions",
    "text": "Semiparametric Regressions\n\nFull non-parametric estimations are powerful to identify very flexible functional forms. To avoid overfitting, however, one must choose tuning parameters appropriately (\\(h\\) and \\(cutoffs\\) ).\nA disadvantage: Curse of dimensionality. More variables need more data to provide good results. But, the more data you have, the more difficult to estimate (computing time).\nIt also becomes extremly difficult to interpret. (too much flexibility)\nAn alternative, Use the best of both worlds: Semiparametric regression\n\nFlexiblity when needed with the structure of standard regressions, to avoid the downfalls of fully nonparametric models"
  },
  {
    "objectID": "03par_spar_npar.html#partially-linear-model",
    "href": "03par_spar_npar.html#partially-linear-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Partially Linear model",
    "text": "Partially Linear model\n\\[\ny = x\\beta +g(z) +e\n\\]\nThis model assumes that only a smaller set of covariates need to be estimated non-parametrically in the model.\nEstimators:\n\nnpregress series: Use BasisF to estimate \\(g(z)\\) . Or other series regressions, fractional polynomial fp\nYatchew 1997: For a single z, sort variables by it, and estimate: \\(\\Delta y=\\Delta X\\beta+ \\Delta g(z) + \\Delta e\\). This works because \\(\\Delta g(z)\\rightarrow 0\\)\nEstimate \\(g(z)\\) regressing \\(y-x\\hat \\beta\\) on \\(z\\). See plreg\nRobinson 1988: Application of FWL. Estimate \\(y = g_y(z)+e_y\\) and \\(x = g_x(z)+e_x\\) and estimate \\(\\beta = (e_x ' e_x)^{-1} e_x ' e_y\\) . For \\(g(z)\\) same as before. See semipar.\nOther methods available see semi_stata"
  },
  {
    "objectID": "03par_spar_npar.html#generalized-additive-model",
    "href": "03par_spar_npar.html#generalized-additive-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Generalized Additive model",
    "text": "Generalized Additive model\n\\[\ny = g(x) +g(z)+e\n\\]\nThis model assumes the effect of X and Z (or any other variables) are additive separable, and may have a nonlinear effect on y.\n\nnpregress series: with non-interaction option. Fractional polynomials mfp, cubic splines mvrs (see mvrs) , or manual implementation.\nKernel regression possible. (as in Robinson 1988), but requires an iterative method. (back fitting algorithm)\n\n\\(g(x) = smooth (y-g(z)|x)\\), center \\(g(x)\\) , and \\(g(z) = smooth (y-g(x)|z)\\), center \\(g(z)\\) until convergence\n\nIn general, it can be easy to apply, but extra work required for marginal effects."
  },
  {
    "objectID": "03par_spar_npar.html#smooth-coefficient-model",
    "href": "03par_spar_npar.html#smooth-coefficient-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Smooth Coefficient model",
    "text": "Smooth Coefficient model\n\\[\ny = g_0(z)+g_1(z)x + e\n\\]\nThis model assumes that \\(X's\\) have a locally linear effect on $y$, but that effect varies across values of \\(z\\), in a non-parametric way.\n\nfp or manual implementation of basis functions, with interaction. May allow for multiple variables in \\(z\\)\nOne can also use Local Kernel regressions. locally weighted regression where All X variables are considered fixed, or interacted with polynomials of Z. Choice of bandwidth problematic, but doable (LOOCV).\nvc_pack can estimate this models with a single z, as well as test it. Overall marginal effects still difficult to obtain."
  },
  {
    "objectID": "03par_spar_npar.html#example",
    "href": "03par_spar_npar.html#example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nfrause oaxaca\nnpregress kernel lnwage age exper\nmargins, dydx(*)\n\nvc_bw lnwage educ exper tenure female married divorced, vcoeff(age)\nvc_reg lnwage educ exper tenure female married divorced, vcoeff(age) k(20)\nssc install addplot\nvc_graph educ exper tenure female married divorced, rarea\naddplot grph1:, legend(off) title(Education)\naddplot grph2:, legend(off) title(Experience)\naddplot grph3:, legend(off) title(Tenure)\naddplot grph4:, legend(off) title(Female)\naddplot grph5:, legend(off) title(Married)\naddplot grph6:, legend(off) title(Divorced)\n\ngraph combine grph1 grph2 grph3 grph4 grph5 grph6"
  },
  {
    "objectID": "03par_spar_npar.html#example-1",
    "href": "03par_spar_npar.html#example-1",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nWage Profile across years"
  },
  {
    "objectID": "04cqreg.html#introduction",
    "href": "04cqreg.html#introduction",
    "title": "Conditional Quantile Regressions",
    "section": "Introduction",
    "text": "Introduction\nQuestion: What are quantiles? and why do we care??\n\nQuantiles are statistics that have the purpose of providing a better characterization of distributions.\n\nThis is possible, because it provides you with more information than standard summary statistics (means and variance)\n\nHow so? In general, there are 3 ways you can use to know “everything” about a distribution.\n\nYou either have access to every single \\(y_i\\)\nOr you know the distribution function \\(f(y)\\) (or probability density function pdf)\nOr you know the cumulative distribution function \\(F(y)=\\int_\\infty^y f(t) dt = P(Y\\leq y)\\)\n\nHowever, there is an additional way. Quantile:\n\n\\[\nQ(\\theta) = F^{-1}(p)\n\\]\n\nWhich in principle, is nothing but the inverse cumulative density function."
  },
  {
    "objectID": "04cqreg.html#section",
    "href": "04cqreg.html#section",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "\\(Q(\\theta) = F^{-1}(\\theta)\\)"
  },
  {
    "objectID": "04cqreg.html#other-advantages-yes",
    "href": "04cqreg.html#other-advantages-yes",
    "title": "Conditional Quantile Regressions",
    "section": "Other advantages? Yes!",
    "text": "Other advantages? Yes!\n\nQuantiles are far more stable in the presence of outliers. Because of this, they are particularly useful as measures of central tendency (perhaps superior to the mean) (🤔?)\n\nSimple “test”. In the small town of Troy-NY one of the residents wins the 2B$ lottery. How much has welfare increase for the average resident?\n\nScaled IQR can be used as an alternative measure of dispersion.\n\n\\[\nse2 = \\frac{Q_{75}-Q_{25}}{1.34898}\n\\]\n\nThey are also “function-transformation” resistant: \\(exp(Q_{log(y)} (.10)) = Q_y(.10)\\)\nAnd are also very easy to estimate:\n\nSort data by y \\(\\rightarrow\\) Obtain weighted ranks \\(\\rightarrow\\) choose the lowest value so that \\(\\theta\\) % of the data is less of equal to that number\n\n\n\\[\nF^{-1} (tau) = inf(x: F(x)\\geq t)\n\\]\nThis “just” requires obtaining an approximation for \\(F(\\theta)\\), which can be approximated using nonparametric methods!\n\\[\\hat F(x) = \\frac{1}{N}\\sum (K_F(x,x_i,h)) =\n\\frac{1}{N}\\sum 1(x_i&lt;x)\n\\]\nthen we simply “invert” the function for whichever quantile we are interested in."
  },
  {
    "objectID": "04cqreg.html#statistical-inference",
    "href": "04cqreg.html#statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nAs with the mean, sampling quantiles are measured with sampling error. Thus its important to recognize its sampling distribution.\nHowever, because of the nature of how quantiles are defined, their standard errors are not as intuitive to obtain, although they can be derived using the delta Method. We start from:\n\\[Q_y(\\tau) = F_y^{-1}(\\tau) \\rightarrow F_y(Q_y(\\tau)) = \\tau\n\\]\n\\[1 = f_y(Q_y(\\tau)) \\frac{dQ}{d\\tau} \\rightarrow \\frac{dQ}{d\\tau} =  \\frac{1}{f(Q_y(\\tau))}\n\\]\nSo we have:\n\\[\n\\begin{aligned}\n\\hat Q_y(\\tau) - Q_y(\\tau) \\simeq \\frac{1}{f(Q_y(\\tau)}(\\hat \\tau-\\tau) \\\\\nVar(\\hat Q_y(\\tau)) = \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{N^{-1} \\tau(1-\\tau)}{f^2(Q_y(\\tau))}\n\\end{aligned}\n\\]\nLets understand this elements"
  },
  {
    "objectID": "04cqreg.html#quantile-se",
    "href": "04cqreg.html#quantile-se",
    "title": "Conditional Quantile Regressions",
    "section": "Quantile SE",
    "text": "Quantile SE\n\\[\nVar(\\hat Q_y(\\tau)) = \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{N^{-1} \\tau(1-\\tau)}{f^2(Q_y(\\tau))}\n\\]\n\nthe variance of a quantile depends on the distribution of \\(\\tau\\) which is nothing else that the distribution of a Bernoulli experiment: Is \\(y\\geq Q_y\\) or \\(y&lt;Q_y\\).\n\nThis is the largest near the center of the distribution (50%-50%) but smaller (more precise) near the tails of the distribution (more certainty that something will be larger or smaller.\n\nBut also depends on the density of the distribution.\n\nMore precise estimates when the density is high (center), but less precise near tails of the distribution.\n\nAnd as usual, it depends on the sample size (N) (for more precision one needs more data)\n\nA minor problem. This depends on \\(f()\\). Unless this is known, is another source of variation! (that we usually ignore\n\nOf course, you also have the alternative method. Bootstrap!"
  },
  {
    "objectID": "04cqreg.html#example",
    "href": "04cqreg.html#example",
    "title": "Conditional Quantile Regressions",
    "section": "Example",
    "text": "Example\nfrause wage2, clear\nbootstrap q10=r(r1) q25=r(r2) q50=r(r3) q75=r(r4) q90=r(r5), reps(1000): _pctile wage  , p(10 25 50 75 90)\n\nBootstrap results                                        Number of obs =   935\n                                                         Replications  = 1,000\n\n      Command: _pctile wage, p(10 25 50 75 90)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         q10 |        500   8.720396    57.34   0.000     482.9083    517.0917\n         q25 |        668   14.49296    46.09   0.000     639.5943    696.4057\n         q50 |        905   14.61303    61.93   0.000      876.359     933.641\n         q75 |       1160   20.18665    57.46   0.000     1120.435    1199.565\n         q90 |       1444   33.10919    43.61   0.000     1379.107    1508.893\n------------------------------------------------------------------------------\n\n* Analytical\nsort wage\ngen w1 = _n\ngen w0 = _n-1\nby wage:gen p=0.5*(w1[_N]+w0[1])/935\nkdensity wage, at(wage) gen(fwage)\nreplace se = sqrt(p*(1-p)/935)/fwage\ntabstat wage se if inlist(wage,500,668,905,1160,1444), by(wage)\n\n\n    wage |      wage        se\n---------+--------------------\n     500 |       500  13.27634\n     668 |       668  14.78419\n     905 |       905  14.69217\n    1160 |      1160   19.3574\n    1444 |      1444  29.32711\n---------+--------------------\n   Total |  730.7619  15.88035\n------------------------------"
  },
  {
    "objectID": "04cqreg.html#from-q_y-to-q_yx",
    "href": "04cqreg.html#from-q_y-to-q_yx",
    "title": "Conditional Quantile Regressions",
    "section": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)",
    "text": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)\nThe previous approaches used to identify a particular quantile are not the only ones.\nJust like we can use OLS to estimate Means (Can you prove it?), we could also use a similar method to estimate the median. We only need to change the loss function \\(L()\\) from an \\(L^2\\) to a \\(|L|\\).\nConsider this:\n\\[\nmedian(Y) = min_\\mu \\frac{1}{N}\\sum |y-\\mu|=\\frac{2}{N}\\sum (y-u)(0.5-I([y-u]&lt;0)\n\\]"
  },
  {
    "objectID": "04cqreg.html#section-1",
    "href": "04cqreg.html#section-1",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Q and Loss functions\nWhy does it matter?\n\nThe loss function for Quantiles does not penalize “errors” as much as \\(L^2\\) does. This is why its more robust to outliers (almost not affected by them). (the R2 will also need to be changed)\nHowever, the loss function is no longer differentiable (is discontinuous). So requires other methods to find the solution. Even tho it may not look like that:"
  },
  {
    "objectID": "04cqreg.html#section-2",
    "href": "04cqreg.html#section-2",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "From B to XB\nKoenker and Bassett (1978) extended this last approximation in two ways:\n\nAllows for Covariates (\\(X's\\)) variation\nAllows to identify other quantiles in the distribution:\n\n\\[ \\beta(\\tau) = \\underset {b}{min} \\ N^{-1} \\sum \\rho_\\tau(y_i-X_i'b) \\\\\n\\rho_\\tau (u) = u (\\tau-I(u&lt;0))\n\\]\n\nThis implicitly states that you want to find a combination of \\(X's\\) such that \\(\\tau\\) proportion of \\(y_i\\) are lower than the \\(X_i'\\beta(\\tau)\\) .\n\nBut because we are using controls, we also need that, conditional on \\(x^c\\), \\(\\tau\\)% is lower than \\(x'^{c}\\beta\\)\n\n\n\\[Q_{Y|X}(\\tau) = \\beta_0(\\tau) + \\beta_1(\\tau)x_1 +...+ \\beta_k(\\tau)x_k\n\\]"
  },
  {
    "objectID": "04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "href": "04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "title": "Conditional Quantile Regressions",
    "section": "Interpretation: Why is it so different from OLS?",
    "text": "Interpretation: Why is it so different from OLS?\n\nIn Rios-Avila and Maroto(2022?) we stress that OLS can be interpreted at different “levels”. So consider the following:\n\n\\[\ny_i = b_0 + b_1 x_1 + b_2 x_2  + e\n\\]\nIf there errors are exogenous, and there is no heteroskedasticty, you can “obtain” marginal effects at many levels:\n\\[\n\\begin{aligned}\nInd &: \\frac{dy_i}{dx_{1i}}=b_1 \\\\\nCond &: E(y_i|X=x)=b_0 + b_1 x_1 + b_2 x_2 \\\\\n&: \\frac{dE(y|x)}{dx_1} =b_1 \\\\\nUcond &: E(y_i) = b_0 + b_1 E(x_1) + b_2 E(x_2) \\\\\n&:  \\frac{dE(y)}{dE(x_1)} =b_1\n\\end{aligned}\n\\]\n\nSo in OLS, assuming a linear model in parameters, Nothing changes. The effect is the same! (although magnitude of the “experiment” changes)"
  },
  {
    "objectID": "04cqreg.html#but-cqreg",
    "href": "04cqreg.html#but-cqreg",
    "title": "Conditional Quantile Regressions",
    "section": "But CQreg?",
    "text": "But CQreg?\nFor quantile regressions, things are not that simple.\n\nThere is no “individual” level quantile effect, because we do not observe individual ranks \\(\\tau\\) . If we could observe them, and we assume they are fixed, then one can obtain individual level effects.\nBecause \\(\\tau\\) is unobserved, all Qregression coefficients, should be interpreted as effects on Conditional Distributions (thus the name CQREG).\n\nIn other words, effects are just expected changes in some points in the distribution.\n\nYou cannot use it for unconditional effects either (not easily), because\n\n\\[\nE(Q_{Y|X}(\\tau)) \\neq Q_Y(\\tau)\n\\]\nand you cannot “simply” average the CQREG effects to get unconditional.\nBut what does it mean? This means that CQREG interpretation are percentile \\(\\tau\\) and covariate \\(X\\) specific. But that is not all:\n\nFixed rank. If you happen to be on the top of the distribution (and stay there), the quantile effect is given by the \\(\\beta(\\tau)\\)\nChanges in Conditional distributions: What we see are how distribution changes along characteristics\n\nSo this must be kept in mind, whenever one interpret results"
  },
  {
    "objectID": "04cqreg.html#example-wages",
    "href": "04cqreg.html#example-wages",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…\nfrause oaxaca, clear\nqreg  lnwage educ exper tenure female, nolog q(10)\nest sto m1\nqreg  lnwage educ exper tenure female, nolog q(50)\nest sto m2\nqreg  lnwage educ exper tenure female, nolog q(90)\nest sto m3\n* ssc install estout\nesttab m1 m2 m3, nogaps mtitle(q10 q50 q90)\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      q10             q50             q90   \n------------------------------------------------------------\neduc                0.103***       0.0694***       0.0639***\n                   (6.21)         (16.03)          (7.09)   \nexper              0.0200***      0.00758***      0.00402   \n                   (4.06)          (5.91)          (1.50)   \ntenure           0.000669         0.00657***      0.00774*  \n                   (0.11)          (4.19)          (2.37)   \nfemale             -0.151         -0.0689**       -0.0543   \n                  (-1.87)         (-3.29)         (-1.24)   \n_cons               1.462***        2.474***        2.984***\n                   (6.67)         (43.36)         (25.10)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n** Also\n** ssc install qregplot\nqregplot educ exper tenure female, cons q(5/95)"
  },
  {
    "objectID": "04cqreg.html#example-wages-1",
    "href": "04cqreg.html#example-wages-1",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…"
  },
  {
    "objectID": "04cqreg.html#random-coefficents",
    "href": "04cqreg.html#random-coefficents",
    "title": "Conditional Quantile Regressions",
    "section": "Random coefficents",
    "text": "Random coefficents\nOne approach to both understanding, and simulating QREG is by also understanding the intuition behind the data generating process.\n\\[\n\\begin{aligned}\ny = b_0(\\tau)+b_1(\\tau)x_1 + +b_2(\\tau)x_2+...+b_k(\\tau) x_k \\\\\n\\tau \\sim runiform(0,1)\n\\end{aligned}\n\\]\nwhere all coefficients are some function (preferably monotonically increasing or decreasing) of \\(\\tau\\) .\n\nWe want them to be monotonically increasing or decreasing because we want that \\[\nX B(\\tau_1 ) \\geq\nX B(\\tau_2 ) \\  if \\ \\tau_1 &gt; \\tau_2\\]\n\nThis specification suggest that the unobserved component \\(\\tau\\) is a random indicator a kind to luck. If you are lucky and get a high \\(\\tau\\) then you will have better outcomes than anyone of your peers.\nAlso notice that this setup assumes that \\(\\tau\\) is the only random factor, and should be uncorrelated with \\(X\\) (you do not make your luck!)\nCan you create data with these characteristics?"
  },
  {
    "objectID": "04cqreg.html#svc-model-with-a-latent-running-variable",
    "href": "04cqreg.html#svc-model-with-a-latent-running-variable",
    "title": "Conditional Quantile Regressions",
    "section": "SVC model with a latent running variable",
    "text": "SVC model with a latent running variable\nAnother way you can think of Qreg is to align it to the -semiparametric- method we introduced ealier. SVC model.\nThe difference here is that the running variable is unknown. Given the outcome, and characteristics, however, we can identify something akin to the presence of a “latent” component. (but not really estimating it).\nThere are a few (recent) papers that focus on estimation and identification of these models. The general intuition is that the qreg model is identified by the following moment condition:\n\\[\nE\\Big( 1[x\\beta(\\tau) - y &gt; 0 ] - \\tau \\Big) = 0\n\\]\nbut substitute the indicator function with a smooth function. CDF\n\\[\nE\\Big( F(x\\beta(\\tau) - y) - \\tau \\Big) = 0\n\\]\nBeing differentiable, this problem is relatively easier to solve (given good initial values)"
  },
  {
    "objectID": "04cqreg.html#example-1",
    "href": "04cqreg.html#example-1",
    "title": "Conditional Quantile Regressions",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "04cqreg.html#scale-and-location-model",
    "href": "04cqreg.html#scale-and-location-model",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location Model",
    "text": "Scale and Location Model\nAnother approach that can be used to understand Quantile regressions (and elaborate the interpretation) is to assume that the coefficients are in fact capturing two components:\n\\[y = Xb + Xg(\\tau)\n\\]\n\nLocation: which indicates what is the average relationship between X and Y. \\({b}\\)\nScale: which indicates how far one could be from the average effect, given a relative to its position \\(g(\\tau)\\)\n\nThe model Still assumes random coefficients\nEstimation of this model is not standard. But can be manually implemented:\n\nEstimate OLS and get residuals\nEstimate QREG using those residuals\n\nRequires additional care for the estimation of SE, or just bootstrap"
  },
  {
    "objectID": "04cqreg.html#scale-and-location-2-heteroskedasticity",
    "href": "04cqreg.html#scale-and-location-2-heteroskedasticity",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location 2: Heteroskedasticity",
    "text": "Scale and Location 2: Heteroskedasticity\nA second approach that is useful to understand and interpret CQreg is to consider a parametric version of the LS model:\n\\[y = Xb + \\gamma (X) * e\n\\]\nWhere we assume \\(\\gamma(X)&gt;0\\) . which directly shows the relationship between a quantile regressions and heteroskedasticity in the error term. (typically model as \\(X\\gamma\\))\nBecause Heteroskedasticity is parametric, it constrains the relationship across all quantile coefficients:\n\\[y = X(b+\\gamma F^{-1}(\\tau)) \\rightarrow b(\\tau)=b+\\gamma(\\tau)\n\\]\nMaking it more efficient, albeit imposing constrains of the relationship.\nIt shows more clearly the nature of the overall inequality increasing or decreasing effect."
  },
  {
    "objectID": "04cqreg.html#visual-loc-vs-scale",
    "href": "04cqreg.html#visual-loc-vs-scale",
    "title": "Conditional Quantile Regressions",
    "section": "Visual Loc vs Scale",
    "text": "Visual Loc vs Scale"
  },
  {
    "objectID": "04cqreg.html#visual-loc-vs-scale-1",
    "href": "04cqreg.html#visual-loc-vs-scale-1",
    "title": "Conditional Quantile Regressions",
    "section": "Visual Loc vs Scale",
    "text": "Visual Loc vs Scale"
  },
  {
    "objectID": "04cqreg.html#estimation-and-statistical-inference",
    "href": "04cqreg.html#estimation-and-statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Estimation and Statistical Inference",
    "text": "Estimation and Statistical Inference\nAs hinted previously, there are many approaches that can be used for the estimation of Conditional Quantile regressions.\n\nStata: qreg, sqreg, bsqreg, qreg2, qrprocess, mmqreg, smqreg, sivqr\n\nEach one with its own assumptions. For Standard errors, however, there are 3 options. Under the assumption of iid error. Non iid error (robust), and assuming clustered standard errors.\n\\[\n\\begin{aligned}\niid: \\Sigma_\\beta &=\\frac{\\tau(1-\\tau)}{f^2_y(F^{-1}(\\tau))}(X'X)^{-1} \\\\\nniid: \\Sigma_\\beta &= \\tau(1-\\tau) (X'f(0|x)X)^{-1} \\ (X'X) \\ (X'f(0|x)X)^{-1} \\\\\nalt: \\Sigma_\\beta &= (IF_\\beta \\ ' IF_\\beta) N^{-2}\n\\end{aligned}\n\\]\nOr simply Bootstrap"
  },
  {
    "objectID": "04cqreg.html#problems-and-considerations",
    "href": "04cqreg.html#problems-and-considerations",
    "title": "Conditional Quantile Regressions",
    "section": "Problems and Considerations",
    "text": "Problems and Considerations\n\nUnless otherwise specified, quantile regressions are linear in variables (and parameters?)\nWith few exceptions, quantile regressions are quantile specific. Comparisons across quantiles require joint estimation (to construct VCV matrix)\nBecause they are “local” estimators, there is risk of crossing quantiles. (Violation of Monotonicity)\nNon-linear effects will be present if either the location or scale components are nonlinear.\nQuantile regressions are very sensitive to measurement errors in both dependent and independent variables\nThey can be difficult to interpret (see references)"
  },
  {
    "objectID": "05uqreg.html#introduction",
    "href": "05uqreg.html#introduction",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introduction",
    "text": "Introduction\nAs we saw last class, conditional quantile regressions have only one purpose:\n\nAnalyze relationships between conditional distributions.\n\nThis is a very useful tool!. As it allows you to move beyond Average relationships.\n\nHow do people (who are not all average) would be affected by changes in \\(Xs\\)\n\nThere is a limitation, however. The effects you may estimate, will depend strongly on model specification.\n\nThis is similar to OVB. Changing covariates could drastically change the conditional distributions and associated coefficients\n\nWhat if, you are interested in distributional effects across the whole population! Not only a subsample?"
  },
  {
    "objectID": "05uqreg.html#eqyx-is-not-qy",
    "href": "05uqreg.html#eqyx-is-not-qy",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "E(q(y|X)) is not Q(y)",
    "text": "E(q(y|X)) is not Q(y)\nOne common mistake one makes when analyzing QRegressions is to make interpretations as if the average effects on the \\(qth\\) conditional quantiles would be the same as the effect on the “overall” \\(qth\\) quantile.\nExcept for few cases (when Quantile regressions are not relevant), CQ effects do not translate directly into Changes into the unconditional quantile.\nHowever, as a policy maker, this would be the most relevant estimand you may be interested in :\n\nHow does improving education affect inequality?\nWould eliminating Unionization would increase wage inequality?\nIs there heterogeneity in consumption expenditure?\n\nHowever, going from Conditional to unconditional statistics (not only Q) is not always straight forward."
  },
  {
    "objectID": "05uqreg.html#waitwhat-do-we-mean-unconditional",
    "href": "05uqreg.html#waitwhat-do-we-mean-unconditional",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "⌚Wait…What do we mean unconditional?",
    "text": "⌚Wait…What do we mean unconditional?\nOne of the questions I read a lot regarding UQR is what do we mean unconditional?\n\nThis is perhaps a someone poor choice of words.\nAnytime we estimate ANY statistic, we condition on something.\n\nWe condition on all individual characteristics (including errors)\nWe condition on groups characteristics (CQREG and CEF)\nor, We condition on all characteristics (distributions). We happen to call this, unconditional statistics.\n\nThis, however, does make a big difference in interpretation."
  },
  {
    "objectID": "05uqreg.html#from-condition-on-individuals",
    "href": "05uqreg.html#from-condition-on-individuals",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "From Condition on Individuals,",
    "text": "From Condition on Individuals,\nto conditioning on Distributions\n\\[\\begin{aligned}\ny_i &= b_0 + b_1 x_i + e_i + x_i e_i \\\\\n\\frac{dy_i}{dx_i}&=b_1 + e_i \\\\\nE(y_i|x_i=x) &= b_0 + b_1 x  \\\\\n\\frac{dE(y_i|x)}{dx}&=b_1 \\\\\nE(E(y_i|x_i=x))=E(y_i) &= b_0 + b_1 E(x_i)    \\\\\n\\frac{dE(y_i)}{dE(x_i)}&=b_1\n\\end{aligned}\n\\]\nSame effects, but different interpretations (specially last one)"
  },
  {
    "objectID": "05uqreg.html#how-are-unconditional-effects-estimated",
    "href": "05uqreg.html#how-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How are Unconditional effects Estimated?",
    "text": "How are Unconditional effects Estimated?\nConsider any distributional statistic \\(v\\), which takes as arguments, all observations, density distributions \\(f()\\), or cumulative distributions \\(F()\\).\n\\[\nv = v(F_y) \\ or \\ v(f_y) \\ or \\ v(y_1, y_2, ...,y_n)\n\\]\nAnd to simplify notation, lets say this function is defined as follows:\n\\[\nv(f_y) = \\int_{-\\infty}^\\infty h(y,\\theta) f(y)dy\n\\]\nThis simply considers distributional statistics \\(v\\) that can be estimated by simply integrating a transformation of \\(h(y,\\theta)\\) given a set of parameters \\(\\theta\\).\nBut for now, lets consider only the Identify function \\(h(y,\\theta)=y\\)\nbut…What about Controls??"
  },
  {
    "objectID": "05uqreg.html#introducing-controls",
    "href": "05uqreg.html#introducing-controls",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introducing controls",
    "text": "Introducing controls\nAssume there is a joint distribution of function \\(f(y,x)\\), then\n\\[\n\\begin{aligned}\nf(y,x)&=f(y|x)f(x) \\\\\nf(y) &= \\int f(y|x) f(x) dx\n\\end{aligned}\n\\]\nAnd all together:\n\\[\n\\begin{aligned}\nv(f_y) &= \\int y \\int f(y|x) f(x) dx \\ dy \\\\\nv(f_y) &= \\iint y f(y|x) dy \\ f(x) dx  \\\\\nv(f_y) &= \\int  E(y|X) f(x) dx  \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "05uqreg.html#or-a-bit-more-general",
    "href": "05uqreg.html#or-a-bit-more-general",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Or a bit more General",
    "text": "Or a bit more General\n\\[\nv(f_y) = \\iint h(y,\\theta) f(y|x)f(x)dxdy\n\\]\nSo, the statistic \\(v\\) will change if:\n- We change the function \\(h\\) or its parameters \\(\\theta\\).\n- Assume some shocks that change the conditional \\(f(y|x)\\)\n- or the distribution of characteristics change!\nNote: \\[f(y|x) \\sim \\beta \\text{ and }\nf(x) \\sim x\n\\]"
  },
  {
    "objectID": "05uqreg.html#againhow-are-unconditional-effects-estimated",
    "href": "05uqreg.html#againhow-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Again…How are Unconditional effects Estimated?",
    "text": "Again…How are Unconditional effects Estimated?\nIn an ideal scenario, you simple get the data under to regimes (before and after changes in \\(x\\)), and do the following:\n\\[\\Delta v = v(f'_y)-v(f_y)\n\\]\nThat is, just estimate the statistic in two scenarios (\\(f'\\) and \\(f\\)), and calculate the difference. (impossible!)\nBut there are (at least) three alternatives:\n\nUsing Reweighting approaches to “reshape” the data: \\(f(x)\\) (non-parametric)\nIdentify \\(f(y|x)\\) so one can simulate how \\(\\Delta X\\) affect y\nFocus on the statistic \\(v\\) and indirectly identify the effects of interest. (RIF!)"
  },
  {
    "objectID": "05uqreg.html#op1-re-weighting",
    "href": "05uqreg.html#op1-re-weighting",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op1: Re-weighting",
    "text": "Op1: Re-weighting\nConsider the following. there is a policy such that you plan to improve education in a country. Every single person will have at least 7 years of education, and will have free access to two additional years of education if they want to.\nIn other words, characteristics change from \\(f(x) \\rightarrow g(x)\\) . But you do not see this! \\[ v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{g(x)}dxdy \\]\nbut perhaps, we could see this:\n\\[\\hat v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{\\hat w(x)}f(x) dxdy\n\\]\nif we can come up with a set of weights \\(\\color{red}{\\hat w(x)}\\) such that \\(f(x)\\hat w(x)=g(x)\\)\n\\[\n\\hat w(x) = \\frac{\\hat g(x)}{\\hat f(x)}\n\\]"
  },
  {
    "objectID": "05uqreg.html#op1-re-weighting-1",
    "href": "05uqreg.html#op1-re-weighting-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op1: Re-weighting",
    "text": "Op1: Re-weighting\nSimple, yet hard. Estimation of multivariate densities can be a difficult task. So assume the following\n\\[\nf(x) = h(x|s=0) ; g(x) = h(x|s=1)\n\\]\nThis makes things “easier”.\n\nBayes: \\[\n\\begin{aligned}\nh(x|s=k) &= \\frac{h(x)p(s=k|x)}{p(s=k)} \\\\\n\\hat w(x)  \n     &= \\frac{h(x)p(s=1|x)}{h(x)p(s=0|x)}\\frac{p(s=0)}{p(s=1)} \\\\\n     &=\\frac{p(s|x)}{1-p(s|x)} \\frac{1-p(s)}{p(s)}\n\\end{aligned}\n\\]\n\nEasier to estimate conditional probabilities, (logit probit or other) than Densities"
  },
  {
    "objectID": "05uqreg.html#example",
    "href": "05uqreg.html#example",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\n. *** UQR\n. *** Reweighting\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. ** Create Fake Sample\n. gen id = _n\n\n. expand 2\n(500 observations created)\n\n. bysort id:gen smp = _n ==2\n\n. ** Now you have two of ever person. So lets do some Policy\n. ** Fines increase lower fines more than higher ones, up to 12\n. replace fines = 0.1*(12-fines)+fines if smp==1\n(498 real changes made)\n\n. ** Estimate logit \n. logit smp c.fines##c.fines taxes i.csize college\n\nIteration 0:   log likelihood = -693.14718  \nIteration 1:   log likelihood = -680.74735  \nIteration 2:   log likelihood = -680.68931  \nIteration 3:   log likelihood = -680.68931  \n\nLogistic regression                                     Number of obs =  1,000\n                                                        LR chi2(6)    =  24.92\n                                                        Prob &gt; chi2   = 0.0004\nLog likelihood = -680.68931                             Pseudo R2     = 0.0180\n\n---------------------------------------------------------------------------------\n            smp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n----------------+----------------------------------------------------------------\n          fines |   4.231032   1.762476     2.40   0.016     .7766421    7.685422\n                |\nc.fines#c.fines |  -.1923691   .0877831    -2.19   0.028    -.3644209   -.0203173\n                |\n          taxes |     .00044   .1407545     0.00   0.998    -.2754338    .2763139\n                |\n          csize |\n        Medium  |   .0549913    .161242     0.34   0.733    -.2610373    .3710198\n         Large  |   .0399068   .1526637     0.26   0.794    -.2593086    .3391222\n                |\n        college |  -.0218305   .1488361    -0.15   0.883    -.3135439    .2698829\n          _cons |  -22.99551   8.826624    -2.61   0.009    -40.29537   -5.695646\n---------------------------------------------------------------------------------\n\n. predict pr_smp\n(option pr assumed; Pr(smp))\n\n. gen wgt = pr_smp / (1-pr_smp) \n\n. replace wgt = 1 if smp==0\n(500 real changes made)\n\n.  xi:tabstat fines i.csize college  taxes [w=wgt],  by(smp)\ni.csize           _Icsize_1-3         (naturally coded; _Icsize_1 omitted)\n(analytic weights assumed)\n\nSummary statistics: Mean\nGroup variable: smp \n\n     smp |     fines  _Icsiz~2  _Icsiz~3   college     taxes\n---------+--------------------------------------------------\n       0 |    9.8952       .29      .358      .248      .704\n       1 |  10.24873  .2926133  .3584506  .2474627  .7045357\n---------+--------------------------------------------------\n   Total |  10.07894  .2913582  .3582342  .2477208  .7042784\n------------------------------------------------------------\n\ntwo (kdensity citations if smp==0 ) ///\n    (kdensity citations if smp==1 [w=wgt]) /// \n    , legend(order(1 \"Before Policy\" 2 \"After Policy\"))\n\ntabstat citations [w=wgt], by(smp) stats(p10 p25 p50 mean p75 p90  )\n(analytic weights assumed)\n\nSummary for variables: citations\nGroup variable: smp \n\n     smp |       p10       p25       p50      Mean       p75       p90\n---------+------------------------------------------------------------\n       0 |      11.5        15        20    22.018        27      34.5\n       1 |        11        15        19  20.69419        26        32\n---------+------------------------------------------------------------\n   Total |        11        15        20  21.32998        27        33\n----------------------------------------------------------------------\n    \nIncreasing fines, may reduce citations in about 1.3., but have almost no effect at the bottom of the distribution.\nWhat about Standard errors? Bootstrap! (logit and estimation, probably clustering at individual level)\nEasy to extend to other Statistics, but, can only provide results “within” support."
  },
  {
    "objectID": "05uqreg.html#op2-model-conditional-distribution",
    "href": "05uqreg.html#op2-model-conditional-distribution",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op2: Model Conditional Distribution",
    "text": "Op2: Model Conditional Distribution\nSay that you are interested in the same Policy, but do not trust re-weighting. Instead you want to model the Outcome, using some parametric or nonparametric analysis\n\nDefine your model. Should be feasible enough to accommodate changes in the conditional distribution. (one “model” for each \\(X's\\) combination?)\nUse the model to make predictions of your outcome (quite a few times). and summarize all results.\n\nOptions for flexible mode?\n\nYou can use Heteroskedastic OLS \\(y\\sim N(x\\beta,x\\gamma)\\) and predict from here\nYou can use CQregressions to simulate the results.\n\nOne of this is similar to what we do in simulation analysis, and imputation. The other is similar to the work of Machado Mata (2005) and Melly(2005). Where you invert the whole distribution “globally”"
  },
  {
    "objectID": "05uqreg.html#example-1-hetregress",
    "href": "05uqreg.html#example-1-hetregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #1: Hetregress",
    "text": "Example #1: Hetregress\n** Example for OPT2\nwebuse dui, clear\n** Modeling OLS with heteroskedastic errors\n    hetregress citations fines i.csize college taxes ,  het(fines i.csize college taxes )\n    \n    \n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ncitations    |\n       fines |   -6.18443   .3018298   -20.49   0.000    -6.776006   -5.592855\n             |\n       csize |\n     Medium  |   4.683941   .5028377     9.32   0.000     3.698397    5.669484\n      Large  |   9.655742   .5261904    18.35   0.000     8.624428    10.68706\n             |\n     college |   4.495635   .5283579     8.51   0.000     3.460072    5.531197\n       taxes |  -3.640864   .4938209    -7.37   0.000    -4.608735   -2.672993\n       _cons |   79.48011   3.118008    25.49   0.000     73.36892    85.59129\n-------------+----------------------------------------------------------------\nlnsigma2     |\n       fines |  -.5261208    .082495    -6.38   0.000     -.687808   -.3644337\n             |\n       csize |\n     Medium  |    .331204   .1681709     1.97   0.049     .0015952    .6608129\n      Large  |   .5578834   .1662309     3.36   0.001     .2320768    .8836899\n             |\n     college |   .3186815   .1539424     2.07   0.038     .0169599    .6204032\n       taxes |  -.3988692   .1437708    -2.77   0.006    -.6806548   -.1170836\n       _cons |   8.257714   .8201063    10.07   0.000     6.650335    9.865093\n------------------------------------------------------------------------------\nLR test of lnsigma2=0: chi2(5) = 75.42                    Prob &gt; chi2 = 0.0000\n\n\n**  make Policy\nclonevar fines_copy = fines\nreplace fines = 0.1*(12-fines)+fines \n\npredict xb, xb\npredict xbs, sigma\n\n** Simulate results\ncapture program drop sim1\nprogram sim1, eclass\n    capture drop cit_hat \n    gen cit_hat = rnormal(xb,xbs)   \n    qui:sum citations, d \n    local lp10 = r(p10)\n    local lp25 = r(p25)\n    local lp50 = r(p50) \n    local lpmn = r(mean)\n    local lp75 = r(p75)\n    local lp90 = r(p90)\n    qui:sum cit_hat, d \n    matrix b = r(p10)-`lp10',r(p25)-`lp25', r(p50)-`lp50' , r(mean) -`lpmn',r(p75)-`lp75',r(p90)-`lp90'\n    matrix colname b = p10 p25 p50 mean p75 p90\n    ereturn post b\nend\n\nsimulate, reps(1000): sim1\nsum\n\n-------------+---------------------------------------------------------\n      _b_p10 |      1,000    -1.08147    .3913698   -2.31713   .1689796\n      _b_p25 |      1,000   -.3262908    .3230118  -1.817808   .6465259\n      _b_p50 |      1,000   -.2085465     .316455   -1.09237   .7785921\n     _b_mean |      1,000   -1.675626    .2234377  -2.400322   -1.03909\n      _b_p75 |      1,000   -1.541725    .4210822  -2.857586  -.2505198\n-------------+---------------------------------------------------------\n      _b_p90 |      1,000   -3.543298    .6079578  -5.464802  -1.682991\nEffects larger than Reweigthing. Statistical inference here may be flawed. (first stage error not carried over)"
  },
  {
    "objectID": "05uqreg.html#example-2-qregress",
    "href": "05uqreg.html#example-2-qregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #2: Qregress",
    "text": "Example #2: Qregress\nwebuse dui, clear\ngen id = _n\n** Expand to 99 quantiles\nexpand 99 \nbysor id:gen q=_n\n** make policy\ngen fines_policy=0.1*(12-fines)+fines \ngen fines_copy  =fines \n** Estimate 99 quantiles (in theory one should do more..but choose at random)\nssc install qrprocess // Faster than qreg\n** Save Cit hat (prediction)\n** cit policy (with policy)\ngen cit_hat=.\ngen cit_pol=.\n\nforvalues  i = 1 / 99 {\n    if `i'==1   _dots 0 0\n    _dots `i' 0\n    qui {\n        local i100=`i'/100\n        capture drop aux\n        qrprocess citations c.fines##c.fines  (i.csize college taxes) if q==1, q(`i100')\n        ** predicts the values as if they were in q100\n        predict aux\n        replace cit_hat=aux if q==`i'\n        drop aux\n        replace fines = fines_policy\n        predict aux\n        replace cit_pol=aux if q==`i'   \n        replace fines = fines_copy \n    }   \n}\n\n tabstat citations cit_hat cit_pol, stats(p10 p25 p50 mean p75 p90)\n \n   Stats |  citati~s   cit_hat   cit_pol\n---------+------------------------------\n     p10 |      11.5  10.70744  9.911633\n     p25 |        15  15.42857  14.27302\n     p50 |        20  21.15557  19.68303\n    Mean |    22.018   22.0002  20.31824\n     p75 |        27  27.65936  25.56173\n     p90 |      34.5  34.03413  31.39192\n----------------------------------------\nVery demanding (computationally) and may only capture effects to the extend that we have good coverage of the distribution.\nStandard Errors…Bootstrapping. Perhaps use random quantile assignment, and may have problems near boundaries."
  },
  {
    "objectID": "05uqreg.html#opt-1-and-2-comments",
    "href": "05uqreg.html#opt-1-and-2-comments",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 1 and 2: Comments",
    "text": "Opt 1 and 2: Comments\nThe first two options allow you to estimate effects of changes in \\(f(x)\\) on the unconditional distribution of \\(y\\), and in consequence, the distributional statistics of interest.\nThey have limitations:\n\nThey both are limited to a single experiment. A different policy requires a change in the setup.\nReweighing is simple to apply, but has limitation on the type of policies. They all need to be within the support of \\(X\\)\nModeling the conditional distribution is a more direct approach, but more computationally intensive, specially for obtaining Stand errors."
  },
  {
    "objectID": "05uqreg.html#opt-3.-local-approximation-rif-regression",
    "href": "05uqreg.html#opt-3.-local-approximation-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. Local Approximation: RIF regression",
    "text": "Opt 3. Local Approximation: RIF regression\nThe third approach was first introduced by Firpo, Fortin and Lemieux 2009, as a computationally simple way to analyze how changes in \\(X's\\) affect the unconditional quantiles of \\(y\\).\nThis strategy was later extended to analyze the effects on a myriad of distributional statistics and rank dependent indices. as well as an approach to estimate distributional treatment effects. See Rios-Avila (2020).\nIn contrast with other approaches, it can be used to analyze multiple types of policies without re-estimating the model. However the identification and interpretation needs particular attention.\nIt also allows you to easily make Statistical inference. (except for quantiles…)"
  },
  {
    "objectID": "05uqreg.html#opt-3.-from-ground-up",
    "href": "05uqreg.html#opt-3.-from-ground-up",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. From ground up",
    "text": "Opt 3. From ground up\nReconsider the Original question. How do you capture the effect of changes of distribution of \\(x\\) on the distribution of \\(y\\).\n\\[\n\\Delta v=v(G_y) - v(F_y)\n\\]\nNow, assume that \\(G_y\\) is just marginally different from \\(F_y\\) (different in a very particular way)\n\\[\nG_y(y_i) = (1-\\epsilon)F_y+ \\epsilon 1(y&gt;y_i)\n\\]\nThis function puts just a bit more weight on observation \\(y_i\\). Think of it as “dropping” a new person in the pool.\nIf this is the case, the \\(\\Delta v(y_i)\\) Captures how would the Statistic \\(v\\) changes if the distribution puts just a bit extra weight on 1 observation. (this would be very small)"
  },
  {
    "objectID": "05uqreg.html#opt-3.-one-more-thing",
    "href": "05uqreg.html#opt-3.-one-more-thing",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. One more thing",
    "text": "Opt 3. One more thing\nLets Rescale it:\n\\[\nIF(v,F_y,y_i) =lim_{\\epsilon \\rightarrow 0} \\frac{v(G_y(y_i))-v(F_y)}{\\epsilon}\n\\]\nThe influence function is a measure of direction of change, we should expect the statistic \\(v\\) will have as we change \\(F_y \\rightarrow G_y\\) .\nFrom here the RIF is just \\(RIF(v,F_y,y_i) = v + IF(v,F_y,y_i)\\)\nWhich has some properties:\n\\[\n\\begin{aligned}\n\\int IF(v,F_y,y_i)f_ydy=0 &;\n\\int RIF(v,F_y,y_i)f_y dy=v \\\\\nv(F_y) \\sim N \\left(v(F_y),\\frac{\\sigma^2_{IF}}{N} \\right) &;\n\\int IF^2f_ydy =\\sigma^2_{IF}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "05uqreg.html#opt-3.-rif-regression",
    "href": "05uqreg.html#opt-3.-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. RIF Regression",
    "text": "Opt 3. RIF Regression\nFirst:\n\\[\nv(F_y) = \\iint RIF(v,F_Y,y_i) f(y|x)f(x)dy = \\int E(RIF(.)|x) f(x)\n\\]\nFrom here is similar to Opt 3. Use some econometric model to estimate \\(E(RIF(.)|X)\\), and use that to make predictions on how \\(v(F_y)\\) would change, when there is a distributional change in \\(X\\).\nRIF-OLS: Unconditional effect!\n\\[\nRIF(v,F_y,y_i) = X\\beta+e  \\ \\rightarrow\\\nE(RIF) = v(F_y) = \\bar X \\beta \\\\\n\\frac{dv(F_y)}{d\\bar X}=\\beta\n\\]\nLogic. When \\(F_x\\) changes, it will change the distribution of \\(F_y\\), which will affect how the statistic \\(v\\) will change. But, we can only consider changes in means! (and Var)"
  },
  {
    "objectID": "05uqreg.html#why-it-works-and-why-it-may-not",
    "href": "05uqreg.html#why-it-works-and-why-it-may-not",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Why it works, and why it may not",
    "text": "Why it works, and why it may not\nRIF regressions works by using a linear approximation of the statistic \\(v\\) with the changes in \\(F_y\\) which are caused by changes in \\(F_x\\), proxied by changes in \\(\\bar X\\).\n\nChanges at the individual \\(x_i\\) are not interesting (in a population of 1million, what happens to person 99 may not be large enough to matter)\n\nDepending on the model specification, however, we may only be able to identify changes in first and second moments of the distribution of \\(x\\). (Mean and variance).\n-\nHowever, as any linear approximation to a non-linear function, the approximations are BAD when the changes in \\(F_x\\) are too large. The most relevant example…Dummies and treatment!"
  },
  {
    "objectID": "05uqreg.html#rif-reg-and-dummies",
    "href": "05uqreg.html#rif-reg-and-dummies",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "RIF-Reg and dummies",
    "text": "RIF-Reg and dummies\nDummies are a challenge. At individual or conditional level, we usually consider changes from 0 to 1 (off or on).\n\nFor unconditional effects this is not correct (too large of a change) (No-one treated vs All treated). Thus you need to change the question…Not on and off changes, but Changes in proportion of treated!\n\nVery important. a 1% increase in pop treated is different if current treatment is 10% or 90%.\n\nHowever, its possible to restructure RIF regressions to be partially conditional (Rios-Avila and Maroto 2023) (Combines CQREG with UQREG)\nSimilar problems are experienced if the change in continuous variables is large!\n\nMinor point. How do you construct RIFs? (analytically and Empirically)"
  },
  {
    "objectID": "05uqreg.html#example-1",
    "href": "05uqreg.html#example-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\nwebuse dui, clear\n**  Consider the policy change\ngen change_fines= 0.1*(12-fines)\n**  consider average change in fines.Since we are only considering this effect\nsum change_fines\n\nrifhdreg citations fines i.csize college taxes, rif(q(10)) \nest sto m1\nrifhdreg citations fines i.csize college taxes, rif(q(50)) \nest sto m2\nrifhdreg citations fines i.csize college taxes, rif(q(90)) \nest sto m3\n** This are Rescaled to show true effect\nrifhdreg citations fines i.csize college taxes, rif(q(10)) scale(.21048)\nest sto m4\nrifhdreg citations fines i.csize college taxes, rif(q(50)) scale(.21048)\nest sto m5\nrifhdreg citations fines i.csize collegetaxes, rif(q(90)) scale(.21048)\nest sto m6\n\n. esttab m1 m2 m3 m4 m5 m6, se mtitle(q10 q50 q90 r-q10 r-q50 r-q90) compress nogaps\n\n----------------------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)          (6)   \n                 q10          q50          q90        r-q10        r-q50        r-q90   \n----------------------------------------------------------------------------------------\nfines         -4.476***    -6.700***    -9.887***    -0.942***    -1.410***    -2.081***\n             (0.491)      (0.493)      (0.978)      (0.103)      (0.104)      (0.206)   \n1.csize            0            0            0            0            0            0   \n                 (.)          (.)          (.)          (.)          (.)          (.)   \n2.csize        4.603***     7.325***     6.370***     0.969***     1.542***     1.341***\n             (0.963)      (0.966)      (1.917)      (0.203)      (0.203)      (0.404)   \n3.csize        6.504***     13.54***     12.97***     1.369***     2.851***     2.729***\n             (0.914)      (0.917)      (1.820)      (0.192)      (0.193)      (0.383)   \ncollege        2.922**      5.948***     9.973***     0.615**      1.252***     2.099***\n             (0.890)      (0.892)      (1.771)      (0.187)      (0.188)      (0.373)   \ntaxes         -3.279***    -3.303***    -8.319***    -0.690***    -0.695***    -1.751***\n             (0.842)      (0.844)      (1.676)      (0.177)      (0.178)      (0.353)   \n_cons          53.71***     81.04***     129.2***     11.30***     17.06***     27.20***\n             (4.964)      (4.977)      (9.880)      (1.045)      (1.048)      (2.080)   \n----------------------------------------------------------------------------------------\nN                500          500          500          500          500          500   \n----------------------------------------------------------------------------------------\nConsider the basic change. Fines increases in 1 unit, Cntys with taxes, increase 10%, etc\nOr consider rescaled effects"
  },
  {
    "objectID": "05uqreg.html#how-do-they-compare",
    "href": "05uqreg.html#how-do-they-compare",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How Do they Compare",
    "text": "How Do they Compare"
  },
  {
    "objectID": "05uqreg.html#other-considerations",
    "href": "05uqreg.html#other-considerations",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Other Considerations",
    "text": "Other Considerations\nRIF Regressions are useful, but again, one must use them with care.\n\nOnly Small changes! Larger changes may be meaningless\n\nExcept for Stata (see rif and rifhdreg), the applications of RIF regressions outside Mean, Variance and Quantiles are non-existent. (paper?)\n\nFor most Common Statistics, RIF’s automatically provide correct Standard errors (which can be Robustized!). In fact, a simple LR can be considered as a special case of RIF’s\n\n\\[\n\\begin{aligned}\nRIF(mean,y_i,F_y) &= y_i \\\\\nRIF(variance,y_i,F_y) &= (y_i-\\bar y)^2 \\\\\nRIF(Q,y_i,F_Y) &= Q_y(\\tau) + \\frac{\\tau-1(y_i \\leq Q_y(\\tau))}{f_Y(y_i)}\n\\end{aligned}\n\\]\nExcept for quantile related functions! (\\(f_y\\) also needs estimation, thus errors!)\n\nAccounting for “local” unconditional effects beyond means require Center Polynomials:\n\n\\[\nRIF(.,y) = b_0 + b_1 x + b_2 (x-\\bar x)^2+\\varepsilon\n\\]\n\nQuantile treatment effects (on and off) are possible using PC-RIF (When you condition the distribution on just 1 variable)\n\n\\[\nRIF(.,F_{Y|D},y) = b_0 + b_1 D+b_2 x + b_3 (x-\\bar x)^2+\\varepsilon\n\\]"
  },
  {
    "objectID": "05uqreg.html#final-words-on-rif",
    "href": "05uqreg.html#final-words-on-rif",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Final words on RIF",
    "text": "Final words on RIF\nBecause this implementation uses LR, you can add Multiple Fixed effects as well. (with limitations)\nAnd you can skip LR all together, and model RIF using Other approaches! (which may be even better than OLS)."
  },
  {
    "objectID": "06nlreg.html#sowhat-is-non-linear",
    "href": "06nlreg.html#sowhat-is-non-linear",
    "title": "NLS, IRLS, and MLE",
    "section": "So…what is non-linear?",
    "text": "So…what is non-linear?\nOptions:\n\\[\n\\begin{aligned}\ny = b_0 + b_1 x^{b_2}+e \\\\\ny = exp(b_0+b_1 x)+e \\\\\ny = exp(b_0+b_1 x+e) \\\\\ny = h(x\\beta)+e \\\\\ny = h(x\\beta+e)\n\\end{aligned}\n\\]\nAll of them are Nonlinear, but some of them are linearizable.\n\nA Linearizable model is one you can apply a transformation and make it linear.\n\nFor models #2 and #4, you could apply (logs) or \\(h^{-1}()\\) (if functions), and use OLS. For the others you need other methods."
  },
  {
    "objectID": "06nlreg.html#how-do-you-do-nl",
    "href": "06nlreg.html#how-do-you-do-nl",
    "title": "NLS, IRLS, and MLE",
    "section": "How do you do, NL?",
    "text": "How do you do, NL?\nNonlinear models are tricky. In contrast with our good old OLS, there no “close form” solution we can plug in:\n\\[\n\\beta = (X'X)^{-1}X'y\n\\]\nWe already saw this! For Quantile regressions, we never did it by-hand (requires linear programming). Because, Qregressions are also nolinear.\nIn this section, we will cover some of the few methodologies that are available for the estimation of Nonlinear models. We start with the first, an extension to OLS, we will call NLS.\n\\[\ny = h(x,\\beta) + e\n\\]\nWhat makes this model NLS, is that the error adds to the outcome (or CEF)! However, the CEF is modeled as a nolinear function of \\(X's\\) and \\(\\beta's\\). (but we still aim to MIN SSR)"
  },
  {
    "objectID": "06nlreg.html#some-assumptions",
    "href": "06nlreg.html#some-assumptions",
    "title": "NLS, IRLS, and MLE",
    "section": "Some Assumptions",
    "text": "Some Assumptions\nFor identification and estimation NLS requires similar assumptions as OLS:\n\nFunctional form: \\(E(y|X)\\) is given by \\(h(x,\\beta)\\), which is continuous and differentiable.\nThere is a unique solution! (like no-multicolinearity). if \\(\\beta\\) Minimizes the errors, then there is no other \\(\\beta_0\\) that will give the same solution.\nThe expected value of the error is zero \\(E(e)=0\\), and \\(E(e|h(x,\\beta))=0\\) . Similar to No endogeneity, but constrained by functional form.\nData is well behaved. (no extreme distributions, so that mean and variance exists)\n\nUnder this assumptions, its possible to Estimate the coefficients of interest."
  },
  {
    "objectID": "06nlreg.html#but-how",
    "href": "06nlreg.html#but-how",
    "title": "NLS, IRLS, and MLE",
    "section": "But How?",
    "text": "But How?\nNLS aims to choose \\(\\beta's\\) to minimize the sum of squared residuals:\n\\[\nSSR(\\beta) = \\sum(y-h(x,\\beta))^2 = [y-h(x,\\beta)]'[y-h(x,\\beta)]\n\\]\nThe FOC of this model are a non-linear system of equations.\n\\[\n\\frac{\\partial SSR(\\beta)}{\\partial \\beta}=-2 \\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]' [y-h(x,\\beta)] =-2\\color{red}{\\tilde X'} e\n\\]\nSo how? Lets Start with a 2nd Order Taylor Expansion:\n\\[\n\\begin{aligned}\nSSR(\\beta) &\\simeq  SSR(\\beta_0) + g(\\beta_0)' (\\beta-\\beta_0)+\\frac{1}{2}(\\beta-\\beta_0)'H(\\beta_0)(\\beta-\\beta_0) \\\\\ng(\\beta) &=-2 \\tilde X'e ;H(\\beta)=2 \\tilde X'\\tilde X ; \\tilde X=\n\\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06nlreg.html#foc-again..",
    "href": "06nlreg.html#foc-again..",
    "title": "NLS, IRLS, and MLE",
    "section": "FOC again..",
    "text": "FOC again..\n\\[\n\\begin{aligned}\ng(\\beta_0)+H(\\beta_0)(\\beta-\\beta_0)&=0 \\\\\n\\beta-\\beta_0 &= -H(\\beta_0)^{-1}g(\\beta_0) \\\\\n\\beta_t &=\\beta_{t-1}-H(\\beta_{t-1})^{-1}g(\\beta_{t-1})\n\\end{aligned}\n\\]\nThis simply says, In order to solve the system, you need to use a recursive system, so that \\(\\beta's\\) are updated until there is no longer a change.\nThis Iterative process is also known as a Newton Raphson method to solve nonlinear equations (if a solution exists).\nWhy does this work?\n\nYou change \\(\\beta\\) in the direction that should minimize SSR. (that direction is \\(g(.,.)\\).\nThat get to that change the “fastest” way possible using the Hessian\n\nThis is the most basic numerical optimization method."
  },
  {
    "objectID": "06nlreg.html#small-example",
    "href": "06nlreg.html#small-example",
    "title": "NLS, IRLS, and MLE",
    "section": "Small Example",
    "text": "Small Example\nConsider the function \\(y = x^4 -18 x^2 + 15x\\) , find the Minimum.\nS1. Gradient: \\(\\frac{\\partial y}{\\partial x}=4x^3-36*x+15\\)\nS2. Hessian: \\(\\frac{\\partial y ^2}{\\partial^2 x}=12*x^2-36\\)\nSolution:\n\\[x_t = x_{t-1} - \\frac{dy/dx}{dy^2/d^2x}\\]\nmata:\n     // function to obtain the Value, the gradient and Hessian \n     real matrix  fgh_x(real matrix x, real scalar g){\n        real matrix y\n        if (g==0)      y =    -x:^4 :- 18*x:^2 :+ 15*x\n        else if (g==1) y =  -4*x:^3 :- 36*x    :+ 15\n        else if (g==2) y = -12*x:^2 :- 36   \n        return(y)\n     }\n    // Some initial values\n     x = -2 , 2 ,0 \n     xt = x,fgh_x(x,0),fgh_x(x,1)\n    for(i=1;i&lt;8;i++) {\n         x = x :- fgh_x(x,1):/fgh_x(x,2)\n         xt = xt \\ x,fgh_x(x,0),fgh_x(x,1)\n    }\n    xt[,(1,4,7)] \n    xt[,(2,5,8)] \n    xt[,(3,6,9)] \n    end\n\n        +----------------------------------------------+\n      1 |            -2            -86             55  |\n      2 |  -6.583333333    999.5046779   -889.2939815  |\n      3 |  -4.746265374    30.78669521   -241.8115913  |\n      4 |  -3.714313214   -113.7119057   -56.25720696  |\n      5 |  -3.280073929   -127.1074326   -8.077091068  |\n      6 |  -3.193322943   -127.4662895   -.2936080906  |\n      7 |  -3.189923432   -127.4667891   -.0004426933  |\n      8 |  -3.189918291   -127.4667891   -1.01177e-09  |\n        +----------------------------------------------+\n                      1              2              3\n        +----------------------------------------------+\n      1 |             2            -26            -25  |\n      2 |   4.083333333    39.13430748    140.3356481  |\n      3 |    3.22806275   -30.56155349    33.34042084  |\n      4 |   2.853639205   -37.46140286    5.220655055  |\n      5 |   2.769051829    -37.6890608    .2425933896  |\n      6 |   2.764720715   -37.68958705    .0006229957  |\n      7 |   2.764709535   -37.68958705    4.14680e-09  |\n      8 |   2.764709535   -37.68958705    1.42109e-14  |\n        +----------------------------------------------+\n                     1             2             3\n        +-------------------------------------------+\n      1 |            0             0            15  |\n      2 |  .4166666667   3.155140818   .2893518519  |\n      3 |  .4251979252   3.156376126   .0003663956  |\n      4 |  .4252087555   3.156376128   5.98494e-10  |\n      5 |  .4252087556   3.156376128   1.77636e-15  |\n      6 |  .4252087556   3.156376128             0  |\n      7 |  .4252087556   3.156376128             0  |\n      8 |  .4252087556   3.156376128             0  |\n        +-------------------------------------------+"
  },
  {
    "objectID": "06nlreg.html#why-so-many-solutions",
    "href": "06nlreg.html#why-so-many-solutions",
    "title": "NLS, IRLS, and MLE",
    "section": "Why So many Solutions?",
    "text": "Why So many Solutions?\n\nStata"
  },
  {
    "objectID": "06nlreg.html#nls",
    "href": "06nlreg.html#nls",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS",
    "text": "NLS\nThe same principle (as above) can be used for Regression:\n\\[\ny = b_0 + b_1 x ^{b_2} + e = h(x,b) + e\n\\]\nPseudo Regressors and gradients\n\\[\n\\begin{aligned}\n\\tilde X(b) = \\frac{\\partial h(x,b)}{\\partial b_0},\n\\frac{\\partial h(x,b)}{\\partial b_1},\n\\frac{\\partial h(x,b)}{\\partial b_2} \\\\\n\\tilde X(b) = 1, x^{b_2},{b_1} x^{b_2}\\ log\\ b_2 \\\\\n\\beta_t =\\beta_{t-1}-(\\tilde X ' \\tilde X) ^{-1} \\tilde X' \\hat e \\\\\n\\hat e=y-h(x,b_{t-1})\n\\end{aligned}\n\\]\nIt turns out that:\n\\[\n\\begin{aligned}\nb^* \\sim N(b, (\\tilde X' \\tilde X)^{-1}\\tilde X' \\Omega \\tilde X (\\tilde X' \\tilde X)^{-1} )\\\\\n\\Omega = f(\\hat e)\n\\end{aligned}\n\\]\nSo…you can 🥪 just like with regular OLS!"
  },
  {
    "objectID": "06nlreg.html#nls-in-stata-long-example",
    "href": "06nlreg.html#nls-in-stata-long-example",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Long Example",
    "text": "NLS in Stata: Long Example\nclear\nset seed 101\nset obs 100\n** Generating fake data\ngen x = runiform()\ngen y = 1+0.5*x^0.5+rnormal()*.1\n*** Load data in Mata...to make things quick\nmata\nx=st_data(.,\"x\")\ny=st_data(.,\"y\")\nend\n\nmata\n// Initial data\nb=1\\1\\1\nb0=999\\999\\999\nbb=b'\n// and a loop to see when data converges\nwhile (sum(abs(b0:-b))&gt; 0.000001 ) { \n    b0=b    \n    // residuals\n    e=y:-(b[1]:+b[2]*x:^b[3])\n    // pseudo regressors\n    sx=J(100,1,1),x:^b[3],b[2]*x:^b[3]:*ln(x)\n    // gradient and Hessian\n    g=-cross(sx,e)\n    H=cross(sx,sx)\n    // updating B\n    b=b-invsym(H)*g\n    // Storing results\n    bb=bb\\b'\n}\n/// Now STD ERR (for fun 😃 )\nvcv = e'e / (100-3) * invsym(H)\nb , diagonal(vcv):^0.5\n\nend\n: b , diagonal(vcv):^0.5\n                 1             2\n    +-----------------------------+\n  1 |   1.06407084   .0527603468  |\n  2 |  .4228891942   .0477125017  |\n  3 |  .5788053878    .159931095  |\n    +-----------------------------+"
  },
  {
    "objectID": "06nlreg.html#nls-in-stata-short-example-nl-function",
    "href": "06nlreg.html#nls-in-stata-short-example-nl-function",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Short Example: NL function",
    "text": "NLS in Stata: Short Example: NL function\n** Stata has the function -nl- (nonlinear)\n** it can be used to estimate this types of models\n** see help nl\n** Be careful about Initial values\nnl ( y = {b0=1} + {b1=1} * x ^ {b2=1})\n\nIteration 0:  residual SS =   .854919\nIteration 1:  residual SS =  .7742535\nIteration 2:  residual SS =   .766106\nIteration 3:  residual SS =  .7660948\nIteration 4:  residual SS =  .7660947\nIteration 5:  residual SS =  .7660947\nIteration 6:  residual SS =  .7660947\n\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =        100\n       Model |  1.2706842          2  .635342093    R-squared     =     0.6239\n    Residual |  .76609469         97  .007897883    Adj R-squared =     0.6161\n-------------+----------------------------------    Root MSE      =     .08887\n       Total |  2.0367789         99  .020573524    Res. dev.     =  -203.3743\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   1.064071   .0527608    20.17   0.000     .9593554    1.168786\n         /b1 |   .4228891   .0477129     8.86   0.000     .3281923     .517586\n         /b2 |   .5788057   .1599306     3.62   0.000     .2613878    .8962236\n------------------------------------------------------------------------------\nNote: Parameter b0 is used as a constant term during estimation.\nSo, you could now estimate many nonlinear models! (logits, probits, poissons, etc) or can you?"
  },
  {
    "objectID": "06nlreg.html#nls-for-logit",
    "href": "06nlreg.html#nls-for-logit",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS for logit",
    "text": "NLS for logit\nThe model (as NLS)\n\\[\nP(y=1|x) = \\frac{exp(x\\beta)}{1+exp(x\\beta)}+e\n\\]\nThis guaranties the predicted value is between 0 and 1. But, still assumes errors are homoskedastic!\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =      1,647\n       Model |  1272.8283          4  318.207079    R-squared     =     0.8876\n    Residual |  161.17168       1643  .098095973    Adj R-squared =     0.8873\n-------------+----------------------------------    Root MSE      =   .3132028\n       Total |       1434       1647  .870673953    Res. dev.     =   845.9593\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   3.432866   .9601508     3.58   0.000     1.549617    5.316114\n  /b1_female |  -3.056149   .8625563    -3.54   0.000    -4.747975   -1.364324\n     /b1_age |  -.0205121   .0054815    -3.74   0.000    -.0312635   -.0097607\n    /b1_educ |   .1522987   .0329513     4.62   0.000     .0876679    .2169296\n------------------------------------------------------------------------------\nlogit lfp female age educ\n\n\nLogistic regression                                     Number of obs =  1,647\n                                                        LR chi2(3)    = 251.69\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -508.42172                             Pseudo R2     = 0.1984\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   -3.21864    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550208     5.35   0.000     2.223367    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "href": "06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM and Interative Reweighted LS (IRLS)",
    "text": "GLM and Interative Reweighted LS (IRLS)\nGeneralized Linear Model are a natural extension to LR models. It changes how LR models are estimated.\n\nPuts more emphasis in modeling the CEF (conditional mean) of the distribution\nAllows for different transformations that relate the index \\(xb\\) to \\(E(y|x)\\) (links)\nConsiders data can come from different distributions. ( family )\n\n\\[\nE(y|X) = \\eta ^{-1}(x\\beta) ; Var(E(y|X)) = \\sigma^2(x)\n\\]\nFor example:\nLogit model: Family \\(\\rightarrow\\) binomial, Link function logistic function is logistic \\(p(y|x) = \\frac {e^{x\\beta} }{1+e^{x\\beta}} \\rightarrow x\\beta=log \\left( \\frac{p}{1-p} \\right)\\)\nFamily: Binomial, so variance is given by \\(Var(y|X) = p(y|x)(1-p(y|x))\\)"
  },
  {
    "objectID": "06nlreg.html#how-does-this-change-estimation-nls",
    "href": "06nlreg.html#how-does-this-change-estimation-nls",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\n\nFusion"
  },
  {
    "objectID": "06nlreg.html#how-does-this-change-estimation-nls-1",
    "href": "06nlreg.html#how-does-this-change-estimation-nls-1",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nRecall GLS?\n\nHeteroskedasticity was addressed by either using “weights” to estimate the model, or by transforming the data!\nHere, when we choose a family, we are also choosing a particular source of heteroskedasticy. (Logit, probit, poisson, have very specific heteroskedastic errors)\nThus, you just need to apply NLS to transformed data!\n\n\\[\n\\begin{aligned}\nSSR(\\beta) = \\sum \\left( \\frac{y-h(x,\\beta)}{\\sigma(x,\\beta)} \\right)^2 \\\\\\\n\\tilde X = \\frac{1}{\\sigma(x,\\beta)} \\frac{\\partial h(x,\\beta)}{\\partial \\beta} ; \\tilde y = \\frac{y}{\\sigma(x,\\beta)} ; \\tilde e=\\frac{1}{\\sigma(x,\\beta)}(y-h(x,\\beta))\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06nlreg.html#how-does-this-change-estimation-nls-2",
    "href": "06nlreg.html#how-does-this-change-estimation-nls-2",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nThen, just apply the iterative process we saw before, until it converges!\n\\[\n\\beta_t =\\beta_{t-1}-(\\tilde X'\\tilde X)^{-1} (\\tilde X' \\tilde e)\n\\]\nThen simply derive Standard errors from here.\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))   \nlogit lfp female age educ\n\n** IRSL\ngen one =1\nmata\ny = st_data(.,\"lfp\")\nx = st_data(.,\"female age educ one\")\nb = 0\\0\\0\\0\n end\n\n\nmata:\nb0=999\\999\\999\\999\nbb=b'\nwhile (sum(abs(b0:-b))&gt; 0.000001 )  {\n    b0=b    \n    yh = logistic(x*b)\n    err = y:-yh\n    se = sqrt(yh:*(1:-yh))\n    wsx =  yh:*(1:-yh):*x:/se\n    werr=  err:/se\n    g = -cross(wsx,werr)\n    h = cross(wsx,wsx)\n    b = b:-invsym(h)*g;b'\n    }\nb,diagonal(cross(werr,werr)/1643*invsym(h)):^.5\nend\n    +-------------------------------+\n  1 |  -3.218640936    .3458501096  |\n  2 |  -.0233149268    .0068867539  |\n  3 |   .1719904055    .0389557282  |\n  4 |   3.507185231    .6200958697  |\n  +-------------------------------+"
  },
  {
    "objectID": "06nlreg.html#glm-in-stata",
    "href": "06nlreg.html#glm-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM in Stata",
    "text": "GLM in Stata\nThis one is easy as 🥧\nhelp glm\n* see for advanced options if interested\n* syntax\nglm y x1 x2 x3 , family( family ) link(function) method\nfrause oaxaca\nglm lfp female age educ, family(binomial)  link(probit)\nglm lfp female age educ, family(binomial)  link(identity)\nglm lfp female age educ, family(binomial)  link(logit)\n\n\nGeneralized linear models                         Number of obs   =      1,647\nOptimization     : ML                             Residual df     =      1,643\n                                                  Scale parameter =          1\nDeviance         =   1016.84343                   (1/df) Deviance =   .6188944\nPearson          =  1472.464769                   (1/df) Pearson  =    .896205\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   .6222486\nLog likelihood   = -508.4217152                   BIC             =  -11152.38\n\n------------------------------------------------------------------------------\n             |                 OIM\n         lfp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -3.218641    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550209     5.35   0.000     2.223368    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "06nlreg.html#what-is-mle",
    "href": "06nlreg.html#what-is-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "What is MLE",
    "text": "What is MLE\nMLE is an estimation method that allows you to find asymptotically efficient estimators of the parameters of interest \\(\\beta's\\).\nHow?\n\nUnder the assumption that you know something about the data conditional distribution, MLE will find the set of parameters that maximizes the probability (likelihood)that data “comes” from the chosen distribution.\n\nok….but HOW???\nLets do this by example"
  },
  {
    "objectID": "06nlreg.html#poisson-regression-via-mle",
    "href": "06nlreg.html#poisson-regression-via-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Poisson Regression via MLE",
    "text": "Poisson Regression via MLE\nConsider variable \\(y={1,1,2,2,3,3,3,3,4,5}\\)\nAnd say, we assume it comes from a poisson distribution:\n\\[\np(y|\\lambda) = \\frac{e^{-\\lambda} \\lambda ^y}{y!}\n\\]\nThis function depends on the value of \\(\\lambda\\) .\n\nWhen \\(\\lambda\\) is known, this is the probability \\(y=\\#\\), assuming a poisson distribution.\nWhen \\(\\lambda\\) is unknown, this function (now \\(f(y|\\lambda)\\) gives the likelihood that we observe \\(y=\\#\\) , for any given \\(\\lambda\\)."
  },
  {
    "objectID": "06nlreg.html#the-likelihood-of-lambda",
    "href": "06nlreg.html#the-likelihood-of-lambda",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "06nlreg.html#section",
    "href": "06nlreg.html#section",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "Previous figure only considers the likelihood function of a single observation. And every curve suggests its own parameter. (care to guess?)\nWhat if we want one that Maximizes the likelihood of ALL observations at once!. For this we need to impose an additional assumption: Independent distribution.\nThis means that JOINT density is defined as:\n\n\\[\nL(\\lambda|y_1,y_2,...,y_{10})=\\prod f(y_i|\\lambda)  \n\\]"
  },
  {
    "objectID": "06nlreg.html#the-likelihood-of-lambda-1",
    "href": "06nlreg.html#the-likelihood-of-lambda-1",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "06nlreg.html#what-mle-does",
    "href": "06nlreg.html#what-mle-does",
    "title": "NLS, IRLS, and MLE",
    "section": "What MLE does",
    "text": "What MLE does\nWhat MLE does, then, is to find the parameter \\(\\lambda\\) that maximizes the Joint probability of observing the dataset \\(Y\\). Simple as that….\nWith one more difference. Because products are Hard, MLE aims to maximize the Log-Likelihood of observing the data:\n\\[\n\\begin{aligned}\nLnL(\\lambda|y_1,y_2,...,y_{10}) &=\\sum ln f(y_i|\\lambda) \\\\\\\n&= \\sum (-\\lambda + y_i ln(\\lambda) - ln(y_i!))\n\\end{aligned}\n\\]\nAnd just two more changes."
  },
  {
    "objectID": "06nlreg.html#section-1",
    "href": "06nlreg.html#section-1",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "When you have \\(X's\\), you can further modify this model, to allow for covariates. For example:\n\n\\[\n\\lambda = e^{x\\beta}\n\\]Which guaranties the mean, or more specifically conditional mean \\(\\lambda = E(y|X)\\) , is positive.\n\nWe divide LnL by \\(N\\). (Number of observations)\n\n\\[\n\\begin{aligned}\n\\beta = \\min_\\beta LnL(\\beta|Y,X) \\\\\n\\beta = \\min_\\beta \\frac{1}{N}\\sum -e^{x'\\beta}+y_i  x_i'\\beta - ln(y_i!)\n\\end{aligned}\n\\]\nWhich arrives to the Same solution"
  },
  {
    "objectID": "06nlreg.html#how-to-solve-this",
    "href": "06nlreg.html#how-to-solve-this",
    "title": "NLS, IRLS, and MLE",
    "section": "How to solve this?",
    "text": "How to solve this?\nWe actually already covered this…We use Iterative methods!\n\\[\n\\begin{aligned}\nLL &=  \\sum ln \\ f(y|x,\\beta) \\\\\nFOC&: \\\\\n\\frac{\\partial LL}{\\partial \\beta}&= \\sum \\frac{\\partial ln \\ f(y|x,\\beta)}{\\partial \\beta} = \\sum g_i=n E[g_i] =0 \\\\\nSOC&: \\\\\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'}&=\n\\sum\\frac{\\partial^2 ln f(y|x,\\beta)}{\\partial \\beta \\partial \\beta'} =\\sum H_i = n E(H_i)\n\\end{aligned}\n\\]\nThus, \\(\\beta's\\) can be estimated using the iterative process (or other more efficient process)\n\\[\n\\beta_t = \\beta_{t-1} - E(H)^{-1} E(g_i)\n\\]"
  },
  {
    "objectID": "06nlreg.html#why-do-we-like-mle",
    "href": "06nlreg.html#why-do-we-like-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Why do we like MLE?",
    "text": "Why do we like MLE?\nProperties of MLE:\n\nThe estimates are consistent \\(plim \\ \\hat \\theta = \\theta\\)\nIts MLE estimates are asymptically normal \\(\\hat \\theta\\sim N(\\theta,\\sigma^2_\\theta)\\)\nAnd asymptotically efficient (smallest variance)\n\nWait…What about Variances? How do you estimate them!"
  },
  {
    "objectID": "06nlreg.html#regularity-conditions-and-mle",
    "href": "06nlreg.html#regularity-conditions-and-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions and MLE ⚠️",
    "text": "Regularity Conditions and MLE ⚠️\nThe variance of estimated coefficients has three estimators for its variance:\n\nWe can 🥪(Sandwich) the variance:\n\n\\[\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = H^{-1}g'gH^{-1} = A^{-1}BA^{-1}\n\\]\n\nOr you can use of of the following: \\[\n\\begin{aligned}\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = -H^{-1} \\\\\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = \\left(\\frac{1}{N}\\sum g_i g_i'\\right)^{-1}\n\\end{aligned}\n\\]\n\nBut if all is well (Regularity conditions), They are all equivalent. Otherwise Opt1 is similar to HC1, and Option 2a is closer to non-robust Standard Errors"
  },
  {
    "objectID": "06nlreg.html#regularity-conditions",
    "href": "06nlreg.html#regularity-conditions",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions ⚠️",
    "text": "Regularity Conditions ⚠️\n\nThe LogLikelihood function is build on a well behaved distribution function. Which implies FOC:\n\n\\[\n\\begin{aligned}\n\\int f(y|\\theta) dy =1 \\\\\n\\int \\frac{\\partial f(y|\\theta)}{\\partial \\theta}dy=\\int  \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} f(y|\\theta)=0 \\\\\nE(g_i)=0\n\\end{aligned}\n\\]\n\nOrder of Diff and Integration is interchangeable. We obtain SOC: \\[\n\\int \\left( \\frac{\\partial^2 ln f(y|\\theta)}{\\partial \\theta \\partial \\theta'}f(y|\\theta) +\n\\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta'} \\right)dy = 0 \\\\\nE(H_i) = -E(g_ig_i')\n\\]"
  },
  {
    "objectID": "06nlreg.html#lr-as-mle",
    "href": "06nlreg.html#lr-as-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE",
    "text": "LR as MLE\nUnder the assumption of normality, LR can be estimated using ML:\n\\[y_i = x_i'\\beta + e_i \\ | \\ e_i\\sim N(0,\\sigma^2) \\rightarrow y|x \\sim N(x'\\beta, \\sigma^2)\n\\]\nHow does the MLE look? \\[\n\\begin{aligned}\nL_i = f(y_i|x_i,\\beta,\\sigma^2) = \\frac{1}{ \\sqrt{2\\pi \\sigma^2 }} e^{-\\frac{1}{2} \\frac{ (y_i-x_i'\\beta)^2}{\\sigma^2} } \\\\\nLL_i = -\\frac{1}{2}\\frac{(y_i-x_i'\\beta)^2}{\\sigma^2} - \\frac{1}{2}ln(2\\pi)-\\frac{1}{2}ln(\\sigma^2)\n\\end{aligned}\n\\]\nTotal LL?\n\\[\nLL = -\\frac{1}{2\\sigma^2} \\sum (y_i-x_i'\\beta)^2-\\frac{N ln(2\\pi)}{2}-\\frac{N}{2} ln(\\sigma^2)\n\\]"
  },
  {
    "objectID": "06nlreg.html#lr-as-mle-pii",
    "href": "06nlreg.html#lr-as-mle-pii",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE PII",
    "text": "LR as MLE PII\nFOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial LL}{\\partial \\beta} = -\\frac{1}{\\sigma^2}\\sum x'(y_i-x'\\beta)=0 \\rightarrow \\hat\\beta=(X'X)^{-1}X'y \\\\\n\\frac{\\partial LL}{\\partial \\sigma^2} = \\frac{\\sum e^2}{2\\sigma^4}-\\frac{N}{2\\sigma^2}=0 \\rightarrow \\hat \\sigma^2 = \\frac{\\sum e^2}{N}\n\\end{aligned}\n\\]\nSOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'} = - \\frac{X'X}{\\sigma^2} ;\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\sigma} = - \\frac{X'y-X'X\\beta }{\\sigma^2} \\\\\n\\frac{\\partial^2 LL}{\\partial \\sigma \\partial \\beta'}=0;\n\\frac{\\partial^2 LL}{\\partial \\sigma ^2}=-\\frac{\\sum e^2}{\\sigma^6}+\\frac{N}{2\\sigma^4}=-\\frac{N}{2\\sigma^4}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06nlreg.html#soc",
    "href": "06nlreg.html#soc",
    "title": "NLS, IRLS, and MLE",
    "section": "SOC",
    "text": "SOC\n\\[\n\\begin{aligned}\nH = \\Bigg[\\begin{matrix} -\\frac{X'X}{\\sigma^2}& 0 \\\\\n0 & -\\frac{N}{2\\sigma^4}\n\\end{matrix} \\Bigg] \\\\\n& \\rightarrow Var(\\beta,\\sigma^2)=-H^{-1}=\n\\Bigg[\\begin{matrix}\n\\sigma^2 (X'X)^{-1} & 0 \\\\\n0 & \\frac{2\\sigma^4}{N}\n\\end{matrix} \\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06nlreg.html#other-considerations",
    "href": "06nlreg.html#other-considerations",
    "title": "NLS, IRLS, and MLE",
    "section": "Other considerations",
    "text": "Other considerations\nMLE is consistent if the model is correctly Specified.\nThis means that one correctly specifies the conditional distribution of \\(y\\).\n\nOften, its possible to especify the CEF, but specify the variance correctly, may be difficult\n\nUsually, this would be grounds of inconsistency. However if the distribution function is part of the exponential family (normal, bernulli, poisson, etc), one only needs to correctly specify the CEF correctly!\n\nIn this case, the model is no longer estimated using MLE but QMLE\n\nIn this cases, use Robust!"
  },
  {
    "objectID": "06nlreg.html#mle-in-stata",
    "href": "06nlreg.html#mle-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "MLE in Stata",
    "text": "MLE in Stata\nMany native commands in Stata actually estimate models using MLE on the background.\n\nlogit; probit; poisson; ologit; mlogit, etc\n\nSome multiple equation models as well.\n\nmovestay , craggit , ky_fit\n\nAre just a few user written commands that also rely on MLE.\nSo how do you estimate this models?\nyou do it by hand!"
  },
  {
    "objectID": "06nlreg.html#this-is-the-way",
    "href": "06nlreg.html#this-is-the-way",
    "title": "NLS, IRLS, and MLE",
    "section": "This is the way",
    "text": "This is the way"
  },
  {
    "objectID": "06nlreg.html#programming-mle",
    "href": "06nlreg.html#programming-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE",
    "text": "Programming MLE\nStata has one feature that would allow you to estimate almost any model via MLE. the -ml- suit.\nThis has many levels of programming (lf, d0, d1, d2, etc), but we will concentrate on the easiest one : lf (linear function)\n\nThis only requires you to provide the individual level likelihood function!\n\nBut how do we start?"
  },
  {
    "objectID": "06nlreg.html#section-2",
    "href": "06nlreg.html#section-2",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "You need three pieces of information:\n\nIdentify distribution or objective function to maximize.\nIdentify the parameters that the distribution depends on, and how will they be affected by characteristics\nIf more than one equation exists, Identify possible connections across variables\nWrap it all in a program\n\nFor details on a few examples see Rios-Avila & Canavire-Bacarreza 2018"
  },
  {
    "objectID": "06nlreg.html#programming-mle-pi",
    "href": "06nlreg.html#programming-mle-pi",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE PI",
    "text": "Programming MLE PI\nOLS via MLE:\n\n\\(y\\) distributes as normal distribution, which depends on the mean \\(\\mu\\) and variance \\(\\sigma^2\\). We assume that only the mean depends on \\(X\\).\n\n** Define Program\ncapture program drop my_ols\nprogram define my_ols\n  args lnf ///   &lt;- Stores the LL for obs i\n       xb  ///   &lt;- Captures the Linear combination of X's\n       lnsigma // &lt;- captures the Log standard error\n  ** Start creating all aux variables and lnf     \n  qui {\n    tempvar sigma // Temporary variable\n    gen double `sigma' = exp(`lnsigma')\n    tempvar y\n    local y $ML_y1\n    replace `lnf'      = log( normalden(`y',`xb',`sigma' ) )\n  }     \nend\nNow Just Call on the program\n* load some data\nfrause oaxaca, clear\nml model lf   /// Ask to use -ml- to estimate a model with method -lf-\n   my_ols     /// your ML program\n   (xb: lnwage = age educ female  ) /// 1st Eq (only one in this case)\n   (lnsig: = female ) // Empty, (no other Y, but could add X's)\n   // I could haave added weights, or IF conditions\nml check     // checks if the code is correct\nml maximize  // maximizes\nml display   // shows results\n\n* Short version \nml model lf  my_ols /// Model and method\n   (xb: lnwage = age educ female) (lnsig: = female ) /// model Parms\n   , robust maximize // Other SE options, and maximize\n   \n\nml display\n\n                                                        Number of obs =  1,434\n                                                        Wald chi2(3)  = 393.05\nLog pseudolikelihood = -870.41117                       Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n         age |   .0177272   .0012221    14.51   0.000      .015332    .0201224\n        educ |   .0685501   .0056712    12.09   0.000     .0574347    .0796655\n      female |  -.1487184   .0247333    -6.01   0.000    -.1971948   -.1002421\n       _cons |   1.949072   .0888074    21.95   0.000     1.775012    2.123131\n-------------+----------------------------------------------------------------\nlnsig        |\n      female |   .3440266   .0691673     4.97   0.000     .2084611     .479592\n       _cons |  -.9758137   .0488944   -19.96   0.000    -1.071645   -.8799825\n------------------------------------------------------------------------------\n\n. reg  lnwage age educ female\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(3, 1430)      =    167.12\n       Model |  104.907056         3  34.9690188   Prob &gt; F        =    0.0000\n    Residual |  299.212747     1,430  .209239683   R-squared       =    0.2596\n-------------+----------------------------------   Adj R-squared   =    0.2580\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45743\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0161424   .0010978    14.70   0.000      .013989    .0182959\n        educ |   .0719322   .0050365    14.28   0.000     .0620524     .081812\n      female |  -.1453936   .0243888    -5.96   0.000    -.1932352    -.097552\n       _cons |   1.970021   .0725757    27.14   0.000     1.827654    2.112387\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "06nlreg.html#other-considerations-1",
    "href": "06nlreg.html#other-considerations-1",
    "title": "NLS, IRLS, and MLE",
    "section": "Other Considerations",
    "text": "Other Considerations\nWith MLE, as with logit probit tobit, etc, you cannot interpret the models directly!\nThen what?\n\nIdentify what is the Statistic of interest (most of the time its the expected value, conditional mean, or predicted mean). But others may be something related to other parameters (we care about \\(\\sigma\\) not \\(ln \\ \\sigma\\)).\nSee margins, and option expression\nIdentify if you are interested in Average effects, or effects at the average.\nSome times, you may need to use your own predictions!\n\nsee example for probit"
  },
  {
    "objectID": "06nlreg.html#margins-in-action",
    "href": "06nlreg.html#margins-in-action",
    "title": "NLS, IRLS, and MLE",
    "section": "Margins in action",
    "text": "Margins in action\n**Estimate Logit model\nlogit lfp educ female age married divorced\n** use margins to estimate predicted probability\nmargins\n** or use expression\nmargins, expression(exp(xb())/(1+exp(xb())))\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |    .870674   .0071023   122.59   0.000     .8567537    .8845942\n------------------------------------------------------------------------------\n\n** marginal effects\nmargins, dydx(educ) \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\n\n\n** Marginal effect of the marginal effect?\nmargins, expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\nmargins, dydx(*) expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n\n-------------+----------------------------------------------------------------\n        educ |  -.0010759   .0004973    -2.16   0.031    -.0020507   -.0001012\n    1.female |   .0246023   .0054734     4.49   0.000     .0138747    .0353299\n         age |   5.41e-06   .0000502     0.11   0.914     -.000093    .0001039\n   1.married |    .021062    .004861     4.33   0.000     .0115346    .0305893\n  1.divorced |   .0037604   .0013899     2.71   0.007     .0010362    .0064846\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "06nlreg.html#going-beyond",
    "href": "06nlreg.html#going-beyond",
    "title": "NLS, IRLS, and MLE",
    "section": "Going Beyond?",
    "text": "Going Beyond?\nMLE can also be used in more Advanced models.\n\nMulti equation model that may or may not depend on each other.\nFor example, Oaxaca-Blinder Decomposition, requires using Multiple Equations\nIV - CF, or IV TSLS are also multi equation models but which depend on each other.\nOne could also estimate nonlinear versions of Standard models. For example Nonlinear Tobit model (See Ransom 1987)\nEstimation of latent groups of finite mixture models . This combine models that are otherwise unobserved (See Kapteyn and Ypma 2007)\n\nBasically, if you can figure out \\(f(.)\\)’s and how are they connected, you can estimate any model via MLE (with few exceptions)"
  },
  {
    "objectID": "07po_ci.html#introduction-what-if",
    "href": "07po_ci.html#introduction-what-if",
    "title": "Potential outcomes and Causal Models",
    "section": "Introduction: What if?",
    "text": "Introduction: What if?\nFrom here on, the whole purpose of the methodologies we will dicusess is the analysis of causal effects of some:\n\nPolicy, treatment, experiment, or otherwise event\n\nBut, How is it different from what we did before?\nIt isn’t.\nUnder Exogeneity assumption \\(E(e|X)=0\\), one can make causal effect claims.\n\n\n\n\n\n\nWe seek the truth\n\n\nHow much of the change in outcome is caused by the program alone?\n\n\n\nBut this is not as easy."
  },
  {
    "objectID": "07po_ci.html#exampleswhere-is-the-challenge",
    "href": "07po_ci.html#exampleswhere-is-the-challenge",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples…Where is the challenge?",
    "text": "Examples…Where is the challenge?\nA few examples for Causal effect questions:\n\nDo minimum wages increase unemployment ?\nDo Conditional cash transfers improve health outcomes in children?\nDo Covid Vaccines help reduce the Spread of Covid?\n\nThese questions are, however, difficult to answer.\n\nHow do you make sure the “treatment” is the Only factor that explains the difference in outcome across groups??\n\nTo do this we need strategies that rule out any other explanation that could “take away” the connection we seek.\n\nWe need to close all back doors, block all alternative explanations, or nuisanse factors"
  },
  {
    "objectID": "07po_ci.html#potential-outcomes",
    "href": "07po_ci.html#potential-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nChoice"
  },
  {
    "objectID": "07po_ci.html#the-path-not-taken",
    "href": "07po_ci.html#the-path-not-taken",
    "title": "Potential outcomes and Causal Models",
    "section": "The Path not taken",
    "text": "The Path not taken\nThe way we express the \\(TE_i\\), compares counterfactuals. Two possible States of the world, where an allknowing researcher can perfectly identify TE.\nUnfortunately, the same person cannot take both paths, and we cannot see both options. A person is either Treated or Untreated. Thus, the first approach to Casual effects is impossible.\n…\nBut it does provide us with a clue of how to go ahead and analyze Causal effects. We “simply” need to estimate the counterfactual!\nBut before going deeper into how to estimate the counterfactuals And treatment effects some notation"
  },
  {
    "objectID": "07po_ci.html#potential-outcomes-notation",
    "href": "07po_ci.html#potential-outcomes-notation",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes: Notation",
    "text": "Potential Outcomes: Notation\n\n\\(i\\) will represent a unit. Person, city, country, school, classroom, etc\n\\(D\\) will indicate the treatment Status of a unit. \\(D=1\\) means is treated, and \\(D=0\\) is untreated.\nEach unit has two potential outcomes \\(Y_i(D=1)\\) and \\(Y_i(D=0)\\)\nAll units have only one effective or realized outcome: \\(Y_i\\), which is what we observe, and depends on their treatment status:\n\n\\[Y_i=(1-D_i)*Y_i(0)+D_i*Y_i(1)\\]\n\nUnit Specific causal effect is the difference between potential outcomes: \\[\\delta_i = Y_i(1)-Y_i(0)\\]\n\\(\\pi\\) is the proportion of treated units"
  },
  {
    "objectID": "07po_ci.html#parameters-of-interest",
    "href": "07po_ci.html#parameters-of-interest",
    "title": "Potential outcomes and Causal Models",
    "section": "Parameters of Interest",
    "text": "Parameters of Interest\nAssume we can see the true USCE (unit specific casual effect) for all units. In addition to their Treatment Status.\nThere are three parameters one might be interested in analyzing:\n\\[\n\\begin{aligned}\nATE &= E(\\delta_i) \\\\\nATT &= E(\\delta_i|D_i=1) \\\\\nATU &= E(\\delta_i|D_i=0)\n\\end{aligned}\n\\]\nIn general, this three estimands may tell very different stories.\nLets Put some numbers here"
  },
  {
    "objectID": "07po_ci.html#simulating-effects-stata",
    "href": "07po_ci.html#simulating-effects-stata",
    "title": "Potential outcomes and Causal Models",
    "section": "Simulating effects Stata",
    "text": "Simulating effects Stata\n\n\nCode\nclear\nset linesize 255\nset seed 101\nset obs 1000\ngen y0 = rnormal(5)\ngen t  = rnormal(0.0,0.5)\ngen y1 = y0+t\n** Assume only those with t&gt;0 take treatment\ngen trt =(t&gt;0)\ngen y = y0 * (1-trt) +  y1 * (trt)\nformat y0 y1 y t %4.3f \nlist in 1/10, sep(0) clean\n** For Everyone 100 obs\ntabstat t, by(trt)\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n          y0        t      y1   trt       y  \n  1.   5.254   -0.801   4.453     0   5.254  \n  2.   4.997    0.039   5.035     1   5.035  \n  3.   2.608   -0.490   2.118     0   2.608  \n  4.   6.280   -0.579   5.700     0   6.280  \n  5.   5.761    0.090   5.850     1   5.850  \n  6.   6.132    0.211   6.342     1   6.342  \n  7.   4.928    0.153   5.081     1   5.081  \n  8.   4.377   -0.073   4.304     0   4.377  \n  9.   3.142    0.427   3.569     1   3.569  \n 10.   6.050   -0.332   5.718     0   6.050  \n\nSummary for variables: t\nGroup variable: trt \n\n     trt |      Mean\n---------+----------\n       0 | -.3984745\n       1 |  .3748617\n---------+----------\n   Total | -.0187664\n--------------------"
  },
  {
    "objectID": "07po_ci.html#but-there-is-only-1",
    "href": "07po_ci.html#but-there-is-only-1",
    "title": "Potential outcomes and Causal Models",
    "section": "But there is only 1",
    "text": "But there is only 1\nBut, we never see potential outcomes, nor unit specific effects.\nThe most naive estimator is to just estimate the mean difference in “post-treatment” outcome after treatment was in place. But that would be very biased!\n\n\nCode\ngen yy0 = y if trt==0\ngen yy1 = y if trt==1\nlist y yy1 yy0 trt in 1/10\nreg  y trt\n\n\n(491 missing values generated)\n(509 missing values generated)\n\n     +-----------------------------------+\n     |     y        yy1        yy0   trt |\n     |-----------------------------------|\n  1. | 5.254          .   5.254051     0 |\n  2. | 5.035   5.035442          .     1 |\n  3. | 2.608          .   2.608155     0 |\n  4. | 6.280          .    6.27964     0 |\n  5. | 5.850   5.850478          .     1 |\n     |-----------------------------------|\n  6. | 6.342    6.34237          .     1 |\n  7. | 5.081   5.080995          .     1 |\n  8. | 4.377          .   4.376629     0 |\n  9. | 3.569   3.568727          .     1 |\n 10. | 6.050          .   6.049989     0 |\n     +-----------------------------------+\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =     49.49\n       Model |  49.4759442         1  49.4759442   Prob &gt; F        =    0.0000\n    Residual |  997.714103       998   .99971353   R-squared       =    0.0472\n-------------+----------------------------------   Adj R-squared   =    0.0463\n       Total |  1047.19005       999  1.04823829   Root MSE        =    .99986\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |   .4449359   .0632467     7.03   0.000      .320824    .5690477\n       _cons |   4.988793   .0443179   112.57   0.000     4.901826     5.07576\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "07po_ci.html#bias-direction",
    "href": "07po_ci.html#bias-direction",
    "title": "Potential outcomes and Causal Models",
    "section": "Bias Direction",
    "text": "Bias Direction\n\\[\n\\begin{aligned}\nE(Y_i|D_i=1)-E(Y_i|D_i=0) &= ATE  \\\\\n&  +E(Y(0)|D=1)-E(Y(0)|D=0) \\\\\n& +(1-\\pi)(ATT-ATU)\n\\end{aligned}\n\\]\nIntuition: Simple Difference will be biases because\n\nThere could be a selection bias (one group baseline outcome is different from the other)\nTreatment Heterogeneity. Some groups are affected differently from others\n\nFrom these two problems, the second one is easier to handle (either concentrate on ATT or ATU).\nThe first one, however, requires using strategies to be able to account for selection bias."
  },
  {
    "objectID": "07po_ci.html#independence-assumption",
    "href": "07po_ci.html#independence-assumption",
    "title": "Potential outcomes and Causal Models",
    "section": "Independence assumption",
    "text": "Independence assumption\nWhile the Simple mean estimator is most likely to be biased, under Independence assumption, it may still work:\n\\[\nY(1),Y(0) \\perp D\n\\]\nThis means that Treatment Status should NOT depend on the potential outcomes.\nIn other words, there shouldnt be any differneces in the potential outcomes before or after treatment takes place.\nThis eliminates the selection bias. And group Heterogeneity.\n\\[\n\\begin{aligned}\nATT - ATU &= E(Y|D=1) -  E(Y(0)|D=1) \\\\\n&- E(Y(1)|D=0) -  E(Y|D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07po_ci.html#sutva",
    "href": "07po_ci.html#sutva",
    "title": "Potential outcomes and Causal Models",
    "section": "SUTVA",
    "text": "SUTVA\nStable Unit Treatment value assumption\nThis is a strong assumption that is still required to estimate of treatment effects.\nIt assumes that:\n\nTreatment is Homogenous. Same intensity, quality, type of treatment among all treated.\nThere are no spill overs. Your Treatment Status effects you and you only, and you are only affected by your treatment status. No externalities nor Spillovers.\nAlso, there are no general equibrium effects\nAnd NO Anticipation.\n\nThis assumptions namely guaranties that when a unit is not treated, its/his/her outcome will not change."
  },
  {
    "objectID": "07po_ci.html#narrowing-down-the-problem",
    "href": "07po_ci.html#narrowing-down-the-problem",
    "title": "Potential outcomes and Causal Models",
    "section": "Narrowing down the problem",
    "text": "Narrowing down the problem\n\nIndividual level effects are impossible to identify. We only observe one outcome at a time. (not both)\nIt is possible to identify causal effects on groups (treated, not-treated, kind-of-treated). But..\nSimple Mean difference will not identify causal effects, unless Independence and Sutva assumptions hold\n\nThis however, suggests a path. Constructing good counterfactuals can help idenfiying the Causal effects.\nGoal:\n\nIdentify a control/comparison group that is statistically identical to the treated group, except for the Treatment Status"
  },
  {
    "objectID": "07po_ci.html#the-gold-standard-randomized-control-trials",
    "href": "07po_ci.html#the-gold-standard-randomized-control-trials",
    "title": "Potential outcomes and Causal Models",
    "section": "The Gold Standard: Randomized Control Trials",
    "text": "The Gold Standard: Randomized Control Trials\n\n\n\n\n\n\nTo keep in mind\n\n\nSearching for good controls doesnt require having access to perfect “clones”. However, in average, we need groups (T vs UT) that are very similar to each other.\n\n\n\nIn general, research designed is guided by the rules of program or treatment assignment on participants.\nWhen researchers have control on the assingment rules, the best approach is to design a randomized control trial.\nIn an RCT, randomized assigment, eliminates any selection-bias problems (although SUTVA remains as an assumption)"
  },
  {
    "objectID": "07po_ci.html#rct-and-selection-problems",
    "href": "07po_ci.html#rct-and-selection-problems",
    "title": "Potential outcomes and Causal Models",
    "section": "RCT and Selection Problems",
    "text": "RCT and Selection Problems\nConsider the example of the Health Impacts of Hospitals.\n\nHospitals (or health care) should improve health of individuals. but\nOnly unhealthy people will use Health care services. Selection bias\nThus It may look that Hospitals Hurt people’s health becuase those who used it have lower health than those who dont: \\[\nE(Y|D=1)-E(Y|D=0) = ATT + E(Y(0)|D=1)-E(Y(0)|D=0)\n\\]\n\nSo even if Health services help those in need (ATT). If the selection bias is large, Naive estimations may suggest Hospitals are harmful.\nThis is a problem caused because Treatment(going to the Hospital) is affected by pre-conditions or potential treatment outcomes."
  },
  {
    "objectID": "07po_ci.html#stata-example",
    "href": "07po_ci.html#stata-example",
    "title": "Potential outcomes and Causal Models",
    "section": "Stata Example",
    "text": "Stata Example\n\n\nCode\ngen trt2=rnormal()&gt;0\ngen yrct = y0 * (1-trt2) +  y1 * (trt2)\ntabstat t y0 y1 yrct, by(trt2)\nreg yrct trt2\n\n\n\nSummary statistics: Mean\nGroup variable: trt2 \n\n    trt2 |         t        y0        y1      yrct\n---------+----------------------------------------\n       0 | -.0097507  5.061626  5.051875  5.061626\n       1 |  -.027891  4.984309  4.956418  4.956418\n---------+----------------------------------------\n   Total | -.0187664  5.023199  5.004433  5.009338\n--------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =      2.64\n       Model |  2.76706003         1  2.76706003   Prob &gt; F        =    0.1046\n    Residual |  1046.22695       998   1.0483236   R-squared       =    0.0026\n-------------+----------------------------------   Adj R-squared   =    0.0016\n       Total |  1048.99401       999  1.05004406   Root MSE        =    1.0239\n\n------------------------------------------------------------------------------\n        yrct | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        trt2 |  -.1052076   .0647568    -1.62   0.105    -.2322827    .0218675\n       _cons |   5.061626   .0456524   110.87   0.000      4.97204    5.151212\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "07po_ci.html#internal-vs-external-validity",
    "href": "07po_ci.html#internal-vs-external-validity",
    "title": "Potential outcomes and Causal Models",
    "section": "Internal vs External Validity",
    "text": "Internal vs External Validity\nRCTs are not the only strategy that allows you to control the Rules of Treatment. In Experimental Economics this is done quite often\n\nyou select your sample (of students), and randomly assigned treatments of interest (Experimental design).\n\nThere are, however, further considerations to be taken:\n\n\n\n\n\n\nInternal Validity\n\n\nThe estimated Impact is net of all other confunding factors (Random assigment)\n\n\n\n\n\n\n\n\n\nExternal Validity\n\n\nThe estimated impact can be generalized to the population in general. (Random Sample)\n\n\n\n\n\n\n\n\n\nHow to Randomize\n\n\nDepends mostly on how reasonable is to mantain SUTVA assumption"
  },
  {
    "objectID": "07po_ci.html#examples",
    "href": "07po_ci.html#examples",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\nCCT and Education in Mexico\n\n\nProgresa/Prospera is a CCT program for poor mothers based on children school enrollment to suport education attainment.\nEligibility was based on Census data on Poverty levels and Baseline Data collection. But during phase-in period, only 2/3 if localities were selected to receive the Transfer\n\n\n\n\n\n\n\n\n\nWater and Sanitation Intervention in Bolivia\n\n\nIn 2012 IADB and Bolivian Goverment implemented a random assigment of water/sanitation interventions in Small Rural Communities.\nFrom 369 eliginle communities, 182 were selected at random for the program implementation, via public lotteries, constraining on Community Size"
  },
  {
    "objectID": "07po_ci.html#how-to-analyze-the-data",
    "href": "07po_ci.html#how-to-analyze-the-data",
    "title": "Potential outcomes and Causal Models",
    "section": "How to Analyze the Data",
    "text": "How to Analyze the Data\n\nVerify Data Balance: Even if treatment was assigned at random, it is important to verify of groups remain comparable. (Thus avoid compossition effects)\nMean Difference: Because of Random Assigment, one could use simple mean differences to estimate ATE\nConsider using controls: Under RA, controls will not affect the outcome, but may improve precision: \\(y_i = \\alpha_0 + \\tau D_i + x_i\\beta + \\varepsilon_i\\).\n\nBut controls should not be affected by the treatment itself (thus should be pre-treatment)\n\nMay consider falsification tests\nAnd Other Robustness test: Outliers, or distributional impacts may be of interest"
  },
  {
    "objectID": "07po_ci.html#background",
    "href": "07po_ci.html#background",
    "title": "Potential outcomes and Causal Models",
    "section": "Background",
    "text": "Background\nProgresa is a Cash Transfer Program designed to increase School Enrollment among the poor, minimizing desincentives to work.\nThis program provided Grants to families whos children attended school for atleast 85% of the school year, covering between 50/75% of school cost.\nWhile there were 495 localities that were eligible to benefit from the program, only 314 were randomly selected to start reciving resources for the first 2 years. With the unselected localities being treated a couple of years later.\n\nThe program continued beyond the original scope of the policy, now its known as Progresa/Oportunidades"
  },
  {
    "objectID": "07po_ci.html#method",
    "href": "07po_ci.html#method",
    "title": "Potential outcomes and Causal Models",
    "section": "Method",
    "text": "Method\nGoal. Estimate the impact of Progresa (P) on Enrollment (S) \\[\nS_i = a_0 +a_1 P_i + a_2 E_i + a_3 P_iE_i + \\delta Enrolled_c + \\beta X + e_i\n\\]\n\\(P_i=1\\) if the comunity is eligible\n\\(E_i=1\\) if the child is Poor\n\\(P_i * E_i\\) the impact on Poor Chilren in eligible communities.\nModel can be estimated Separately (5 years), or using pooled data\nThis is a kind of DIDID model. However, we could consider it as a simple mean comparison between those Effectively treated and those untreated."
  },
  {
    "objectID": "07po_ci.html#differences-in-characteristics",
    "href": "07po_ci.html#differences-in-characteristics",
    "title": "Potential outcomes and Causal Models",
    "section": "Differences in Characteristics",
    "text": "Differences in Characteristics"
  },
  {
    "objectID": "07po_ci.html#raw-differences",
    "href": "07po_ci.html#raw-differences",
    "title": "Potential outcomes and Causal Models",
    "section": "Raw Differences",
    "text": "Raw Differences\n\nPoor HH"
  },
  {
    "objectID": "07po_ci.html#with-controls",
    "href": "07po_ci.html#with-controls",
    "title": "Potential outcomes and Causal Models",
    "section": "With Controls",
    "text": "With Controls"
  },
  {
    "objectID": "07po_ci.html#other-outcomes",
    "href": "07po_ci.html#other-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Other Outcomes",
    "text": "Other Outcomes"
  },
  {
    "objectID": "08panel_FE.html#re-cap-potential-outcome-model",
    "href": "08panel_FE.html#re-cap-potential-outcome-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nWhy does this work??\nOne way to understand this it to imagine that potential outcomes are a function of all observed and unobserved individual characteristics, plust the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\n\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, except for the Treatment Status.\nDifferences between the two states are explained only by the treatment!"
  },
  {
    "objectID": "08panel_FE.html#the-problem-and-first-solution-rct",
    "href": "08panel_FE.html#the-problem-and-first-solution-rct",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The Problem and first Solution RCT",
    "text": "The Problem and first Solution RCT\nWe do not observe both States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nOne way to do so is via RCT, for example using a lottery!\n\nWhy does it work?\n\nPotential outcomes will be unrelated to treatment, because treatmet is assigned at random.\nHere, it also means that \\(X's\\) and \\(u's\\) will be similar across groups (because of random assigment)\nBut…you cannot estimatate individual effects, but at least estimate aggregate effects (ATE = ATT = ATU)"
  },
  {
    "objectID": "08panel_FE.html#other-solutions",
    "href": "08panel_FE.html#other-solutions",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Other Solutions?",
    "text": "Other Solutions?\nSo RCTs can be very expensive, and difficult to implement after the fact. In those situations, however, you can use observed data to try answering the same questions!.\nOne option? Something we have done before…Regression Analysis!\n\\[y_i = a_0 + \\delta D_i + X_i\\beta + e_i\\]\nThe idea is that you directly control for all confounding factors that could be related to \\(y_i\\) and \\(d_i\\).\nIn other words, you add controls until \\(D_i\\) is exogenous! \\(E(e_i|D)=0\\)"
  },
  {
    "objectID": "08panel_FE.html#implications-to-the-po-model",
    "href": "08panel_FE.html#implications-to-the-po-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Implications to the PO model?",
    "text": "Implications to the PO model?\n\nAssumes all individuals have the same outcome structure (\\(\\beta s\\)), except for the TE\nThe treatment is effect is homogenous (no heterogeneity)\nand that functional form is correct (for extrapolation)\n\nHowever, explicitly controlling for covariates, balances characteristics (FWL):\n\\[\n\\begin{aligned}\nD_i &= X\\gamma + v_i \\\\\ny_i &= a_0 + \\delta v_i + u_i \\\\\n\\delta &=\\frac{1}{N} \\sum \\frac{D_i - X\\gamma}{var(v_i)} y_i\n\\end{aligned}\n\\]\n\nTreated units will get positive weights, and controls negative weights, with exceptions because of the LPM.\nWeights will “balance Samples” to estimate ATE."
  },
  {
    "objectID": "08panel_FE.html#controlling-for-unobservables",
    "href": "08panel_FE.html#controlling-for-unobservables",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Controlling for Unobservables",
    "text": "Controlling for Unobservables"
  },
  {
    "objectID": "08panel_FE.html#what-if-you-can-see-xs",
    "href": "08panel_FE.html#what-if-you-can-see-xs",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "What if you can See X’s",
    "text": "What if you can See X’s\nSome times, you may have situations where some covariates cannot be observed (Z_i): \\[\ny_i = \\delta D_i +X_i \\beta + Z_i \\gamma + e_i\n\\]\nIf \\(Z_i\\) is unrelated to \\(D_i\\), you are on the clear. If its unrelated to \\(Y_i\\) you are also ok. But what if that doesn happen?\n\nThen you have a problem!\n\nYou no longer can use regression, because the potential outcomes will no longer be independent of the treatment.\nyou are dooomed!\n(when would this happen)"
  },
  {
    "objectID": "08panel_FE.html#having-access-to-more-data",
    "href": "08panel_FE.html#having-access-to-more-data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Having access to More Data",
    "text": "Having access to More Data\nSolution?: Say you have access to panel data: Same individuals across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{it} \\gamma + e_{it}\n\\]\nIf we can’t measure \\(Z_{it}\\), and you estimate this using Pool OLS (just simple OLS), you still need the assumption that:\n\\[E(Z_{it}\\gamma + e_{it}|D_it)=0\n\\]\nBut that doesnt solve the problem if \\(Z_{it}\\) is related to \\(D_it\\).\nOne option, in cases like this, is assuming that individual unobservables are fixed across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it}\n\\]\nin which case, it may be possible to estimate Treatment effects"
  },
  {
    "objectID": "08panel_FE.html#fixed-effects",
    "href": "08panel_FE.html#fixed-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nWith panel data and assuming unobservables are fixed across time, estimating TE is “Easy”. Just add Dummies for each individual!\n\\[\ny_{it}= \\delta D_{it} +X_{it} \\beta + \\sum d_i \\gamma_i + e_{it}\n\\]\nHere \\(d_i \\gamma_i\\) is our proxy for ALL unobserved factors. OLS can be used to estimate ATEs\nThis happens because we can estimate potential outcome under the same assumptions as before.\nyou could, in fact, consider adding fixed effects for all dimensions you consider important to account for:\n\nCity, school, region, age, industry, etc\n\nThe only limitation…how many dummies can your computer handle? What happens internally?"
  },
  {
    "objectID": "08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "href": "08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects: Estimation - The variation within",
    "text": "Fixed Effects: Estimation - The variation within\nThe obvious approach is using dummies. But that can take you only so far (why?), and may create other problems! (excluded variables)\nThe alternative is using the within estimator. Say we take the means across individuals, and use that to substract information from the original regression:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it} \\\\\n\\bar y_i &= \\delta \\bar D_i + \\bar X_i \\beta +  Z_{i} \\gamma + \\bar e_i \\\\\ny_{it}-\\bar y_i = \\tilde y_{it} &=\\delta \\tilde D_{it} + \\tilde X_{it}\\beta+\\tilde e_{it}\n\\end{aligned}\n\\]\nLast equation is easier to estimate (no dummies!) however you need within variation. IF unobserved factors are fixed, they will be “absorbed”.\nAlso, the SE will have to be adjusted for degrees of freedom. (but nothing else)\nThis is nothing else but the use of FWL and regression on residuals."
  },
  {
    "objectID": "08panel_FE.html#dont-forget-random-effects",
    "href": "08panel_FE.html#dont-forget-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Dont Forget Random Effects",
    "text": "Dont Forget Random Effects\nThis approach is more efficient than Fixed effects because you don’t estimate fixed effects, just the distribution.\nSo how does this affect the estimation:\n\nErrors have two components. One time fixed \\(e_i\\), and one time variant \\(u_{it}\\). Then total errors will be correlated with themselves across time \\[corr(v_{it}, v_{is}) =  corr(e_i+u_{it}, e_i+u_{is}) = \\sigma^2_e\n\\]\nApply FGLS to eliminating this source of auto-correlation! \\[y_{it}-\\lambda \\bar y_i = (X_{it}-\\lambda \\bar X_it) + v_{it}\\]\n\nBut, you need the assumption that unobservables \\(e_i\\) are unrelated to \\(X's\\). (because we are not directly controlling for them).\nThe advantage, however, is that you do no need within variation!"
  },
  {
    "objectID": "08panel_FE.html#fe-vs-re",
    "href": "08panel_FE.html#fe-vs-re",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE vs RE",
    "text": "FE vs RE\nSo there are two ways to Analyze data Panel data.\n\nFE: Uses only within variation, is more consistent, but less efficient (too many dummies)\nRE: Uses all variation in data, is less consistent (stronger assumptions), but more efficient!\n\nHow to choose?\nThe Standard approach is to apply a Hausan Test:\n\nH0:\\(\\beta^{FE} = \\beta^{RE}\\) using Chi2\n\nIf they are not different (H0 cannot be rejected), then choose RE (efficient). If they are different then choose FE (consistent)"
  },
  {
    "objectID": "08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "href": "08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "More Fixed effects: TWFE - NWFE?",
    "text": "More Fixed effects: TWFE - NWFE?\nWith multiple sets of fixed effects (individual, time, cohort, region, etc), you can still use dummies to add them to the model.\nBut, you can apply something similar to the previous approach:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it}+x_{it}\\beta + \\gamma_i + \\gamma_t + e_{it} \\\\\n\\bar y_i &= \\bar D_i +\\bar x_{i}\\beta + \\gamma_i + E(\\gamma_t|i) + \\bar e_i \\\\\n\\bar y_t &= \\bar D_t +\\bar x_{t}\\beta + E(\\gamma_i|t) + \\gamma_t + \\bar e_t \\\\\n\\bar y &= \\bar D +\\bar x\\beta + E(\\gamma_i) + E(\\gamma_t)+ \\bar e \\\\\n\\tilde y_{it} &= y_{it}-\\bar y_i - \\bar y_t + \\bar y\n\\end{aligned}\n\\]\nSo one can estimate the following:\n\\[\n\\tilde y_{it} = \\delta \\tilde D_{it} + \\tilde X_{it} \\beta + \\tilde e_{it}\n\\]\nThis eliminates FE for both time and individual (if panel is balanced)"
  },
  {
    "objectID": "08panel_FE.html#second-option",
    "href": "08panel_FE.html#second-option",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Second Option:",
    "text": "Second Option:\nAlternatively, you can just run regressions of residuals:\n\\[\nw_{it} = \\gamma^w_i+\\gamma^w_t+rw_{it}\n\\]\nand make regressions using the residuals. (Demeaning also works, but its an iterative process)"
  },
  {
    "objectID": "08panel_FE.html#stata-example",
    "href": "08panel_FE.html#stata-example",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Stata Example",
    "text": "Stata Example\n\n\nCode\n#delimit;\nfrause school93_98, clear;\nxtset schid year;\nqui:reg math4 lunch lenrol lrexpp                     ; est sto m1;\nqui:xtreg math4 lunch lenrol lrexpp                   ; est sto m2;\nqui:xtreg math4 lunch lenrol lrexpp, fe               ; est sto m3;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid)     ; est sto m4;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid year); est sto m5;\nesttab m1 m2 m3 m4 m5, mtitle(ols re fe refe1 refe2) compress se b(3);\nhausman m3 m2;\n\n\n\nPanel variable: schid (strongly balanced)\n Time variable: year, 1993 to 1998\n         Delta: 1 unit\n\n---------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)   \n                 ols           re           fe        refe1        refe2   \n---------------------------------------------------------------------------\nlunch         -0.413***    -0.370***     0.057        0.057       -0.062*  \n             (0.007)      (0.011)      (0.031)      (0.031)      (0.026)   \n\nlenrol        -0.121        0.936        8.766***     8.766***     0.297   \n             (0.425)      (0.616)      (1.704)      (1.704)      (1.468)   \n\nlrexpp        28.887***    39.161***    46.450***    46.450***     2.799*  \n             (0.860)      (0.878)      (1.006)      (1.006)      (1.265)   \n\n_cons       -162.292***  -254.864***  -377.338***  -377.423***    37.398*  \n             (7.960)      (8.681)     (14.913)     (14.918)     (15.847)   \n---------------------------------------------------------------------------\nN               9369         9369         9369         9328         9328   \n---------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       m3           m2         Difference       Std. err.\n-------------+----------------------------------------------------------------\n       lunch |     .056932    -.3703211        .4272531        .0287753\n      lenrol |    8.766051     .9357725        7.830279        1.588902\n      lrexpp |    46.44966     39.16107        7.288595        .4915896\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 627.26\nProb &gt; chi2 = 0.0000"
  },
  {
    "objectID": "08panel_FE.html#correlated-random-effects",
    "href": "08panel_FE.html#correlated-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Correlated Random Effects",
    "text": "Correlated Random Effects\nRandom effects model may produce inconsistent results, because it assumes unobserved factors are uncorrelated to characteristics.\nFixed effects controls for individual effects explicitly, or via demeaning.\nA 3rd approach is known as CRE model. A more explicit modeling of the unobserved but fixed components.\n\nCall the unobserved component \\(a_i\\), and say we suspect it may be related with individual characteristics.\nBecause \\(a_i\\) is constant over time, it may be reasonable assuming its correlated with individual average characteristics: \\[a_i = a + \\bar X_i \\gamma + r_i\n\\qquad(1)\\]\n\nBy construction, \\(r_i\\) and \\(X_{it} \\& \\bar X_i\\) will be uncorrelated. So lets just add that to the main model"
  },
  {
    "objectID": "08panel_FE.html#cre",
    "href": "08panel_FE.html#cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "CRE",
    "text": "CRE\nLets add Equation 1 to our main equation. \\[\ny_i = \\beta X_{it} +  \\theta Z_{i} + \\bar X_i \\gamma + r_i + e_{it}\n\\]\nThis equation can now be estimated using RE, because it already allows controls for the correlation of unobserved factors and the individual effects.\nYou can also estimate the model using pool OLS, clustering errors at individual level.\nResult:\n\nyou now have a model that allows for time variant and time fixed components, that is consistent as FE (same coefficients).\n\nUses:\n\nSimpler way to test for FE vs RE (are the \\(\\gamma 's\\) significant?)\nthere is no need for within variation for any variable! (just overall variation)"
  },
  {
    "objectID": "08panel_FE.html#cre-in-stata",
    "href": "08panel_FE.html#cre-in-stata",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "cre in Stata",
    "text": "cre in Stata\n\n\nCode\n#delimit cr\nfrause school93_98, clear\nreghdfe math4 lunch lenrol lrexpp, abs(schid year) cluster(schid)\n** Experimental\ncre, abs(schid year): reg math4 lunch lenrol lrexpp, cluster(schid)\n\n\n(dropped 41 singleton observations)\n(MWFE estimator converged in 5 iterations)\n\nHDFE Linear regression                            Number of obs   =      9,328\nAbsorbing 2 HDFE groups                           F(   3,   1734) =       2.59\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0515\n                                                  R-squared       =     0.7548\n                                                  Adj R-squared   =     0.6985\n                                                  Within R-sq.    =     0.0014\nNumber of clusters (schid)   =      1,735         Root MSE        =    11.5747\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0324188    -1.92   0.056    -.1256705    .0014978\n      lenrol |   .2966956   1.484868     0.20   0.842    -2.615625    3.209017\n      lrexpp |   2.798777   1.410581     1.98   0.047     .0321579    5.565397\n       _cons |   37.39798    16.8327     2.22   0.026     4.383449     70.4125\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n       schid |      1735        1735           0    *|\n        year |         6           0           6     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n_reghdfe_r~d |      9,328    3.88e-17    .1015168  -1.180599   1.214313\n\n\nLinear regression                               Number of obs     =      9,328\n                                                F(9, 1734)        =     821.90\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4595\n                                                Root MSE          =     15.505\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0327488    -1.90   0.058    -.1263177     .002145\n      lenrol |   .2966948   1.509296     0.20   0.844    -2.663538    3.256927\n      lrexpp |   2.798777   1.435068     1.95   0.051    -.0158696    5.613423\n    m1_lunch |  -.3799049   .0343728   -11.05   0.000    -.4473213   -.3124884\n    m2_lunch |  -2.750266   .5058811    -5.44   0.000    -3.742467   -1.758065\n   m1_lenrol |  -2.394415   1.633657    -1.47   0.143    -5.598561     .809731\n   m2_lenrol |  -273.8924   11.25589   -24.33   0.000    -295.9689   -251.8158\n   m1_lrexpp |   5.667779   2.276245     2.49   0.013     1.203305    10.13225\n   m2_lrexpp |   73.40469   3.276595    22.40   0.000      66.9782    79.83119\n       _cons |   37.39799   17.10933     2.19   0.029     3.840901    70.95507\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "href": "08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Not everything is solved using FE",
    "text": "Caveats: Not everything is solved using FE\n\nWhile FE allows you do control for unobserve but time fixed factors, it will Not help you if those factors are time varying.\nif \\(e_{it}\\) is different across treated and control groups \\(D_{it}=0,1\\) then TE cannot be estimated.\nThis could happen if cases of reverse causality or\nBecause it depends strongly on within variation, it will be more sensitive to measurement errors. Specifically:\n\n\\[\\beta^{fe} = \\beta * \\left(1-\\frac{\\sigma_v^2}{(\\sigma^2_v+\\sigma^2_x)(1-\\rho_x)}\\right)\n\\]\nIn other words. when \\(X\\) has strong autocorrelation (Stable treatment), the measurement error effect is far larger!"
  },
  {
    "objectID": "08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "href": "08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: FE makes things harder to analyze",
    "text": "Caveats: FE makes things harder to analyze\n\nWhen using a single FE, OLS using within variation to identify the slope coefficients. How does a change in X’s (compared to the average) affect changes in the outcomes (respect to averages)\nWhen using Two Fixed effects (individuals and time) identification becomes tricky: \\[\\tilde y_{it} = y_{it}-\\bar y_i - \\bar y_t + \\bar y  \\]\n\nWe are looking for variation across time but also across individuals.\n\nwe are using changes in outcome that are different from the average changes in the sample.\n\n\nwith Multiple FE, same story…we are trying to exploit variation across multiple dimensions! Difficult to understand"
  },
  {
    "objectID": "08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "href": "08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Some times, the variation may be wrong:",
    "text": "Caveats: Some times, the variation may be wrong:\nConsider:\n\\[y_{it} = a_i + a_t + \\delta D_{it} + e_{it}\\]\nIf \\(D_it\\) changes only for some people at the same time, we are good.\n\nThe variation comes from comparing individuals (before and after) (time variation), who were treated and untreated (individual effects)\n\nBut if \\(D_{it}\\) changes at different times for different people, we have a problem.\n\nWho is being compared???\n\nThose before and after (fine) to those with Status change (D=0 -&gt; D=1) to those whos status do not change! (D=0 to D=0) or (D=1 to D=1)\n\n\nWe will discuss this problem again when talking about DID"
  },
  {
    "objectID": "08panel_FE.html#income-schooling-and-ability",
    "href": "08panel_FE.html#income-schooling-and-ability",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Income, Schooling, and Ability:",
    "text": "Income, Schooling, and Ability:\nEvidence from a New Sample of Identical Twins\nby\nOrley Ashenfelter and Cecilia Rouse"
  },
  {
    "objectID": "08panel_FE.html#motivation",
    "href": "08panel_FE.html#motivation",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Motivation:",
    "text": "Motivation:\nIn search of Returns to Education\nThis paper aims to identify returns of education abstracting from the impact of innate ability.\nIn their framework, ability is mostly explained by genetics, thus to control for it, the authors use a sample of identical twins, to “absorb” unobserved genetics using FE.\nThey address some of the problems inherited to FE estimation"
  },
  {
    "objectID": "08panel_FE.html#the-model",
    "href": "08panel_FE.html#the-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\n\nThe theoretical model described states that all individuals have an optimal level of Schooling, such that maximizes the his/her returns.\nHowever, Total schooling may be affected by measurement or optimization errors.\nSchooling will be directly affected by returns to education, but also by the ability of students.\n\nIn their framework, for the twins setup, (log)earnings will be determined by:\n\\[\n\\begin{aligned}\ny_{1j}=A_j + b_j S_{1j} + \\gamma X_j + e_{1j} \\\\\ny_{2j}=A_j + b_j S_{2j} + \\gamma X_j + e_{2j}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "08panel_FE.html#the-model-1",
    "href": "08panel_FE.html#the-model-1",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\nBecause ability is related to schooling, they suggest using the following:\n\\[\ny_{ij}=\\lambda(0.5(S_{1j}+S_{2j}) + b_j S_{1j} + \\gamma X_j + v_j+ e_{ij}\n\\]\nWhich is the equivalent to CRE. Or estimate the fixed effects equivalent:\n\\[y_{1j} - y_{2j} = b(S_{2j}-S_{1j}) + e_{2j} - e_{2j}\n\\]\nThe later is a First difference, rather than FE estimator, but they both identical when T=2.\n\nAn additional model the authors use is one where returns to education could be related to ability.\nOr where ability is measured/proxied by parents education. (which is fixed across twins)"
  },
  {
    "objectID": "08panel_FE.html#data",
    "href": "08panel_FE.html#data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "08panel_FE.html#ols",
    "href": "08panel_FE.html#ols",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "OLS",
    "text": "OLS"
  },
  {
    "objectID": "08panel_FE.html#fe-re-cre",
    "href": "08panel_FE.html#fe-re-cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE-RE-CRE?",
    "text": "FE-RE-CRE?"
  },
  {
    "objectID": "08panel_FE.html#heterogeneity",
    "href": "08panel_FE.html#heterogeneity",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Heterogeneity",
    "text": "Heterogeneity"
  },
  {
    "objectID": "09iv.html#recap-pos-and-rcts",
    "href": "09iv.html#recap-pos-and-rcts",
    "title": "Instrumental Variables",
    "section": "Recap: PO’s and RCT’s",
    "text": "Recap: PO’s and RCT’s\n\nQuick Recap. The goal of the methodologies we are covering is to identify treatment effects.\nIn the PO framework, that is done by simply comparing a group with itself, in two different States (treated vs untreated)\nSince this is impossible, the next best solution is using RCT. Individuals are randomized, and assuming every body follows directions, we can identify treatment effects of the experiments.\nBut only if the RCT is well executed! Sometimes even that may fail"
  },
  {
    "objectID": "09iv.html#instrumental-variables",
    "href": "09iv.html#instrumental-variables",
    "title": "Instrumental Variables",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\n\nWhile here discussed 3rd, the second best approach to identify Treatment effects is by using Instrumental variables.\nIn fact with a Good-Enough instrument, one should be able to identify ANY causal effect. Assuming such IV exists.\n\nbut how?\n\nIf the instrument is good, it may create an exogenous variation, which will allow us to identify Treatment effects by looking ONLY at those affected by the treatment!\nUsing the external variation, we can Estimate TE comparing two groups who are identical in every aspect, except being expose to the Instrument, because they were exposed to the instrument. The randomization comes Because of the IV!"
  },
  {
    "objectID": "09iv.html#cannonical-iv",
    "href": "09iv.html#cannonical-iv",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nAs we have mentioned, the estimation of TE require that we identify two groups of individuals with mostly similar (if not identical) characteristics. This include unobserved characteristics.\nIf the latter is not true, we have a problem of confunders or Endogeneity. But why?\nConsider the following diagram\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is direct, because there is nothing else that would get people confuse why treatment affects outcome\n\n\n\n\n\nflowchart LR\n  e(error) --&gt; Y(Outcome)  \n  D(Treatment) --&gt; Y(Outcome)  \n\n\n\n\n\n\n\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is not as clear, because there is an additional factor \\(v\\) that affects \\(D\\) and \\(Y\\) (is in the way)\n\n\n\n\n\nflowchart LR\n  e( error ) --&gt; Y(Outcome)  \n  D( Treatment ) --&gt; Y(Outcome)\n  v(unobserved) -.-&gt; D(Treatment)  \n  v(unobserved) -.-&gt; Y(Outcome)"
  },
  {
    "objectID": "09iv.html#cannonical-iv-1",
    "href": "09iv.html#cannonical-iv-1",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nHere is where a good instrument comes into play.\n\nNot everything in \\(D\\) is affected by \\(v\\). Some may, but some may be trully exogenous. What if we have an instrument that helps you ID this:\n\n\n\n\n\nflowchart LR\ne( error ) --&gt; Y(Outcome)  \n  subgraph Treatment\n    D1(Exog) \n    D2(Endog)\n  end\n  \n  D1( Exog ) --&gt; Y(Outcome)\n  D2( Endog ) --&gt; Y(Outcome)\n  Z(Instrument) --&gt; D1  \n  v(unobserved) -.-&gt; D2\n  \n  v(unobserved) -.-&gt; Y(Outcome)  \n\n\n\n\n\n\nBy Isolating those affected by the Instrument Alone, we do not need to worry about endogeneity anymore."
  },
  {
    "objectID": "09iv.html#properties",
    "href": "09iv.html#properties",
    "title": "Instrumental Variables",
    "section": "Properties",
    "text": "Properties\nInstrumental variables should have at the very list 2 Properties\n\nThe instrumental variable \\(Z\\) should not be correlated with the model error (Validity).\nBut, it should explain the treatment Itself \\(D\\) (Relevance).\n\nFailure of (1) may reintroduce problems of endogeneity. Faiture of (2) will make the instrument Irrelevant."
  },
  {
    "objectID": "09iv.html#how-does-it-work",
    "href": "09iv.html#how-does-it-work",
    "title": "Instrumental Variables",
    "section": "How does it work",
    "text": "How does it work\nConsider the following.\n\nPeople who study more, tend to earn higher wages\nPeople with high ability tend to study more.\nPeople with high ability, also earn higher wages.\n\nDoes Studying more generate higher wages?\nInstrument. We create a lottery that provides some people resources to pay for their education. This gives them a chance to study more (regardless of ability). \\[Z \\rightarrow D\\]"
  },
  {
    "objectID": "09iv.html#section",
    "href": "09iv.html#section",
    "title": "Instrumental Variables",
    "section": "",
    "text": "So, we know the instrument was Random. We can analyze how much outcome increases among those benefited by the Lottery.\n\\[E(W|Z=1)-E(W|Z=0)\\]\n\nThis is often called the reduced form effect.\nIn principle, \\(Z\\) only affects wages because of education. So looking at this differences should be similar to a treatment effect of Lotteries.\nThese are also known as Intention to treatment effect. Which will bias towards zero, because not everyone will effectively make use of the opporunities"
  },
  {
    "objectID": "09iv.html#section-1",
    "href": "09iv.html#section-1",
    "title": "Instrumental Variables",
    "section": "",
    "text": "In othe words, not everyone will Study more…So we can see if the lotery had that effect.\n\\[E(S|Z=1)-E(S|Z=0)\\]\nThis is the equivalent to the first stage. Where we measure the impact of the “instrument/lottery” on Education (to see, say, relevance)\nFinally, the TE is given by the Ratio of thes two\n\\[TE=\\frac{E(W|Z=1)-E(W|Z=0)}{E(S|Z=1)-E(S|Z=0)}\n\\]\nThis is also known as the Wald Estimator. How much of the changes in wages is due to changes in the “# treated”"
  },
  {
    "objectID": "09iv.html#some-commnents",
    "href": "09iv.html#some-commnents",
    "title": "Instrumental Variables",
    "section": "Some commnents",
    "text": "Some commnents\n\nThis was an example of a binary instrument, which was assigned at random.\nIn fact, this particular scenario is typical byproduct of “failed” RCTs!\n\nPartially failed RCTs: Not every body selected WAS treated\n\n\nConsider the following:\n\nWe make the RCT above giving bouchers to People so they Study more.\nBut, not everybody uses the bouchers:\n\nSome use them and study more.\nSome decide to not use them.\n\n\nComparing Wages among those who receive will only provide you the “intention to treat” effect. (Reduced form)\nBecause of imperfect compliance we need to “readjust/inflate” our TE estimate."
  },
  {
    "objectID": "09iv.html#more-comments",
    "href": "09iv.html#more-comments",
    "title": "Instrumental Variables",
    "section": "More Comments",
    "text": "More Comments\n\nIn this scenario the Reduced form and second stage can be estimated by just comparing means, because the treatment was randonmized.\n\nIn other words, something you really want is an instrument that is as good as random.\n\nThe effect we capture is a LOCAL treatment effect (LATE).\n\nHowever, it could be an ATE if:\n\nThe effect is homogenous for everyone.\nThe people affected by the treatment is a representative of the population\n\nIt all boils down to identifying who is or might be affected by the treatment.\nFor now, lets assume effects are Homogenous (So we get ATEs)"
  },
  {
    "objectID": "09iv.html#who-are-affected-and-who-are-not",
    "href": "09iv.html#who-are-affected-and-who-are-not",
    "title": "Instrumental Variables",
    "section": "Who Are Affected and who are not?",
    "text": "Who Are Affected and who are not?\nEven if we are able to identify ATEs, its important to understand who can be affected by the instrument, because the population is generally selected in 3 groups\n\nnever takers & always takers: These are the individuals who would have never done anything different than their normal.\n\nPerhaps their likelihood was already too low (or high) to be affected.\n\nCompliers: These are the ones who, given they receive “instrument”, they comply and follow up. We use their variation for analysis.\nDefiers: These are the ones who, given Z, will do the oposite. We cannot differentiate them from Compliers, so they will affect how treatment is estimated.\n\nWe do not want to have defiers!"
  },
  {
    "objectID": "09iv.html#extension-1-continuous-instrument",
    "href": "09iv.html#extension-1-continuous-instrument",
    "title": "Instrumental Variables",
    "section": "Extension 1: Continuous Instrument",
    "text": "Extension 1: Continuous Instrument\nThe Wald estimator is for the simplest case of binary treatment. However, if the treatment is continuous, one could modify the IV estimator as follows:\n\\[\n\\delta_{IV} = \\frac{cov(y,z)}{cov(d,z)}\n\\]\nThe logic remains. We are trying to see how variation in the outcome related to Z reates to changes in treatment because of Z.\nThe treatment here is very small (Small changes in d). The intuition is that we are averaging the variation in the outcome across all Zs to estimate the effect."
  },
  {
    "objectID": "09iv.html#extension-2-controls",
    "href": "09iv.html#extension-2-controls",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nAdding controls to the model is also straight forward, and you have quite a few options for it\n\nAdding exogenous controls may help improving model precision, even if instrument was randomized. The easiest way to do this is by applying the 2sls procedure (among others)\n\n\\[\n\\begin{aligned}\n1st: d = z\\gamma_z + x\\gamma_x + e_1  \\\\\n2nd: y = x\\beta_x + \\delta \\hat d+ e_2\n\\end{aligned}\n\\]\n\nThe 1st stage “randomizes” instrument to measure the effect on treatment.\nThe 2nd stage uses predicted values of the first to see what the impact on the outcome will be.\nThis works because \\(\\hat d\\) is exogenous, “carrying over” exogenous changes in the treatment."
  },
  {
    "objectID": "09iv.html#extension-2-controls-1",
    "href": "09iv.html#extension-2-controls-1",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nOne can also think of the approach as a pseudo Wald Estimator, with continuous variables:\n\\[\n\\begin{aligned}\n1st: d &= \\gamma_z * z + x\\gamma_x + e_1  \\\\\nrd:  y &= \\beta_z  * z+ x\\beta_x + e_2 \\\\\nATE &=\\frac{\\beta_z}{\\gamma_z}=\\frac{cov(y,\\tilde z)}{cov(d,\\tilde z)}\n\\end{aligned}\n\\]\nThis compares average changes in the outcome to average changes in the treatment."
  },
  {
    "objectID": "09iv.html#extension-3-multiple-endogenous-variables",
    "href": "09iv.html#extension-3-multiple-endogenous-variables",
    "title": "Instrumental Variables",
    "section": "Extension 3: Multiple Endogenous Variables",
    "text": "Extension 3: Multiple Endogenous Variables\nAlthough less common in Causal Analysis perspective, in other frameworks one may to consider more than 1 instrument or using instrument interactions. In these cases one still has two alternatives\n\n2SLS: One can model more than one endogenous variable at the same time, simply substituting the predicted values in the main regression. When using interactions, or polynomials, each will need its own first stage regression.\nControl function approach. In contrast with the “prediction subtstitution” approach, this method suggests using a “residual inclusion approach”. This controls for endogeneity directly. If there is only one endogenous variable (with interactions of polynomials) only one model is needed.\n\nIn the first case, you need at least 1 instrument per regression. Even if its just a transformation of the original variable\nIn the second case, you need at least 1 instrument per endogenous variable."
  },
  {
    "objectID": "09iv.html#instrument-validity",
    "href": "09iv.html#instrument-validity",
    "title": "Instrumental Variables",
    "section": "Instrument Validity",
    "text": "Instrument Validity\nAs mentioned earlier, Instruments require to fullfill two conditions:\n\nRelevant. They need to be Strongly related to the endogenous variable\nExogenous. instruments should not and cannot be endogenous. In fact, you want instruments that are as good as random, thus not defined by the “system” in anyway."
  },
  {
    "objectID": "09iv.html#iv-validity-exogeneity",
    "href": "09iv.html#iv-validity-exogeneity",
    "title": "Instrumental Variables",
    "section": "IV Validity: Exogeneity",
    "text": "IV Validity: Exogeneity\nUnfortunately, for most cases, this assumption is not testable, because we do not observe the model unobservables, thus dont know if \\(z\\) is related to those unobserved components.\nWhile most efforts for these are done through model design, or argumentation, there are at least 2 options to verify the exogeneity\n\nIf truly exogenous, the instrument should be as good as random. Thus controls shouldnt be affected by the instrument. (Balance test)\nOtherwise, one could test for exogeneity only by comparing Estimates across different IV’s. Different results may suggest instruments are invalid.\n\nRun a regression of Residuals from the main model against all exogenous variables plus other instruments.\n\n\nNote: Unless the instrument was randomized, assumed is going to be slighly endogenous."
  },
  {
    "objectID": "09iv.html#iv-validity-strength",
    "href": "09iv.html#iv-validity-strength",
    "title": "Instrumental Variables",
    "section": "IV Validity: Strength",
    "text": "IV Validity: Strength\nThe only thing we could probably do is try to analyze model strength. How much does the instrument affect treatment take up? is the effect marginal? or a large effect?\nWeaker instruments may create larger problems on the analysis because:\n\nWith weaker instruments, the precision of the estimator drops substantially.\nWith weaker instruments, any “endogeneity” problem (even due to randomness) will generate a bias\n\n\nStock and Yogo (2005) suggest and F~13.9 (or higher) for a 5% bias\nLee, et al (2020) suggest you need even higher F’s if you want to avoid problems with CI\n\n\nWith weak instruments, distribution of beta coefficients will no longer be normal!"
  },
  {
    "objectID": "09iv.html#iv-strength",
    "href": "09iv.html#iv-strength",
    "title": "Instrumental Variables",
    "section": "IV Strength",
    "text": "IV Strength\n\n\nCode\ncapture program drop simx\nprogram simx, eclass\n    clear\n    set obs 500\n    gen z=rnormal()&gt;0\n    gen u1=rnormal()\n    gen u2=rnormal()\n    gen u3=rnormal()\n    forvalues i = 1/5 {\n        gen d`i' = ((-0.5+z) + (u1 + u2)*0.5*`i')&gt;0\n        gen y`i' = 1 + d`i'+u3+u2\n    }\n    forvalues i = 1/5 {\n        reg d`i' z\n        matrix b`i' =(_b[z]/_se[z])^2\n        ivregress  2sls y`i' (d`i'=z)\n        matrix b`i' = b`i',_b[d`i'],_se[d`i'],_b[d`i']/_se[d`i']\n        matrix colname b`i'=f_stat beta beta_se beta_t\n        matrix coleq b`i'=md`i'\n    }\n    matrix b=b1\n    forvalues i = 2/5 {\n        matrix b=b,b`i'\n    }\n    ereturn post b\nend\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmd1_b_f_stat |        494    189.9262    38.63693   101.8865   337.4117\n  md1_b_beta |        494    .9912474    .2289997   .1758122    1.64053\nmd1_b_beta~e |        494    .2440663    .0235266   .1867024   .3293121\n-------------+---------------------------------------------------------\nmd2_b_f_stat |        494    43.33591    13.76347   12.07967   101.6624\n  md2_b_beta |        494    .9717824    .4421832  -.8390987   2.327243\nmd2_b_beta~e |        494    .4711093    .0945389   .3032212   .9873207\n-------------+---------------------------------------------------------\nmd3_b_f_stat |        494    19.25043     8.81203   2.051613   61.55846\n  md3_b_beta |        494    .9354647    .6820819  -1.964163   2.996998\nmd3_b_beta~e |        494    .7430147    .2493365    .383426    2.16554\n-------------+---------------------------------------------------------\nmd4_b_f_stat |        494    11.19082    6.428525   .0483399   38.44057\n  md4_b_beta |        494    .8711808    .9987467  -4.383311   5.634943\nmd4_b_beta~e |        494    1.135114    1.244907   .4609722   25.16503\n-------------+---------------------------------------------------------\nmd5_b_f_stat |        494    7.522731    5.144063   .0544148   28.23535\n  md5_b_beta |        494    .7696057    1.443986  -6.293723   8.951643\nmd5_b_beta~e |        494     1.69479    1.866909    .530196    18.3837"
  },
  {
    "objectID": "09iv.html#iv-strength-bias-distribution",
    "href": "09iv.html#iv-strength-bias-distribution",
    "title": "Instrumental Variables",
    "section": "IV Strength: Bias distribution",
    "text": "IV Strength: Bias distribution\n\n\nCode\nuse resources/simiv.dta, clear\nforvalues i = 1/5 {\n  qui:sum md`i'_b_beta\n  gen new`i'=(md`i'_b_beta-1)/r(sd)\n}\nset scheme white2\ncolor_style tableau\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new1, name(m1, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new2, name(m2, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new3, name(m3, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new4, name(m4, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new5, name(m5, replace) , legend(off)\ngraph combine m1 m2 m3 m4 m5, col(3) xcommon ycommon\ngraph export resources/cmb.png, width(1500)  replace"
  },
  {
    "objectID": "09iv.html#iv-strength-solution-weakiv",
    "href": "09iv.html#iv-strength-solution-weakiv",
    "title": "Instrumental Variables",
    "section": "IV Strength Solution: weakiv",
    "text": "IV Strength Solution: weakiv\n\nWeak IV’s are a problem in the sense that it may induce bias on the estimated coefficients, but also that it may affect how Standard Errors are estimated.\n\nThe distribution of the Statistic is no longer normal\n\nOne solution, in this case, is at least adjusting SE and CI So they better reflect the problem.\nIn Stata, this can be done with weakiv (ssc install weakiv)\nAt the end, however, if you weak instruments, you may be able to correct of potential biases, but you may need to get more data, or better instruments"
  },
  {
    "objectID": "09iv.html#late-local-average-treatement-effect",
    "href": "09iv.html#late-local-average-treatement-effect",
    "title": "Instrumental Variables",
    "section": "LATE: Local Average Treatement Effect",
    "text": "LATE: Local Average Treatement Effect\nUp to this point, we imposed the assumption that TE were homogenous. Thus, IV could identify Treatment effects for everyone. (Average Treatment effect)\nHowever, not everyone may be affected by the instrument, only by the compliers.\nTwo ways of thinking about it:\n\nNot everybody is affected by the instrument. (you have the always and never takers)\nthe instrument was never suppoused to affect certain groups!\n\nSo, IV will identify TE for the compliers only.\nBecause of this, using different instruments may actually identify different effects, based on which population was affected.\nOverid tests may fail in this case."
  },
  {
    "objectID": "09iv.html#simulation-example",
    "href": "09iv.html#simulation-example",
    "title": "Instrumental Variables",
    "section": "Simulation Example:",
    "text": "Simulation Example:\n\n\nCode\nclear\nset obs 10000\ngen sex = rnormal()&gt;0\ngen z1 = rnormal()&gt;0\ngen z2 = rnormal()&gt;0\ngen e_1 =rnormal()\ngen e_2 =rnormal()\ngen e_3 =rnormal()\ngen D =(z1*(sex==0) + z2*(sex==1) + (e_1 + e_2)*.5)&gt;0\ngen Ds =(  (e_1 + e_2)*.5)&gt;0\ngen y = 1 + D*(sex==0) +2*D*(sex==1)+e_3+e_2\n\n\nNumber of observations (_N) was 0, now 10,000.\n\n\n\n\nCode\n%%echo\nivregress 2sls y (D=z1)\nivregress 2sls y (D=z2)\nivregress 2sls y (D=z1 z2)\n\n\n\n. ivregress 2sls y (D=z1)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =      18.40\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.1857\n                                                  Root MSE        =     1.6184\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   .6942855   .1618412     4.29   0.000     .3770826    1.011488\n       _cons |   1.584322    .115817    13.68   0.000     1.357325     1.81132\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1\n\n. ivregress 2sls y (D=z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     180.01\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.3621\n                                                  Root MSE        =     1.4325\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.914025   .1426572    13.42   0.000     1.634422    2.193628\n       _cons |   .7200152   .1020968     7.05   0.000     .5199092    .9201213\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z2\n\n. ivregress 2sls y (D=z1 z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     152.68\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2981\n                                                  Root MSE        =     1.5026\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.306657    .105746    12.36   0.000     1.099399    1.513915\n       _cons |   1.150396   .0764232    15.05   0.000     1.000609    1.300183\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1 z2\n\n."
  },
  {
    "objectID": "09iv.html#canonical-designs",
    "href": "09iv.html#canonical-designs",
    "title": "Instrumental Variables",
    "section": "Canonical Designs",
    "text": "Canonical Designs\n\nThe general message about using IV’s is, and has always been, that they are hard to come by.\nApplied research spends a quite good amount of time explaining why a particular instrument IS valid. (exogenous and relevant)\nRelevance is generally easy to test, but exogeneity is difficult. Little can be done other than relying in other papers, and circumstances.\nThere are also those “clever” IVs, that tend to be case specific\n\nScott Cunningham talks about Instruments being “weird”, because you wouldnt expect them to be in the context of the research\n\nThere are, however, some designs that are used quite often, because they apply to different circumstances."
  },
  {
    "objectID": "09iv.html#cd-lotteries",
    "href": "09iv.html#cd-lotteries",
    "title": "Instrumental Variables",
    "section": "CD: Lotteries",
    "text": "CD: Lotteries\n\nIn RCT, Lotteries are commonly used to decide who gets or doesnt get treatment among participants. Once treatment is assigned, however, not everyone will effectively taking up the treatment.\n\nFurthermore, some people may still end up being effectively treated because of other factors.\n\nThis is a case of imperfect compliance.\nIn cases like this, the lottery itself (which is randomized) can be used as instrument to identify the effect of being effectively treated.\n\nExamples:\n\nVietnam Draft Lottery\nOregon Medicaid Expansion Lottery"
  },
  {
    "objectID": "09iv.html#cd-judge-fixed-effects",
    "href": "09iv.html#cd-judge-fixed-effects",
    "title": "Instrumental Variables",
    "section": "CD: Judge Fixed Effects",
    "text": "CD: Judge Fixed Effects\nThis design is also partially based on a kind of randomized assigment.\n\nIndividuals are “allocated” to work, or be judge, under different officers “judges”, at random.\nJudges are consistent among each other, with only difference being the severity of the judgment.\nThen Judge fixed effect can be used as an instrument on the judgment (treatment), and the final impact on the outcome of interest.\n\nThe idea here is that “judgment-severity” varies by judge. This difference in taste creates exogenous variation on some treatment, which is analyzed on some treatment.\nExample:\n\nTeachers Grading? Driving test officers? Performance tests?"
  },
  {
    "objectID": "09iv.html#cd-shift-share-bartik-instrument",
    "href": "09iv.html#cd-shift-share-bartik-instrument",
    "title": "Instrumental Variables",
    "section": "CD: Shift-Share Bartik Instrument",
    "text": "CD: Shift-Share Bartik Instrument\nOriginally used in a study of regional labor market effects, this kind of instruments have also been used widely in other areas, such as imigration and trade.\nThe instrument was developed to analyze how changes in economic growth would affect market outcomes. (reverse Causality)\nTo do this, Bartik (1991) suggests, that it could be possible to create an instrument, making use of only exogenous variations, to first predict Potential local growth.\n\nEstimate industry shares by local region, based on some Ex ante information.\nEstimate national growth by industry (which should be exogenous to local growth)\nEstimate Potential Local growth using Shares x growth\n\nThis last one should represent the instrument to be used on actual local growth\nThis instrument depends strongly on the assumption that Shares are exogenous, and states are small compare to the national experience."
  },
  {
    "objectID": "10matching.html#recap-potential-outcomes-and-identification",
    "href": "10matching.html#recap-potential-outcomes-and-identification",
    "title": "Matching and Re-weighting",
    "section": "Recap: Potential outcomes and Identification",
    "text": "Recap: Potential outcomes and Identification\nTo identify treatment effects one could just compare potential outcomes in two states:\n\nwith treatment\nwithout treatment\n\nMathematically, average treatment effects would be: \\[\nATE = E(Y_i(1)-Y_i(0))\n\\]\nthe problem: with real data, we are only able to see one outcome. The counter factual is not observed:\n\\[\nY_i = Y_i(1)*D + Y_i(0)*(1-D)\n\\]\nand simple differences may not capture ATE, because of selection bias and heterogeneity in effects."
  },
  {
    "objectID": "10matching.html#recap-gold-standard---rct",
    "href": "10matching.html#recap-gold-standard---rct",
    "title": "Matching and Re-weighting",
    "section": "Recap: Gold Standard - RCT",
    "text": "Recap: Gold Standard - RCT\nThe easiest, but most expensive, way to deal with the problem is using Randomized Control Trials.\nEffectively, you randomize Treatment, so that potential outcomes are independent of treatment:\n\\[\nY(1),Y(0) \\perp D\n\\]\nIn other words, the distribution of potential outcomes is the same for those treated or untreated units.\n\\[\n\\begin{aligned}\nE(Y,D=1)&=E(Y(1),D=1)=E(Y(1),D=0) \\\\\nE(Y,D=0)&=E(Y(0),D=1)=E(Y(0),D=0) \\\\\nATT&=E(Y,D=1) - E(Y,D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "10matching.html#when-unconditional-fails",
    "href": "10matching.html#when-unconditional-fails",
    "title": "Matching and Re-weighting",
    "section": "When unconditional fails",
    "text": "When unconditional fails\nMore often than not, specially if we didn’t construct the data, it would be impossible to find that unconditional independence assumption holds.\nFor example, treatment (say having health insurance) may vary by age, gender, race, location, etc.\nThis is similar to the selection bias: Outcomes across treated and untreated groups will be different because:\n\nComposition: Characteristics of people among the treated could be different than those among the untreated For example, they could be older, more educated, mostly men, etc.\nOther factors: There could be factors we cannot control for, that also affect outcomes."
  },
  {
    "objectID": "10matching.html#there-is-conditional",
    "href": "10matching.html#there-is-conditional",
    "title": "Matching and Re-weighting",
    "section": "There is conditional",
    "text": "There is conditional\nWhen unconditional independence assumption fails, we can call on Conditional independence assumption:\n\\[\nY(1),Y(0) \\perp D | X\n\\]\nIn other words, If we can look into specific groups (given \\(X\\)), it may be possible to impose the Independence assumption.\nThis relaxes the independence condition, but assumes selection is due to observable characteristics only. (it still needs to be as good as randomized given \\(X\\))\nImplications:\n\\[\n\\begin{aligned}\nE(Y|D=1,X) =E(Y(1)|D=1,X)=E(Y(1)|D=0,X)  \\\\\nE(Y|D=0,X) =E(Y(0)|D=1,X)=E(Y(0)|D=0,X)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "10matching.html#intuition",
    "href": "10matching.html#intuition",
    "title": "Matching and Re-weighting",
    "section": "Intuition",
    "text": "Intuition\nMatching is a methodology that falls within quasi-experimental designs. You cannot or could not decide the assignment rules, so now are using data as given.\nThe idea is to construct an artificial control and use it as a counter-factual, so that both treated and control groups “look similar” in terms of observables.\nOnce a group of synthetic controls has been constructed, treatment effects can be calculated for the whole population:\n\\[\n\\begin{aligned}\nATE(X) &= E(Y|D=1,X) -E(Y|D=0,X) \\\\\nATE &= \\int ATE(X) dFx\n\\end{aligned}\n\\]\nHow can we do this?\nwe just need to find observational twins!"
  },
  {
    "objectID": "10matching.html#matching-twins",
    "href": "10matching.html#matching-twins",
    "title": "Matching and Re-weighting",
    "section": "Matching Twins",
    "text": "Matching Twins\n\nMatching on Observables"
  },
  {
    "objectID": "10matching.html#subclassification-or-stratification",
    "href": "10matching.html#subclassification-or-stratification",
    "title": "Matching and Re-weighting",
    "section": "Subclassification or stratification",
    "text": "Subclassification or stratification\nConsider the following dataset:\n\n\nCode\nfrause titanic, clear\nexpand freq\ndrop if freq==0\ngen class1=class==1\ntab survived class1 , nofreq col\n\n\n(Data downloaded from R base)\n(8 zero counts ignored; observations not deleted)\n(2,177 observations created)\n(8 observations deleted)\n\n           |        class1\n  Survived |         0          1 |     Total\n-----------+----------------------+----------\n        No |     72.92      37.54 |     67.70 \n       Yes |     27.08      62.46 |     32.30 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nIf we assume full Independence assumption we would believe that being in first class increased chance of survival in 35.4%. but is that the case?\nWhat if the composition of individuals differs across classes (women and children)\n\n\nCode\ntab age class1, nofreq col\ntab sex class1, nofreq col\n\n\n\n           |        class1\n       Age |         0          1 |     Total\n-----------+----------------------+----------\n     Child |      5.49       1.85 |      4.95 \n     Adult |     94.51      98.15 |     95.05 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n           |        class1\n       Sex |         0          1 |     Total\n-----------+----------------------+----------\n      Male |     82.68      55.38 |     78.65 \n    Female |     17.32      44.62 |     21.35 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nThere were fewer children, but more women in first class. Perhaps that explains the difference in survival rates"
  },
  {
    "objectID": "10matching.html#section",
    "href": "10matching.html#section",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A better approach would be to look into the survival probabilities stratifying the data:\n\n\nCode\ngen surv=survived==2\nbysort age sex class1:egen sr_mean=mean(survived==2)\ntable (age sex) (class1), stat(mean surv) nototal\n\n\n\n-----------------------------------\n             |         class1      \n             |         0          1\n-------------+---------------------\nAge          |                     \n  Child      |                     \n    Sex      |                     \n      Male   |  .4067797          1\n      Female |  .6136364          1\n  Adult      |                     \n    Sex      |                     \n      Male   |  .1883378   .3257143\n      Female |  .6263345   .9722222\n-----------------------------------\n\n\nSo even within each group, the survival probability is larger in first class. What about Average?\n\n\nCode\nbysort age sex:egen sr_mean_class1=max(sr_mean*(class1==1))\nbysort age sex:egen sr_mean_class0=max(sr_mean*(class1==0))\ngen teff = sr_mean_class1-sr_mean_class0\nsum teff if class1==1 // ATT\nsum teff if class1==0 // ATU\nsum teff  // ATE\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |        325    .2375421    .1125033   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,876    .1887847    .1089261   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      2,201    .1959842    .1107948   .1373765   .5932204"
  },
  {
    "objectID": "10matching.html#what-did-we-do",
    "href": "10matching.html#what-did-we-do",
    "title": "Matching and Re-weighting",
    "section": "What did we do?",
    "text": "What did we do?\nThe procedure above is a simple stratification approach, aka matching, to analyze the true impact of the treatment (being a 1st class passenger).\n\nStratified the sample in groups by age and gender.\n\nIdentify the shares of each group by class1\n\nPredict probability of survival per strata and class1\nObtain the Strata level Effects\nAggregate as needed.\n\nHere, we could estimate ATE, ATT or ATU!\n\n\nWhere could things go wrong?"
  },
  {
    "objectID": "10matching.html#overlapping",
    "href": "10matching.html#overlapping",
    "title": "Matching and Re-weighting",
    "section": "Overlapping",
    "text": "Overlapping\nThe procedure describe above works well whenever there is data overlapping.\n\nFor every combination of X, you see data on the control and treated group \\(0&lt;P(D|X)&lt;1\\)\n\nWhen this fails, you wont be able to estimate ATE’s, although ATT’s or ATU’s might still be possible:\n\nfor ATT: \\(P(D|X)&lt;1\\)\nfor ATU: \\(0&lt;P(D|X)\\)\n\nFor example:\n\n\nCode\nfrause hhprice, clear\nkeep price rooms type_h\ntab rooms type_h\n\n\n\n           |    =0 if house, =1\n Number of |       TownHouse\n     rooms |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        37         72 |       109 \n         2 |     1,134        751 |     1,885 \n         3 |     4,634        648 |     5,282 \n         4 |     2,465        115 |     2,580 \n         5 |       465          2 |       467 \n         6 |        46          0 |        46 \n         7 |         7          0 |         7 \n-----------+----------------------+----------\n     Total |     8,788      1,588 |    10,376 \n\n\nWould not be able to estimate ATE nor ATU. Only ATT for townhouses."
  },
  {
    "objectID": "10matching.html#curse-of-dimensionality",
    "href": "10matching.html#curse-of-dimensionality",
    "title": "Matching and Re-weighting",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\nThere is a second problem in terms of stratification. How would we deal with Multiple dimensions? Would it be possible to find “twins” for every observation?\nThe answer is, probably no. Too many groups to track, to many micro cells to make use of:\n\n\nCode\nfrause oaxaca, clear\ndrop if lnwage==.\negen strata=group(educ isco)\nbysort strata:egen flag=mean(female)\nlist educ isco female if (flag==0 | flag==1) & educ == 10, sep(0)\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n      +----------------------+\n      | educ   isco   female |\n      |----------------------|\n 158. |   10      1        0 |\n 159. |   10      1        0 |\n 197. |   10      7        0 |\n 198. |   10      7        0 |\n 199. |   10      9        1 |\n 200. |   10      9        1 |\n      +----------------------+"
  },
  {
    "objectID": "10matching.html#alternative-matching-as-a-weighted",
    "href": "10matching.html#alternative-matching-as-a-weighted",
    "title": "Matching and Re-weighting",
    "section": "Alternative: Matching as a weighted",
    "text": "Alternative: Matching as a weighted\nThe problem of curse of dimensional states that as the number of desired characteristics to match increase, fewer “twins” will be available in the data. At the end…no one will be like you!\nThe alternative, is to look into People that are sufficiently close so they can be used for matching.\n\\[\n\\begin{aligned}\nATT_i &= Y_i -  \\sum_{j \\in C} w(x_j,x_i) Y_j \\\\\nATT   &= \\frac{1}{N_T}\\sum(ATT_i)  \\\\\nATT   &=E(Y|D=1) - E_i\\left( \\sum_{j \\in C} w(x_j,x_i) Y_j  \\Big| D=0 \\right)\n\\end{aligned}\n\\]\nDepending how \\(w(.)\\) is defined, we would be facing different kinds of matching estimators."
  },
  {
    "objectID": "10matching.html#matching-on-covariates",
    "href": "10matching.html#matching-on-covariates",
    "title": "Matching and Re-weighting",
    "section": "Matching on covariates",
    "text": "Matching on covariates\nThe first decision to take is whether one should find matches based on covariates, or based on scores (propensity scores).\nUsing covariates implies that will aim to find the closest “twin” possible, based on multiple dimensions: \\[\n\\begin{aligned}\nEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'(x_i-x_j)} \\\\\nWEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'W (x_i-x_j)} \\\\\nMaha =d(x_i,x_j) &=\\sqrt{(x_i-x_j)'S^{-1}(x_i-x_j)}\n\\end{aligned}\n\\]\nDistance measures are used to identify the closest matches to a given observation, and thus the weight assigned to that observation.\nHas the advantage of looking at individuals who are indeed close to each other, but becomes more difficult as the dimensionality of X’s increase. (you will not find close matches)"
  },
  {
    "objectID": "10matching.html#matching-on-scores",
    "href": "10matching.html#matching-on-scores",
    "title": "Matching and Re-weighting",
    "section": "Matching on Scores",
    "text": "Matching on Scores\nA second approach is to match individuals based on some summary index that condenses the information in \\(X\\) into a single scalar \\(h(x)\\), reducing the dimensionality problem fron K to 1.\nFew candidates:\n\nPropensity Score: \\(P(D|X)\\) based on a logit/probit/binomial model. Most common approach!\nPredicted Mean: \\(X\\beta\\) if there is information on outcome to be predicted\nPCA: Using Principal components to reduce dimensionality before Matching\n\nSince there is only 1 dimension to consider, multiple distance measures are possible:\n\nnearest neighbors, kernel weight matching, radious matching.\n\nBut one has to be careful with the approach. King and Nielsen (2019) Argue about the risks of PSM"
  },
  {
    "objectID": "10matching.html#vs-k-matching-with-and-without-replacement",
    "href": "10matching.html#vs-k-matching-with-and-without-replacement",
    "title": "Matching and Re-weighting",
    "section": "1 vs K matching; With and without replacement",
    "text": "1 vs K matching; With and without replacement\nTwo additional questions remain regarding matching. How many “twins” to use, and if twins will be obtained with/without replacement.\n\nFewer matches reduce bias (choosing only the closest observation), but increase variance.\nMore matches increase bias, but reduce variance. (because of less optimal matches)\nwith replacement: control units may be used more than once. This will improve matching quality reducing bias. But by using the same units multiple times, it will increase variance.\nwithout replacement: Control units are used once, potentially reducing matching quality, but reducing variance. It will be order dependent.\n\nsee Caliendo and Kopeing (2008)"
  },
  {
    "objectID": "10matching.html#what-about-se-and-statistical-inference",
    "href": "10matching.html#what-about-se-and-statistical-inference",
    "title": "Matching and Re-weighting",
    "section": "What about SE? and Statistical inference?",
    "text": "What about SE? and Statistical inference?\nWell….this is one of the few cases where Bootstrapping WON’T work!\nStandard errors are more cumbersome. So we will just rely on software results"
  },
  {
    "objectID": "10matching.html#other-considerations",
    "href": "10matching.html#other-considerations",
    "title": "Matching and Re-weighting",
    "section": "Other considerations",
    "text": "Other considerations\nOnce you have chosen your matching method, find your “statistical twins”, and estimate your differences you are done! (or are you)\nNot yet…common practice: Evaluate the balance of your data\n\nMatching aims to reduce or eliminate differences in characteristics between treatment and control units. Thus, one should evaluate the differences (before and after match) of your characteristis\n\n\nCheck for overlapping condition.\n\n\neither variable by variable or with pscore\n\n\nAssess Matching Quality: Have differences across groups vanished?\n\n\nCheck Standardized differences \\(\\frac{\\mu_1 - \\mu_2}{\\sqrt{0.5*(V_1 + V_2)}}\\)\nt-tests\nPR2 of regression with matched data"
  },
  {
    "objectID": "10matching.html#implementation",
    "href": "10matching.html#implementation",
    "title": "Matching and Re-weighting",
    "section": "Implementation",
    "text": "Implementation\nIn Stata, there are at least two approaches that can be used for matching:\n\npsmatch2 (from ssc)\nteffects (Official Stata command)\n\nWe will use this to answer a simple question:\n\nWhat is the impact of Traing Jobs on Earnings?"
  },
  {
    "objectID": "10matching.html#example",
    "href": "10matching.html#example",
    "title": "Matching and Re-weighting",
    "section": "Example",
    "text": "Example\nThis file contains information on experimental and observed data for the analysis of training on earnings program:\n\n\nCode\nuse https://friosavila.github.io/playingwithstata/drdid/lalonde.dta, clear\nkeep if year==1978 \ndrop if dwincl==0\nlabel define sample 1 \"exper\"  2 \"CPS\" 3 \"PSID\"\nlabel values sample sample\ntab sample treated,m\n\n\n(19,204 observations deleted)\n(277 observations deleted)\n\n           |             treated\n    sample |         0          1          . |     Total\n-----------+---------------------------------+----------\n     exper |       260        185          0 |       445 \n       CPS |         0          0     15,992 |    15,992 \n      PSID |         0          0      2,490 |     2,490 \n-----------+---------------------------------+----------\n     Total |       260        185     18,482 |    18,927 \n\n\nFirst Experimental design - RCT\n\n\nCode\nreg re treated\ntabstat age educ black married nodegree , by(treated)\nlogit treated age educ black hisp married nodegree \n\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |   348013183         1   348013183   Prob &gt; F        =    0.0048\n    Residual |  1.9178e+10       443  43290369.3   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  1.9526e+10       444  43976681.9   Root MSE        =    6579.5\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |   1794.342   632.8534     2.84   0.005     550.5745     3038.11\n       _cons |   4554.801   408.0459    11.16   0.000     3752.855    5356.747\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black   married  nodegree\n---------+--------------------------------------------------\n       0 |  25.05385  10.08846  .8269231  .1538462  .8346154\n       1 |  25.81622  10.34595  .8432432  .1891892  .7081081\n---------+--------------------------------------------------\n   Total |  25.37079  10.19551  .8337079  .1685393  .7820225\n------------------------------------------------------------\n\nIteration 0:  Log likelihood =     -302.1  \nIteration 1:  Log likelihood = -294.72908  \nIteration 2:  Log likelihood = -294.71464  \nIteration 3:  Log likelihood = -294.71464  \n\nLogistic regression                                     Number of obs =    445\n                                                        LR chi2(6)    =  14.77\n                                                        Prob &gt; chi2   = 0.0221\nLog likelihood = -294.71464                             Pseudo R2     = 0.0244\n\n------------------------------------------------------------------------------\n     treated | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0059171   .0142668     0.41   0.678    -.0220452    .0338794\n        educ |  -.0639597    .071354    -0.90   0.370     -.203811    .0758916\n       black |  -.2543689   .3639735    -0.70   0.485    -.9677438    .4590061\n        hisp |  -.8291587   .5042305    -1.64   0.100    -1.817432     .159115\n     married |   .2342415   .2661824     0.88   0.379    -.2874665    .7559495\n    nodegree |  -.8385524   .3093833    -2.71   0.007    -1.444933   -.2321722\n       _cons |   1.053028   1.047384     1.01   0.315    -.9998064    3.105862\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "10matching.html#section-1",
    "href": "10matching.html#section-1",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Then using PScore Matching CPS\n\n\nCode\nkeep if treated == 1 | sample ==2\nreplace treated=0 if treated==.\nreg re treated\ntabstat age educ black hisp married nodegree , by(treated)\n\n\n(2,750 observations deleted)\n(15,992 real changes made)\n\n      Source |       SS           df       MS      Number of obs   =    16,177\n-------------+----------------------------------   F(1, 16175)     =    142.43\n       Model |  1.3206e+10         1  1.3206e+10   Prob &gt; F        =    0.0000\n    Residual |  1.4997e+12    16,175  92717515.8   R-squared       =    0.0087\n-------------+----------------------------------   Adj R-squared   =    0.0087\n       Total |  1.5129e+12    16,176  93528158.4   Root MSE        =      9629\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -8497.516   712.0207   -11.93   0.000    -9893.156   -7101.877\n       _cons |   14846.66   76.14292   194.98   0.000     14697.41    14995.91\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  33.22524  12.02751  .0735368   .072036  .7117309  .2958354\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  33.14051  12.00828  .0823391  .0718922  .7057551  .3005502\n----------------------------------------------------------------------"
  },
  {
    "objectID": "10matching.html#section-2",
    "href": "10matching.html#section-2",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "We need to do trimming\n\n\nCode\nbysort educ black hisp married:egen n11=sum(treated==1)\nbysort age  black hisp married:egen n22=sum(treated==1)\ndrop if n11==0 | n22 ==0\ntabstat age educ black hisp married nodegree , by(treated)\nreg re treated\n\n\n(13,536 observations deleted)\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     2,641\n-------------+----------------------------------   F(1, 2639)      =     73.89\n       Model |  5.7607e+09         1  5.7607e+09   Prob &gt; F        =    0.0000\n    Residual |  2.0575e+11     2,639  77964783.1   R-squared       =    0.0272\n-------------+----------------------------------   Adj R-squared   =    0.0269\n       Total |  2.1151e+11     2,640  80117339.3   Root MSE        =    8829.8\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -5786.584   673.1834    -8.60   0.000    -7106.605   -4466.564\n       _cons |   12135.73   178.1702    68.11   0.000     11786.36     12485.1\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "10matching.html#section-3",
    "href": "10matching.html#section-3",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Lets do some matching\n\n\nCode\nteffects nnmatch (re age educ black   married nodegree  ) (treated)\ntebalance summarize\nteffects nnmatch (re age educ black   married nodegree  ) (treated), nn(2)\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  )\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  ) ,  nn(2)\ntebalance summarize\n\n\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -3685.665   1188.666    -3.10   0.002    -6015.407   -1355.923\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    -.015417      1.305844   .8410946\n           educ |  -.7684118   -.0812288      1.881909   .8598207\n          black |   1.473105           0      .7039609          1\n        married |  -.3351313   -.0008087      .6923501    .999393\n       nodegree |   1.010393           0      1.088086          1\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -5166.888   1107.653    -4.66   0.000    -7337.848   -2995.929\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346   -.0209048      1.305844   .7345997\n           educ |  -.7684118   -.0385284      1.881909   .8978301\n          black |   1.473105    .0074673      .7039609   1.006716\n        married |  -.3351313    -.004586      .6923501   .9965432\n       nodegree |   1.010393    .0016705      1.088086   1.001557\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4278.549   1135.847    -3.77   0.000    -6504.768   -2052.331\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    .0014058      1.305844   .9313458\n           educ |  -.7684118   -.1308249      1.881909   .9665937\n          black |   1.473105   -.0926638      .7039609     .90999\n        married |  -.3351313   -.0973289      .6923501   .9197524\n       nodegree |   1.010393    .0821105      1.088086    1.07103\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4380.078   1158.019    -3.78   0.000    -6649.754   -2110.403\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346     -.06133      1.305844   .8834346\n           educ |  -.7684118   -.1321518      1.881909   1.021302\n          black |   1.473105   -.0698339      .7039609    .933348\n        married |  -.3351313   -.0414439      .6923501   .9674741\n       nodegree |   1.010393    .0939209      1.088086   1.080951\n-----------------------------------------------------------------"
  },
  {
    "objectID": "10matching.html#section-4",
    "href": "10matching.html#section-4",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A missing variable? Earnings in previous year. May capture information of Need to do treatment (selection)\n\n\nCode\ntabstat age educ black hisp married nodegree re74, by(treated)\ngen dre = re-re74\nteffects nnmatch (dre age educ black   married nodegree  ) (treated)\n\nteffects nnmatch (dre age educ black   married nodegree  ) (treated), nn(2)\n\nteffects psmatch (dre) (treated age educ black   married nodegree  )\n\nteffects psmatch (dre) (treated age educ black   married nodegree  ) ,  nn(2)\n\n\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n treated |      re74\n---------+----------\n       0 |  9347.406\n       1 |  2095.574\n---------+----------\n   Total |  8839.421\n--------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2616.653   1803.172     1.45   0.147    -917.4997    6150.806\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   730.2925    1674.91     0.44   0.663     -2552.47    4013.055\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2162.311    1740.12     1.24   0.214    -1248.262    5572.884\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1833.03   1739.496     1.05   0.292    -1576.318    5242.379\n------------------------------------------------------------------------------\n\n\nIn this case, Matching alone could not get the right answer. Who were the most likely to “go to the training?”\nSo instead we change the question: How much the change in earnings compare across groups."
  },
  {
    "objectID": "10matching.html#wait-what-about-reweighting",
    "href": "10matching.html#wait-what-about-reweighting",
    "title": "Matching and Re-weighting",
    "section": "Wait: What about Reweighting?",
    "text": "Wait: What about Reweighting?\nAn alternative method to Matching is to do Re-weighting.\nWe have seen this!\nYour control group has a distribution \\(g(x)\\) and your treatment \\(f(x)\\). We can use some weighting factors \\(h(x)\\) that reshapes \\(g(x)\\rightarrow \\hat f(x)\\).\nHow? Using Propensity scores\nWhy does it work? Just as matching, your goal is to compare distributions of outcomes, forcing differences in observed characteristics to be the same.\nIPW, does this by reweighting the distribution! (rather than matching)"
  },
  {
    "objectID": "10matching.html#inverse-probability-weightingipw",
    "href": "10matching.html#inverse-probability-weightingipw",
    "title": "Matching and Re-weighting",
    "section": "Inverse Probability Weighting:IPW",
    "text": "Inverse Probability Weighting:IPW\ns1: Estimate Pscore\n\\[\np(D=1|X)=F(X\\beta)\n\\] S2: Estimate IPW\nFor ATT: \\(W(D=1,x)=1 \\ \\& \\ W(D=0,X)=\\frac{\\hat p(x)}{1-\\hat p(x)}\\)\nFor ATU: \\(W(D=0,x)=1 \\ \\& \\ W(D=1,X)=\\frac{1-\\hat p(x)}{\\hat p(x)}\\)\nFor ATE: \\(W(D=0,x)=\\frac{1}{1-\\hat p(x)} \\ \\& \\ W(D=1,X)=\\frac{1}{\\hat p(x)}\\)\ns3: Estimate Treatment effect:\n\\[\nTE = \\sum_{i \\in D=1} w_i^s(1) Y_i - \\sum_{i \\in D=0} w_i^s(0) Y_i\n\\]"
  },
  {
    "objectID": "10matching.html#even-better-go-dr",
    "href": "10matching.html#even-better-go-dr",
    "title": "Matching and Re-weighting",
    "section": "Even Better: Go DR!",
    "text": "Even Better: Go DR!\nAn interesting advantage of IPW approach is that you can gain efficiency using Doubly Robust Methods. Namely, instead of comparing outcomes directly, you could compare predicted outcomes!\n\\[\n\\begin{aligned}\nATT &= \\frac{1}{N_t}\\sum(Y_1-X'\\hat\\beta_w^0) \\\\\nATU &= \\frac{1}{N_c}\\sum(X'\\hat\\beta_w^1-Y_0) \\\\\nATE &= \\frac{1}{N}\\sum(X'\\hat\\beta_w^1-X'\\hat\\beta_w^0)\n\\end{aligned}\n\\] where \\(\\hat \\beta^k_w\\) can be modeled using weighted least squares"
  },
  {
    "objectID": "10matching.html#comparing-to-matching",
    "href": "10matching.html#comparing-to-matching",
    "title": "Matching and Re-weighting",
    "section": "Comparing to Matching",
    "text": "Comparing to Matching\nteffects ipw (re) (treated age educ black   married nodegree) , iter(3) nolog\nteffects ipwra (re age educ black   married nodegree) (treated age educ black married nodegree), iter(3) nolog\nteffects ipw (dre) (treated age educ black   married nodegree), iter(3) nolog\nteffects ipwra (dre age educ black   married nodegree) (treated age educ black   married nodegree), iter(3) nolog\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4833.352   1088.667    -4.44   0.000    -6967.101   -2699.603\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11979.19   179.1903    66.85   0.000     11627.99     12330.4\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   -4835.38   1012.598    -4.78   0.000    -6820.036   -2850.724\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11976.52   179.0958    66.87   0.000     11625.49    12327.54\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1475.71   1792.427     0.82   0.410    -2037.382    4988.802\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2746.475   161.2845    17.03   0.000     2430.363    3062.587\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   1286.605   1516.493     0.85   0.396    -1685.666    4258.875\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2754.756   161.4406    17.06   0.000     2438.338    3071.173\n------------------------------------------------------------------------------\nWarning: Convergence not achieved."
  },
  {
    "objectID": "11rdd.html#re-cap-potential-outcome-model",
    "href": "11rdd.html#re-cap-potential-outcome-model",
    "title": "Regression Discontinuity Design",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nThis works because all observed and unobserved individual characteristics are kept fixed, except for the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, and that differences between the two states are explained only by the treatment."
  },
  {
    "objectID": "11rdd.html#the-problem",
    "href": "11rdd.html#the-problem",
    "title": "Regression Discontinuity Design",
    "section": "The Problem",
    "text": "The Problem\nWe do not observe both ALL States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nThis means finding people are very similar to the ones treated, so they can be used as the examples of the “what if” question.\nBut there is a problem. Even in the best scenarios, we can never be asure about how to control for unobservables…or can we?"
  },
  {
    "objectID": "11rdd.html#section",
    "href": "11rdd.html#section",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "You can always RCT But it can be expensive\nYou can IV the problem but its hard to justify\nYou can add FE, but you have time varying errors\n\nThen what?\n\nYou could RDD the problem (if you have the right data!)"
  },
  {
    "objectID": "11rdd.html#what-is-rdd",
    "href": "11rdd.html#what-is-rdd",
    "title": "Regression Discontinuity Design",
    "section": "What is RDD?",
    "text": "What is RDD?\nRDD or Regression Discontinuity design is methodology that is known for its clean identification and with a relatively easy visualization tool to understand the identification, and solve the problem of unobserved distributions. (see here for a recent paper on how to make graphs on this).\nIn fact, the treatment Status has a very clear Rule!\nConsider the following problem:\n\nYou want to study the role of college on earnings.\nYou have data on people who are applying to go to school. They all take some standardized tests. Their grade will determine if they get into College or not.\nPeople with high skills will get a higher grades in the GRE, go to college, and probably get higher salaries.\nBut, there is a problem. How can you figure out if wages are due to College or skill?"
  },
  {
    "objectID": "11rdd.html#possible-solution",
    "href": "11rdd.html#possible-solution",
    "title": "Regression Discontinuity Design",
    "section": "Possible Solution",
    "text": "Possible Solution\nSay that we actually have access to the grades, which range from 100 to 200. And assume that you say, every one with grades higher than 170 will go to college.\nCan you estimate the effect now?\n\nYou can’t compare people with more than 170 to those with less than 170. Because skill or ability will be different across groups.\nHowever, what if you compare individuals with 170-172 vs 167-169?\n\nThese individuals are so close togeter they problably have very similar characteristics as well!\nyou have a Localized randomization.\n\n\nIn this case, your analysis is those individuals just above the thresholds to those just below (counterfactual)\nUnless you think grades near the threshold are as good as random, then you have a design to identify treatment effects!"
  },
  {
    "objectID": "11rdd.html#rdd-how-it-works.",
    "href": "11rdd.html#rdd-how-it-works.",
    "title": "Regression Discontinuity Design",
    "section": "RDD: How it works.",
    "text": "RDD: How it works.\nSelection and Index:\n\nThe first thing you need to see if you can use and RDD is to see if you have access to a variable that “ranks” units.\n\n\nThis variable should be smooth and preferibly continuous.\n\nage, distance from boarder, test score, poverty index\n\n\n\nAssignment into treatment is a function of this index only, with a clear threshold for the index to have an impact the treatment.\n\n\nThose under the Threshold are not treated. Those above are.\nThis is called a Sharp RDD design."
  },
  {
    "objectID": "11rdd.html#section-1",
    "href": "11rdd.html#section-1",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "The threshold should be unique to the Treatment of interest (nothing else happens around that point)\n\n\nIn the College Case, we assume 170 triggers acceptance to School. But if it also triggers Scholarships??\n\n\nPerhaps the Most important: The score cannot be manipulated\n\nOnly then we have true local randomization.\n\nYou want the potential outcomes to be smooth functions of Z. (so we do not mix treatment effects with Nonsmooth changes in outcomes)"
  },
  {
    "objectID": "11rdd.html#sharp-rdd",
    "href": "11rdd.html#sharp-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Sharp RDD",
    "text": "Sharp RDD\n\n\nCode\nclear\nset scheme white2\ncolor_style bay\nset seed 1\nqui:set obs 100\ngen e=rnormal()\ngen z=runiform()+e\ngen t = z&gt;0\ngen y = 1 + z + e + rnormal() + (z&gt;0)\ntwo line t z, sort title(\"Treatment\") ylabel(0 \"Not Treated\" 1 \"Treated\") name(m1, replace) ytitle(\"\")\ngraph export resources\\rdd1.png,  width(1000) height(1000) replace\ntwo scatter y z, sort title(\"Outcome\") pstyle(p1) || lfit y z, lw(1) pstyle(p2) ///\n    || lfit y z if z&lt;0, lw(0.5) || lfit y z if z&gt;0, lw(0.5) , legend(off) name(m2, replace)\ngraph export resources\\rdd2.png,  width(1000) height(1000)  replace"
  },
  {
    "objectID": "11rdd.html#how-it-works-p2",
    "href": "11rdd.html#how-it-works-p2",
    "title": "Regression Discontinuity Design",
    "section": "How it works : p2",
    "text": "How it works : p2\nRecall that in an RCT (or under randomization) treatment effects are estimated by comparing those treated and those not treated.\n\\[E(y|D=1)-E(y|D=0)\\]\nUnder SRDD, you can also think about the same experiment, except that we would need to compare individuals AT the theshold.\n\\[\\begin{aligned}\n\\lim_{z\\downarrow c} E(y|Z=z) &- \\lim_{z\\uparrow c} E(y|Z=z) \\\\\nE(y(1)|Z=c) &-E(y(0)|Z=c)\n\\end{aligned}\n\\]\n\nIn this case, the overlapping assumption is violated. So we need to attemp obtaining effects for groups AT the limit when \\(Z=c\\)."
  },
  {
    "objectID": "11rdd.html#estimation",
    "href": "11rdd.html#estimation",
    "title": "Regression Discontinuity Design",
    "section": "Estimation",
    "text": "Estimation\nThe most simple way to proceed is to estimate the model using a parametric approach (OLS)\n\\[y = a_0 + \\delta D_{z&gt;c} + f(z-c) + e\\]\nThe idea here is to identify a “jump” in the outcome (treatment effect) at the point where \\(z\\) crosses the threshold.\nBut to identify the jump only, we also need to model the trend observe before and after that threshold (\\(f(z-c)\\)), which can be modelled as flexible as possible. (this include interactions with the jump)\nAlternatively, we could use smaller bandwidths (nonparametric)"
  },
  {
    "objectID": "11rdd.html#example",
    "href": "11rdd.html#example",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\n qui: {\nclear\nset seed 1\nset obs 200\ngen e=rnormal()\ngen z=runiform()+e\nsum z\nreplace z=(z-r(mean))/r(sd)\ngen t = z&gt;0\ngen y = 1 + 0.5*z + e + rnormal() + (z&gt;0)\n\nqui:reg y t z \npredict yh1\nlocal b1:display %3.2f _b[t]\nqui:reg y t c.z##c.z  \npredict yh2\nlocal b2:display %3.2f _b[t]\nqui:reg y t c.z##c.z##c.z\npredict yh3\nlocal b3:display %3.2f _b[t]\nsort z\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh1 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh1 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh2 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh2 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh3 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh3 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m1, replace)"
  },
  {
    "objectID": "11rdd.html#example-1",
    "href": "11rdd.html#example-1",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui: {\nqui:reg y t c.z#t \npredict yh11\nlocal b1:display %3.2f _b[t]\nqui:reg y t (c.z##c.z)#t  \npredict yh21\nlocal b2:display %3.2f _b[t]\nqui:reg y t (c.z##c.z##c.z)#t\npredict yh31\nlocal b3:display %3.2f _b[t]\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh11 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh11 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh21 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh21 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh31 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh31 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m2, replace)"
  },
  {
    "objectID": "11rdd.html#fuzzy-rd-imperfect-compliance",
    "href": "11rdd.html#fuzzy-rd-imperfect-compliance",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD: Imperfect compliance",
    "text": "Fuzzy RD: Imperfect compliance\nWhile the Idea Scenario happens when there is perfect compliance (above the threshold you are treated), this doesnt happen all the time.\nIn the education example:\n\nSome people with low grades may be “legacy” or have “contacts” (or took a second exam later) and manage to go to college\nSome decided not to go, even after entering to college\n\nSounds Familiar? (Never takers vs always takers)\nWhen this happens, you can still do RDD, but you need more steps"
  },
  {
    "objectID": "11rdd.html#fuzzy-rd",
    "href": "11rdd.html#fuzzy-rd",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD",
    "text": "Fuzzy RD\n\nEstimate the impact of Discontinuity on Treatment\nEstimate the impact of Discontinuity on Outcome\nEstimate the ratio between (1) and (2)\n\nSounds Familiar?\n\nIts a kind of wald/IV estimator.\n\nThe instrument is the discontinuity\nThe the endogenous variable is the treament\n\n\nYou still need to estimate the effect as close to the Discontinuity as possible\nivregress may still do most of this for you"
  },
  {
    "objectID": "11rdd.html#example-2",
    "href": "11rdd.html#example-2",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nTreatment\n\n\n\n\n\n\n\nOutcome\n\n\n\n\n\nEffect:\n\nCode\nqui: gen dz = z&gt;0\nqui: reg y dz c.z##c.z#i.dz\nlocal b1 = _b[dz]\nqui: reg t dz c.z##c.z#i.dz\nlocal b2 = _b[dz]\n\ndisplay \"There is a \" %3.2f `b1' \" effect on the outcome\"\ndisplay \"and a \" %3.2f `b2' \" effect on the treatment\"\ndisplay \"which imply a LATE of \" %3.2f `=`b1'/`b2''\n\nThere is a 0.45 effect on the outcome and a 0.44 effect on the treatment which imply a LATE of 1.03"
  },
  {
    "objectID": "11rdd.html#things-to-consider",
    "href": "11rdd.html#things-to-consider",
    "title": "Regression Discontinuity Design",
    "section": "Things to consider",
    "text": "Things to consider\nTheoretical:\n\nYou need to identify “jumps” caused by a running variable. (depends on knowing how things works)\nThe potential outcomes have to be smooth functions of the running variable\n\nEmpirical:\n\nThe running variable shouldnt be manipulated. (random)\n\nImplies Assignment rules are not known, are exogenous, and there is no random heaping\n\nControls should be balanced around the threshold"
  },
  {
    "objectID": "11rdd.html#testing-empirical-assumptions",
    "href": "11rdd.html#testing-empirical-assumptions",
    "title": "Regression Discontinuity Design",
    "section": "Testing Empirical Assumptions",
    "text": "Testing Empirical Assumptions\nManipulation of running variable may cause a non-smooth density in the running variable:\n\nIf there is no manipulation, you may expect density round threshold to be smooth.\n\nIn Stata: ssc install rddensity. In r install.packages(c(‘rdd’,‘rddensity’))\n\n\n\n\nCode\nset linesize 100\nqui:ssc install  lpdensity, replace\nqui:ssc install  rddensity, replace\nrddensity z, c(0) plot\ngraph export resources\\frdd3.png, width(1000) replace"
  },
  {
    "objectID": "11rdd.html#section-2",
    "href": "11rdd.html#section-2",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "In cases of nonrandom heaping, it may be possible to avoid the problem by restricting the data.\nThis is an example of measurement error, when individuals may “round-up/down” answers. And may occure near threshold.\nThis does not necessarily mean there is manipulation.\nPossible Solution? Estimate RDD excluding observations around (excluding) threshold."
  },
  {
    "objectID": "11rdd.html#covariate-balance-and-placebo-tests",
    "href": "11rdd.html#covariate-balance-and-placebo-tests",
    "title": "Regression Discontinuity Design",
    "section": "Covariate balance and Placebo tests",
    "text": "Covariate balance and Placebo tests\n\nIf treatment is locally randomized, then covariates should not be affected by discontinuity.\nAlternatively, one could estimate effects on variables you know CANNOT be affected by the treatment\nOne could also implement a placebo test, checking the impact on a different threholds.\n\nNo effect should be observed on the outcome, (but some on the treatment)"
  },
  {
    "objectID": "11rdd.html#example-3",
    "href": "11rdd.html#example-3",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\nImpact of Scores on Scholarship recipiency\n\n\nCode\nuse resources\\fuzzy, clear\ncolor_style bay\nqui:rdplot d x1, graph_options(legend( pos(6)))"
  },
  {
    "objectID": "11rdd.html#manipulation-test",
    "href": "11rdd.html#manipulation-test",
    "title": "Regression Discontinuity Design",
    "section": "Manipulation test",
    "text": "Manipulation test\n\n\nCode\nqui:rddensity x1, plot"
  },
  {
    "objectID": "11rdd.html#intention-to-treat",
    "href": "11rdd.html#intention-to-treat",
    "title": "Regression Discontinuity Design",
    "section": "Intention to treat",
    "text": "Intention to treat\nImpact on Enrollment\n\n\nCode\nqui:rdplot y x1, graph_options(legend( pos(6)))"
  },
  {
    "objectID": "11rdd.html#estimation-of-the-effect",
    "href": "11rdd.html#estimation-of-the-effect",
    "title": "Regression Discontinuity Design",
    "section": "Estimation of the effect:",
    "text": "Estimation of the effect:\n\n\nCode\ngen dx1 = x1&gt;0\nivregress 2sls y (d = dx1) c.x1##c.x1#dx1 \n\n\n\nInstrumental variables 2SLS regression            Number of obs   =     23,132\n                                                  Wald chi2(5)    =    1645.56\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2845\n                                                  Root MSE        =     .39876\n\n-------------------------------------------------------------------------------\n            y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n            d |   .4432824   .0215111    20.61   0.000     .4011214    .4854434\n              |\n     dx1#c.x1 |\n           0  |   .0002949   .0019179     0.15   0.878    -.0034642    .0040539\n           1  |  -.0034238    .000779    -4.39   0.000    -.0049507   -.0018969\n              |\ndx1#c.x1#c.x1 |\n           0  |   .0000741   .0000703     1.05   0.292    -.0000637    .0002119\n           1  |   .0000616   .0000164     3.76   0.000     .0000295    .0000936\n              |\n        _cons |   .5103635    .010876    46.93   0.000     .4890469    .5316801\n-------------------------------------------------------------------------------\nEndogenous: d\nExogenous:  0b.dx1#c.x1 1.dx1#c.x1 0b.dx1#c.x1#c.x1 1.dx1#c.x1#c.x1 dx1"
  },
  {
    "objectID": "11rdd.html#some-sensitivity",
    "href": "11rdd.html#some-sensitivity",
    "title": "Regression Discontinuity Design",
    "section": "Some Sensitivity",
    "text": "Some Sensitivity\n\n\nCode\nqui:{   \nmatrix b1 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b1=b1\\[_b[d],_se[d]]\n}\n\nmatrix b2 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b2=b2\\[_b[d],_se[d]]\n}\n\nmatrix b3 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b3=b3\\[_b[d],_se[d]]\n}\n\nlbsvmat b1\nlbsvmat b2\nlbsvmat b3\nforvalues i = 1/3 {\n  gen ll`i'=b`i'1-b`i'2*1.96\n  gen ul`i'=b`i'1+b`i'2*1.96\n}\ngen z = _n-1 if b11!=.\nreplace z=. in 1\n\n}\ngen z1 = z + 0.25\ngen z2 = z + 0.5\ntwo (rspike ll1 ul1 z, lw(1) pstyle(p1) color(%50) ) (scatter b11 z, pstyle(p1) ) ///\n    (rspike ll2 ul2 z1, lw(1) pstyle(p2) color(%50) ) (scatter b21 z1, pstyle(p2) ) ///\n    (rspike ll3 ul3 z2, lw(1) pstyle(p3) color(%50) ) (scatter b31 z2, pstyle(p3) ), ///\n    legend(order(1 \"Linear\" 3 \"Quadratic\" 5 \"Cubic\")) xtitle(\"Bandwidth\")\n\n\n(23,117 missing values generated)\n(23,117 missing values generated)"
  },
  {
    "objectID": "11rdd.html#flasification-test",
    "href": "11rdd.html#flasification-test",
    "title": "Regression Discontinuity Design",
    "section": "Flasification test",
    "text": "Flasification test\n\n\nCode\nqui {\nivregress 2sls icfes_female (d = dx1) c.x1#dx1    if abs(x1)&lt;20\nest sto m1\nivregress 2sls icfes_age (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m2\nivregress 2sls icfes_urm (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m3\nivregress 2sls icfes_famsize (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m4\n}\nesttab m1 m2 m3 m4, keep(d)\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n             icfes_female       icfes_age       icfes_urm    icfes_fams~e   \n----------------------------------------------------------------------------\nd                  0.0199           0.128         0.00791          0.0439   \n                   (0.80)          (1.05)          (0.65)          (0.63)   \n----------------------------------------------------------------------------\nN                   14841           14799           14841           14801   \n----------------------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "12did.html",
    "href": "12did.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "title: “Differences in Differences” subtitle: “1 is good, 2 is twice as good” author: Fernando Rios-Avila format: revealjs: slide-number: true reveal-js-url: ppt_files width: 1400 height: 900 code-overflow: wrap"
  },
  {
    "objectID": "13SC.html#introduction",
    "href": "13SC.html#introduction",
    "title": "Synthetic Control",
    "section": "Introduction",
    "text": "Introduction\n\nOne more last time. What is the Goal of Causal Analysis?\n\n\n\n\n\n\n\nImportant\n\n\nThe goal of Causal Analysis is to identify how a treatment affects the outcome by itself, once all other factors are kept constant or controlled for.\n\n\n\nFrom a theoretical point of view, that is very easy. You simply compare two Potential outcomes:\n\\[\nTE_i = y_i(1)- y_i(0)\n\\]\nand aggregate those outcomes as needed:\n\\[\nATT=E(TE_i|D=1);ATU=E(TE_i|D=0);ATE=E(TE_i);ATX=E(TE_i|X)\n\\]"
  },
  {
    "objectID": "13SC.html#section",
    "href": "13SC.html#section",
    "title": "Synthetic Control",
    "section": "",
    "text": "Unfortunately, we only observe one outcome. You are either treated or untreated…So how do we fix this?\nYou need to find counterfactuals so both observed (\\(X\\)) and unobserved (\\(e\\)) are the same (or close) between treated and contro group.\n\nRCT: Gold Standard, You randomize treatment and compare means. If correctly done, \\(X's\\) and \\(e's\\) will be comparable across groups, and ATE’s can be identified.\nReg + FE: For other cases, we just work with observational data. First method, Regression (OLS?). Adding covariates controls for their presence, working as a pseudo balancing approach.\nYou could also add fixed effects, to control for factors that are fixed (across time), but you do not observe. (requires Panel data).\nIt works if Treatment occurs at the same time for everyone treated. and if Unobserved are “fixed”"
  },
  {
    "objectID": "13SC.html#section-1",
    "href": "13SC.html#section-1",
    "title": "Synthetic Control",
    "section": "",
    "text": "Instrumental variables: 2nd Best to RCT. It uses IV to generate a small randomization process that can be used for estimating ATT. Technically it compares the effect among those potentially affected by the random instrument. Requires Randome instrument, and no-defiers. Its a Local ATE\nMatching and Reweigthing. Similar to Regression, but better to balance characteristics. The goal is to find units with similar characteristics for all treated units. You can estimate ATE, ATT or ATU. Depends on how well Matching is done\nRDD. If you have data where treatment depends on a single variable and a threshold, you can use this to identify TE for those “Near” the threshold. They Key assumption, treatment assigment is as good at random at the threshold."
  },
  {
    "objectID": "13SC.html#section-2",
    "href": "13SC.html#section-2",
    "title": "Synthetic Control",
    "section": "",
    "text": "DD. Differences in differences uses variation across time and across individual to identify treatment effects. Under PTA, and SUTVA\nDif. within individuals eliminates common time trends, Dif across time, eliminates individual fixed effects. DD provide you with ATT’s for the treated, after treatment.\n\\[ATT=(Y_{g=1,t=1}-Y_{g=1,t=0}) - [(Y_{g=0,t=1}-Y_{g=0,t=0})]\\]\nCan be generalized to Many periods and many groups, but requires stronger assumptions (no anticipation and no change in treatment status), and further aggregation.\nOr combined with Matching for even better results."
  },
  {
    "objectID": "13SC.html#synthetic-control-special-case",
    "href": "13SC.html#synthetic-control-special-case",
    "title": "Synthetic Control",
    "section": "Synthetic Control: Special case",
    "text": "Synthetic Control: Special case\nAs previous Cases, Synthetic control aims to identify treatment constructing appropriate “counterfactuals”.\nIt is said that Synthethic controls may be even MORE credible methodology, because the treated group is by construction Exogenous…but how?\n\nThe treated group is a Case Study.\nAn isolated event or unit that is affected by a treatment, and should not affect other units !\n\nIn this sense, the treatment is exogenous, because it affected a single unit.\nBut what about the counterfactual?"
  },
  {
    "objectID": "13SC.html#section-3",
    "href": "13SC.html#section-3",
    "title": "Synthetic Control",
    "section": "",
    "text": "In other methods (in particular Matching), our “conterfactual” mean to look for observations that had the same characteristics as the treated observation.\nSome times, we needed to settle to use a single “bad” control, because we couldnt find one better. (people are very different).\n\nUsing Stricter criteria would make it unfeasible.\nMore relax and we have lots of biases.\n\nSC is different. You have MANY controls, so why settle with only one?\nSC is like Dr Frankenstein, where you “build” a single comparison group by averaging information of all controls.\nYou build the synthetic control getting “weighted averages”.\nBut…we assume you can see all units across time (panel data)"
  },
  {
    "objectID": "13SC.html#this-is-a-very-popular-method",
    "href": "13SC.html#this-is-a-very-popular-method",
    "title": "Synthetic Control",
    "section": "This is a very popular method",
    "text": "This is a very popular method\nWhere has this method been used:\n\neffects of right-to-carry laws (Donohue et al., 2019),\nlegalized prostitution (Cunningham and Shah, 2018),\nimmigration policy (Bohn et al., 2014),\ncorporate political connections (Acemoglu et al., 2016),\ntaxation (Kleven et al., 2013),\norganized crime (Pinotti, 2015)\n\nJust to name a few."
  },
  {
    "objectID": "13SC.html#assumptions",
    "href": "13SC.html#assumptions",
    "title": "Synthetic Control",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nThe Donor Pool should be a good match for the treated unit. Thus, the synthethic control should be Zero before treatment.\n\nThis is similar to PTA, but stronger. Before treatment, there should be no difference between Treated and synthetic control\n\nSUTVA. Only the treated group is affected by treatment. The control group should be unaffected (no spill over effects).\nThere should be NO other “event” in the period of analysis. (Thus we only capture treatment impact)"
  },
  {
    "objectID": "13SC.html#how-does-it-work.",
    "href": "13SC.html#how-does-it-work.",
    "title": "Synthetic Control",
    "section": "How does it work.",
    "text": "How does it work.\nRecall, we want to estimate TE for the single untreated unit:\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}\n\\]\nbut we do not observe \\(Y(0)_{1t}\\). We only know that before treatment\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}=0\\]\nWe could construct a synthetic control:\n\\[\\hat Y_{1t}(0) = \\sum_{i = 2}^N w_i Y_{it}\n\\]\nAt the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]"
  },
  {
    "objectID": "13SC.html#section-4",
    "href": "13SC.html#section-4",
    "title": "Synthetic Control",
    "section": "",
    "text": "At the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]\nHavent we seen seen something like this Before? OLS:\n\\[y = x\\beta + e\\] \\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\\[\n\\begin{bmatrix}\ny^1_1 \\\\ y^1_2 \\\\ ... \\\\ y^1_{G-1} \\\\\n\\end{bmatrix} = \\color{red}{a_0} +\n\\begin{bmatrix}\ny^2_1 & y^3_1 & ... & y^k_1  \\\\\ny^2_2 & y^3_2 & ... & y^k_2 \\\\\n...  & ... & ... & ...\\\\\ny^2_{G-1} & y^3_{G-1} & ... & y^k_{G-1}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_2 \\\\ w_3 \\\\ ... \\\\ w_k\n\\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "13SC.html#section-5",
    "href": "13SC.html#section-5",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\nIn this Specification, each row (observation) is a “pre-treatment” period of observed data.\nand each control unit (from the many controls) will be a variable.\n\nOLS can help you find the weights, which can then be used for obtaining the “Synthetic” control"
  },
  {
    "objectID": "13SC.html#small-example",
    "href": "13SC.html#small-example",
    "title": "Synthetic Control",
    "section": "Small Example",
    "text": "Small Example\n\n\nCode\nqui:frause smoking, clear\ncolor_style tableau\nbysort year:egen mean_cig=mean(cigsale) if state!=3\ntwo (line cigsale year if state ==3) (line mean_cig year if state==1), ///\n    legend(order(1 \"California\" 2 \"Avg Other States\") pos(6) col(2)) xline(1988)\n\n\n\n\n\n(31 missing values generated)"
  },
  {
    "objectID": "13SC.html#section-6",
    "href": "13SC.html#section-6",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\ndrop mean_cig\nqui:reshape wide cigsale lnincome beer age15to24 retprice , i(year) j(state)\nren cigsale1 mcigsale\n\n\n\nNow we have…38 variables, (other States but California)\nAnd 31 periods (only 19 Before treatment)\nCan we estimate the weights using OLS?\n\n…\n\nNop. N&lt;K !"
  },
  {
    "objectID": "13SC.html#section-7",
    "href": "13SC.html#section-7",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:reg mcigsale cigsale* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear  mcigsale cig* if year&lt;=1988, nocons\npredict mcigh2\n two (line mcigsale year, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n\n\n\n\nOLS Not appropriate (specially if N&lt;K)\nLasso Better, because of regularization, but not great.\nWe are not controlling for other factors either (controls)\nOther Details we cover next"
  },
  {
    "objectID": "13SC.html#allowing-for-covariates",
    "href": "13SC.html#allowing-for-covariates",
    "title": "Synthetic Control",
    "section": "Allowing for Covariates:",
    "text": "Allowing for Covariates:\n\nAs with other methodologies, one should also considered controlling for covariates.\nSpecifically, more covariates can be allowed by Stacking them:\n\n\\[\\begin{bmatrix} y^t_{1} \\\\ x^t_{1} \\\\ z^t_{1} \\end{bmatrix}\n= \\color{red}{(a_0=0)} +\n\\begin{bmatrix} y^t_{2} & y^t_{3} ... & y^t_{k} \\\\\n                x^t_{2} & x^t_{3} ... & x^t_{k} \\\\\n                z^t_{2} & z^t_{3} ... & z^t_{k} \\end{bmatrix}\n\\begin{bmatrix} w_2 \\\\ w_3 \\\\ ... \\\\ w_k \\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "13SC.html#section-8",
    "href": "13SC.html#section-8",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:frause smoking, clear\nren (cigsale lnincome beer age15to24 retprice) ///\n      (var1    var2     var3 var4      var5)\nqui: reshape long var, i(state year)    j(new)\nqui: reshape wide var, i(year new)  j(state)\nlabel define new 1 \"cigsale\" ///\n                 2 \"lnincome\" ///\n                 3 \"beer\" ///\n                 4 \"age15to24\" /// \n                 5 \"retprice\", modify\nlabel values new new    \nren var3 cal_out\nqui:reg cal_out var* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear   cal_out var* if year&lt;=1988, nocons\npredict mcigh2\n two (line cal_out year if new==1, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year if new==1, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(32 missing values generated)\n(options xb penalized assumed; linear prediction with penalized coefficients)"
  },
  {
    "objectID": "13SC.html#what-else-to-keep-in-mind",
    "href": "13SC.html#what-else-to-keep-in-mind",
    "title": "Synthetic Control",
    "section": "What else to keep in mind",
    "text": "What else to keep in mind\n\nWith More Variables, the goal is still to be able to choose \\(w's\\) that best explain the observed outcomes (and characteristics) of the “treated unit”.\n\n\\[w = \\min_w \\sum_{m=1}^K \\left[ v_m \\left( X_{1t}-\\sum_{j=2}^J w_j X_{jt}  \\right)^2 \\right]\n\\]\nHowever, we also need to impose restrictions on Weights:\n\n\\(w_j \\geq 0\\) Weights cannot be negative.\n\\(\\sum w_j =1\\) They should sum up to 1.\n\\(v_m\\) can be used to increase, or reduce the relative importance of factors in the model. (lower bound at 0) The constant is zero.\n\nThis is a maximization problem with constrains. Restrictions ensure the prediction is based on a “convex” set, avoiding extrapolation."
  },
  {
    "objectID": "13SC.html#is-it-noise-or-causal",
    "href": "13SC.html#is-it-noise-or-causal",
    "title": "Synthetic Control",
    "section": "Is it noise? or Causal?",
    "text": "Is it noise? or Causal?\n\nWhen using SC, you essentially have a sample \\(n=1\\) to estimate an effect. How do you know that effect is significant? and not just noise?\n\nYou can do a randomization experiment! and answer:\n\n\n\n“how unusual is this estimate under the null hypothesis of no policy effect?”.\n\n\nHow does this work?"
  },
  {
    "objectID": "13SC.html#randomization",
    "href": "13SC.html#randomization",
    "title": "Synthetic Control",
    "section": "Randomization",
    "text": "Randomization\n\nExcluding the treated unit, estimate the pseudo effect of every other unit in the dataset. These are placebos, and you should expect the effect to be zero for them…but you may see some positive and negative effects.\n\nThis may be consider the sampling distribution of the estimated effect.\n\nCalculate the pre- and post- treament Root mean squared prediction error for all units (treated and placebos).\n\nPre-RMSPE provides a statistic of how well the model fits before treatment.\nPost-RMSPE provides a statistic of how unusual is the outcome after the “treatment date”. The largest it is, the more unpredictable (or stronger treatment effect) it would be."
  },
  {
    "objectID": "13SC.html#section-9",
    "href": "13SC.html#section-9",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[\n  \\begin{aligned}\n  RMSPE_i^{pre} &= \\sqrt{ \\frac{1}{g-1}\\sum_{t=1}^{g-1}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 } \\\\\n  RMSPE_i^{post} &= \\sqrt{ \\frac{1}{T-g+1}\\sum_{t=g}^{T}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 }\n  \\end{aligned}\n  \\]\n\nEstimate the ratio between Pre and Post RMSPE, and rank them. \\[Ratio_i = \\frac{RMSPE_i^{post}}{RMSPE_i^{pre}}\n\\]\nThe p-value for the treatment is proportional to the Rank:\n\\[pvalue_i = \\frac{rank(i)}{Tot}\\]"
  },
  {
    "objectID": "13SC.html#lets-continue-the-example",
    "href": "13SC.html#lets-continue-the-example",
    "title": "Synthetic Control",
    "section": "Lets continue the example:",
    "text": "Lets continue the example:\n\n\nCode\nqui:frause smoking, clear\nxtset state year\ntempfile sc3\n** For California\nsynth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), trunit(3) trperiod(1989) keep(`sc3') replace\n** Same Specification for All other States excluding California\nforvalues i =1/39{\n    if `i'!=3 {\n        local pool\n        foreach j of local stl {\n            if `j'!=3 & `j'!=`i' local pool `pool' `j'\n        }\n        tempfile sc`i'\n        synth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), ///\n        trunit(`i') trperiod(1989) keep(`sc`i'') replace counit(`pool')\n    }\n}\n** Some data cleaning and prepration\nforvalues i =1/39{\n    use `sc`i'' , clear\n    gen tef`i' = _Y_treated - _Y_synthetic\n    egen sef`i'a =mean( (_Y_treated - _Y_synthetic)^2) if _time&lt;=1988\n    egen sef`i'b =mean( (_Y_treated - _Y_synthetic)^2) if _time&gt;1988\n  replace sef`i'a=sqrt(sef`i'a[1])\n    replace sef`i'b=sqrt(sef`i'b[_N])\n    drop if _time==.\n    keep tef`i' sef`i'* _time\n    save `sc`i'', replace\n}\n**\n** Merging all together, and getting ready to plot\n** \n\nuse `sc1', clear\nforvalues i = 2/39 {\n    merge 1:1 _time using `sc`i'', nogen\n}\nglobal toplot\nglobal toplot2\n\nforvalues i = 1/39 {\n    global toplot $toplot (line tef`i' _time, color(gs11) )\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        global toplot2 $toplot2 (line tef`i' _time, color(gs11) )\n    }\n}\n\n\nAll Cases\n\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "13SC.html#section-10",
    "href": "13SC.html#section-10",
    "title": "Synthetic Control",
    "section": "",
    "text": "Good Cases Restricts to States with Good RMSEP (less than 2 California)\n\n\nCode\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "13SC.html#rmse-ratio",
    "href": "13SC.html#rmse-ratio",
    "title": "Synthetic Control",
    "section": "RMSE Ratio",
    "text": "RMSE Ratio\n\n\nCode\nforvalues i = 1/39 {\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        matrix rt=nullmat(rt)\\[`i',sef`i'b[1]/sef`i'a[1]]\n    }\n}\nsvmat rt\negen rnk=rank(rt2)\n \ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , ///\n legend(order( 2 \"California\")) ///\n  ytitle(RMSE ratio)  xtitle(RMSE rank)\n\n\nnumber of observations will be reset to 32\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 31, now 32."
  },
  {
    "objectID": "13SC.html#p-values",
    "href": "13SC.html#p-values",
    "title": "Synthetic Control",
    "section": "p-values",
    "text": "p-values\n\n\nCode\n gen rnk2=0\nforvalues i = 1/39 {\n    if   (sef`i'a[1])&lt;(2*sef3a[1]) {\n        local t = `t'+1\n        replace rnk2=rnk2+(tef`i'&lt;=tef3)    \n    }\n} \ngen pv=rnk2*100/`t'\n \ntwo bar pv _time if _time&gt;1988 & rnk2&lt;32, ylabel(0(2)15) xlabel(1989/2000)\n\n\n(10 real changes made)\n(6 real changes made)\n(32 real changes made)\n(5 real changes made)\n(9 real changes made)\n(3 real changes made)\n(5 real changes made)\n(4 real changes made)\n(6 real changes made)\n(4 real changes made)\n(4 real changes made)\n(8 real changes made)\n(5 real changes made)\n(4 real changes made)\n(8 real changes made)\n(3 real changes made)\n(5 real changes made)\n(8 real changes made)\n(5 real changes made)\n(5 real changes made)\n(7 real changes made)\n(2 real changes made)\n(4 real changes made)\n(4 real changes made)\n(2 real changes made)\n(6 real changes made)\n(3 real changes made)\n(6 real changes made)\n(8 real changes made)\n(5 real changes made)\n(4 real changes made)\n(6 real changes made)"
  },
  {
    "objectID": "13SC.html#other-falsification-tests",
    "href": "13SC.html#other-falsification-tests",
    "title": "Synthetic Control",
    "section": "Other Falsification Tests",
    "text": "Other Falsification Tests\n\nChange of treatment Year.\n\nIn the manual implementation you may want to change the treatment year (to an earler point). One should see no effect between false treatment date and the true to be zero.\nUsing synth (Stata) you may want to drop some of the controls, so only “pre-false” treatment data is used.\n\nLook also into synth_runner\n\n\nChange of Outcome.\n\nOne can estimate the effect on alternative outcomes. No effect should be estimated."
  },
  {
    "objectID": "13SC.html#conclusions",
    "href": "13SC.html#conclusions",
    "title": "Synthetic Control",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe basic methodology presented here differs from other strategies because one uses a single treated unit, with pletora of treated groups.\nInstead of comparing single units with the treated group, it aims to compare a weighted average “synthetic control” to do so.\nIt will work better than matching because you are focusing on getting the best “weighted” group for a single unit.\nBut this methodology is still under development, with extensions toward using dissagregated data, or a combination with DD approaches.\nThis may change how much more one can do with the method"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was created with the only purpose of sharing the class slides with all Students.\nIt will also (at some point) contain the homeworks and class assigments."
  },
  {
    "objectID": "adv_class.html",
    "href": "adv_class.html",
    "title": "Advance Econometrics: Causal Effects",
    "section": "",
    "text": "Introduction: The longest journay starts somewhere"
  },
  {
    "objectID": "adv_class.html#part-i-the-tools",
    "href": "adv_class.html#part-i-the-tools",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part I: The tools",
    "text": "Part I: The tools\n\n1. Unveiling the Tapestry of Truth: The Grand Saga of Linear Regression and its Statistical Inference\nLinear Regression Model: Statistical Inference and Extensions: html or pdf\n\n\n2. Unleashing the Power of Infinite Flexibility: Exploring the Cosmos of Semi- and Non-Parametric Regression\nSemi- and Non-Parametric Regression: How Flexible is Flexible Enough?: html or pdf\n\n\n3. Beyond the Ordinary: Embarking on the Quest of Conditional Quantile Regressions\nConditional Quantile Regressions: Because No One is Average html or pdf\n\n\n4. Ascending the Ladder of Equality: Studying the Mysteries of Unconditional Quantile Regressions\nUnconditional Quantile Regressions: When We Care About Everyone html or pdf\n\n\n5. Breaking the Chains of Linearity: A Transcendent Expedition into NLS, IRLS, and MLE\nNLS, IRLS, and MLE: Going Truly Nonlinear html or pdf"
  },
  {
    "objectID": "adv_class.html#part-ii-the-methods",
    "href": "adv_class.html#part-ii-the-methods",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part II: The methods",
    "text": "Part II: The methods\n\n6. Chronicles of Fate: Exploring the Mystical What-Ifs in the Web of Causal Models\nPotential outcomes and Causal Models html or pdf\n\n\n7. The Mirror’s Embrace: Taming Unobservables with the Power of Fixed Effects\nPanel Data and Fixed Effects (Many FE) html or pdf\n\n\n8. The Summoner’s Call: Using Instrumental Variables to estimate LATE’s\nInstrumental Variables html or pdf\n\n\n9. The Blade of Equivalence: Unleashing the Art of Matching in the Realm of Divergence\nMatching and Re-weighting html or pdf\n\n\n10. Beyond the Discontinuity Veil: Crossing thresholds to Illuminate Secrets\nRegression Discontinuity Design html or pdf\n\n\n11. Twofold Wisdom: Unraveling Truths with the Dual Forces of Differences in Differences\nDifferences in Differences pdf\n\n\n12. Cosmic Recreations: Forging New Realities through Synthetic Control\nSynthetic Control html or pdf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics",
    "section": "",
    "text": "Greetings, valiant students of the realm!\nPrepare yourselves, for you stand upon the precipice of a grand and extraordinary odyssey, one that will propel you through the cosmic tapestry of knowledge and understanding. Behold, the epic adventure that awaits you: the realm of Econometrics, where the forces of knowledge converge!\nAs you step into this wondrous expedition, envision yourselves as intrepid explorers, traversing vast galaxies of data and unlocking the secrets of economic phenomena that lie hidden amidst the stars. With quills and scrolls in hand, you shall decipher the arcane languages of equations and statistical models, unraveling the very fabric of reality itself.\nLike brave knights donning their armor, you shall equip yourselves with the tools of econometric analysis, harnessing the powers of regression, causality, and inference. Through the warp and weft of datasets, you shall chart a course through treacherous terrains of multicollinearity and endogeneity, boldly navigating the uncharted territories of economic relationships.\nBut beware, dear scholars, for this adventure is not for the faint of heart. Along your path, you shall encounter formidable challenges that test your mettle and determination. Like cosmic anomalies distorting the very laws of causality, confounding variables shall seek to cloud your vision. Yet fear not, for within your arsenal lies the mighty sword of identification strategies, capable of cleaving through the veils of confusion and revealing the true essence of causation.\nAs you traverse the celestial landscapes of panel data and time series analysis, immerse yourselves in the symphony of econometric theories, their harmonies resonating across the realms of academia and policy. Delve into the arcane mysteries of heteroscedasticity, autocorrelation, and simultaneity, unearthing the hidden treasures of robustness and efficiency.\nTogether, as a fellowship of scholars, you shall brave this extraordinary journey. Engage in spirited debates, exchange insights, and sharpen your intellects as you sail upon the vast sea of econometric inquiry. Embrace the camaraderie of fellow explorers, as you forge alliances and embark upon group projects, unraveling the enigmas of econometric phenomena that defy conventional wisdom.\nAnd when you emerge from this magnificent expedition, equipped with the knowledge and skills of econometrics, you shall be heralded as true heroes of economic analysis, ready to shape the destinies of nations and unlock the frontiers of progress. The legacy you create shall reverberate throughout the annals of scholarly lore, forever remembered as the brave souls who dared to venture into the realm of discovery.\nSo, my esteemed students, with hearts aflame and minds afire, let us embark upon this epic adventure together. May your econometric quest be filled with triumphs and revelations, as you transcend the boundaries of the ordinary and ascend to the echelons of intellectual greatness. Embrace the grandeur of this cosmic odyssey and let the realm of econometrics unveil its splendor before you!\nOnward, intrepid scholars, to the boundless vistas of glory! And let the path take you as far as you can.\nF.G.R.P.A.T."
  },
  {
    "objectID": "PO_RCT.html",
    "href": "PO_RCT.html",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "PO_RCT.html#randomized-control-trial",
    "href": "PO_RCT.html#randomized-control-trial",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "PO_RCT.html#visual-exploration",
    "href": "PO_RCT.html#visual-exploration",
    "title": "RCT Implementation",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nNow that we have a randomized treatment, we could start exploring the data:\n\n\nCode\ntwo (kdensity lnwage if trt == 1) (kdensity lnwage if trt == 0) , ///\n    legend(order(1 \"Treated\" 2 \"Untreated\"))\n\n\n\n\n\nLog wage distribution between Treated and untreated\n\n\n\n\nIn order to estimate the treatment effects, we could simple estimate a regression model of the outcome. Compare it to the treatment effect"
  },
  {
    "objectID": "PO_RCT.html#estimation-of-ate-effect",
    "href": "PO_RCT.html#estimation-of-ate-effect",
    "title": "RCT Implementation",
    "section": "Estimation of ATE Effect",
    "text": "Estimation of ATE Effect\n\n\nCode\n** True Effect\nsum teff\n** Simple Regression\nset linesize 255\nreg lnwage  trt, robust\nest sto m0\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,647   -.2121816    .6613419  -3.024343   2.704082\n\nLinear regression                               Number of obs     =      1,647\n                                                F(1, 1645)        =      79.85\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0464\n                                                Root MSE          =      .5017\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |  -.2213292   .0247679    -8.94   0.000    -.2699092   -.1727492\n       _cons |   3.465982   .0150123   230.88   0.000     3.436537    3.495427\n------------------------------------------------------------------------------\n\n\nBecause treatment is randomized, we could also add other controls to the model, and improve on precision\n\nCode\nqui:reg lnwage  trt age agesq , robust\nest sto m1\nqui:reg lnwage  trt age agesq married divorced , robust\nest sto m2\nqui:reg lnwage  trt age agesq married divorced kids6 kids714 , robust\nest sto m3\n\nesttab m0 m1 m2 m3, se nonum mtitle(\"m0\" \"m1\" \"m2\" \"m3\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\n\n\nm0\nm1\nm2\nm3\n\n\n\n\ntrt\n-0.221***\n-0.214***\n-0.213***\n-0.212***\n\n\n\n(0.0248)\n(0.0225)\n(0.0225)\n(0.0224)\n\n\nN\n1647\n1647\n1647\n1647\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "PO_RCT.html#falsification",
    "href": "PO_RCT.html#falsification",
    "title": "RCT Implementation",
    "section": "Falsification",
    "text": "Falsification\nWe could just use other outcomes that shouldnt be affected by the treatment. You expect they have no impact on outcome\n\nCode\nqui:reg exper  trt age agesq married divorced kids6 kids714 , robust\nest sto m0\nqui:reg tenure trt age agesq married divorced kids6 kids714 , robust\nest sto m1\nesttab m0 m1 , se nonum mtitle(\"m0\" \"m1\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\nm0\nm1\n\n\n\n\ntrt\n-0.104\n-0.458\n\n\n\n(0.366)\n(0.335)\n\n\nN\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "PO_RCT.html#balance-test",
    "href": "PO_RCT.html#balance-test",
    "title": "RCT Implementation",
    "section": "Balance test",
    "text": "Balance test\nYou should also try to create balance tables, where you compare and test if characteristics are similar across treated and control groups:\n\n\nCode\ntabstat age agesq married divorced kids6 kids714 , by(trt)\nsureg age agesq married divorced kids6 kids714 =trt, \n\n\n\nSummary statistics: Mean\nGroup variable: trt \n\n     trt |       age     agesq   married  divorced     kids6   kids714\n---------+------------------------------------------------------------\n       0 |  39.14475   1649.63    .53076  .1206273  .2979493  .3365501\n       1 |   39.3643  1675.521  .5158924  .1466993  .2713936  .3215159\n---------+------------------------------------------------------------\n   Total |  39.25379  1662.489  .5233758  .1335762  .2847602  .3290832\n----------------------------------------------------------------------\n\nSeemingly unrelated regression\n------------------------------------------------------------------------------\nEquation             Obs   Params         RMSE  \"R-squared\"      chi2   P&gt;chi2\n------------------------------------------------------------------------------\nage                1,647        1     11.02798      0.0001       0.16   0.6862\nagesq              1,647        1     893.7224      0.0002       0.35   0.5566\nmarried            1,647        1     .4993979      0.0002       0.36   0.5458\ndivorced           1,647        1     .3399466      0.0015       2.42   0.1197\nkids6              1,647        1     .6626276      0.0004       0.66   0.4161\nkids714            1,647        1     .7071256      0.0001       0.19   0.6662\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nage          |\n         trt |   .2195505   .5434865     0.40   0.686    -.8456636    1.284765\n       _cons |   39.14475   .3830175   102.20   0.000     38.39405    39.89545\n-------------+----------------------------------------------------------------\nagesq        |\n         trt |   25.89111   44.04489     0.59   0.557    -60.43528    112.2175\n       _cons |    1649.63   31.04026    53.14   0.000     1588.792    1710.467\n-------------+----------------------------------------------------------------\nmarried      |\n         trt |  -.0148675   .0246116    -0.60   0.546    -.0631054    .0333703\n       _cons |     .53076   .0173448    30.60   0.000     .4967648    .5647552\n-------------+----------------------------------------------------------------\ndivorced     |\n         trt |    .026072   .0167534     1.56   0.120    -.0067641    .0589081\n       _cons |   .1206273   .0118068    10.22   0.000     .0974863    .1437682\n-------------+----------------------------------------------------------------\nkids6        |\n         trt |  -.0265557    .032656    -0.81   0.416    -.0905602    .0374488\n       _cons |   .2979493    .023014    12.95   0.000     .2528427     .343056\n-------------+----------------------------------------------------------------\nkids714      |\n         trt |  -.0150342   .0348489    -0.43   0.666    -.0833368    .0532685\n       _cons |   .3365501   .0245595    13.70   0.000     .2884143    .3846858\n------------------------------------------------------------------------------\n\n\nHere, the goal is just to see if trt is not-significant across groups"
  },
  {
    "objectID": "rm_class.html",
    "href": "rm_class.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Nothing to see here yet"
  }
]