<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<title>Margins after -ml-</title>
<style>
html { -webkit-text-size-adjust: 100%; }
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 14px; line-height: 1.428;
  margin: 0 auto; padding: 0 15px;
}
h1, h2, h3, h4, h5, h6 { margin: 20px 0 10px; }
h1 { font-size: 28px; } h2 { font-size: 24px; }
h3 { font-size: 18px; } h4 { font-size: 16px; }
h5 { font-size: 14px; } h6 { font-size: 12px; }
a { color: #337AB7; text-decoration: none; }
a:hover { text-decoration: underline; }
img { max-width: 100%; height: auto; }
ul, ol { padding-left: 30px; }
pre, code, samp {
  font-size: 13px;
  font-family: Courier, monospace;
}
code, samp {
  background-color: #F5F5F5;
  border-radius: 3px; padding: 3px;
}
pre code, pre samp {
  white-space: pre; background: transparent;
  border: none; padding: 0;
}
pre {
  line-height: 1.33; background-color: #F5F5F5;
  border: 1px solid #CCCCCC; border-radius: 3px;
  padding: 8px; overflow: auto;
}
</style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script><style>
.stlog { color: #000000; background-color: #F0F3F9; }
.stres { color: #324F58; }
.stinp { font-weight: bold; color: #000000; }
.stcmd .stcmt { font-style: italic; opacity: 0.5; }
.stoom, .stcnp { font-style: italic; }
@media screen { .stcnp { display: none; }}
</style>
</head>
<body>
<h1>Margins and -ml-</h1>
<h1>The IV-Probit model</h1>
<h2>Introduction</h2>
<p> Previously, I have shown how to use -margins- after -ml-, for the linear regression model 
(under normality assumption), and for the probit model.
</p>
<p> Today, I'll provide yet another example, where the goal is to estimate a two equation model. Namely, the infamous "IVprobit", or
instrumental variable probit.
</p>
<p> I call this infamous, because it has been subject of multiple threads on Statalist, because of the consistency (or lack there off) regarding estimated marginal effects.
you can read some of the latest discussion here.
</p>
<p> The bottom line is that through different Stata versions, margins produced different versions of "marginal effects" for this model. All of them correct, 
but correct from a computational point of view. 
</p>

<p> Based on own assessment, corroborated by discussions with colleagues this is the summary of what Stata would do:
</p>

<ul>
<li>Stata 13: Estimated the correct marginal effects for the ivprobit MLE. But not for the two step approach.</li>
<li>Stata 14 & 15: Estimated the Full information marginal effect (I ll explain later what this is). While correct, if considered mechanically, 
it contradicts common sense. This has been discussed extensibly by Prof Wooldridge. He advocates for a) MLE marginal effects or b) two-step marginal effects.</li>
<li>Stata 16: Due to the recent discussions, an update has been provided and -margins-
 now produces something equivalent to the two-step marginal effect...but for the MLE estimation. I suspect it will change again too. </li>
</li>
</ul>

<p> So, without further ado, let me present you my own version of how to estimate an ivprobit model, and how the predict program could be setup, and why we had so many
types of marginal effects lurking around.
</p>

<h2>The Setup</h2>
<p> First thing first. Unless you already have this program saved somewhere in your accesible ado files (most likely the "ado/personal" folder),
make sure to have the following program in memory. 
</p>
<pre id="stlog-1" class="stlog"><samp><span class="stinp">. program adde, eclass</span>
  1<span class="stinp">.         ereturn `0'</span>
  2<span class="stinp">. end</span>
</samp></pre>
<p> So, lets start. 
</p>

<p> The IVprobit model is a nonlinear model that can be used when your dependent variable is a binary variable (0 - 1), thus you want
to estimate the "probability of success (y=1)" given a set of characteristics. \( P(y=1|X) \)., 
but you have some continuous and endogenous explanatory variable.
</p>

<p> Econometrics 101. If a variable is endogenous , we cannot estimate the model and interpret the 
results as causal effects,
 because changes in the endogenous variable may happen at the same time as 
 changes in unobserved commponents. Thus, if the outcome changes, we do not know if it is
 because the endogenous variable changed, or because the unobservables changed. (they are, after all, correlated).
</p>

<p> In this case, one option we have to deal with the unobserved confunders is to use instruments to either 
isolate exogenous variation of the variable of interest (using the 2sls approach for example) or obtain an approximation 
of the endogenous component that we can control for (Control function approach). 
</p>

<p> IV-probit is in fact the application of the later. A control function approach.
</p>

<p> Formally, IVprobit model can be written as follows:
$$y_2 = z_1 \delta_1 + z_2 \delta_2 + u_2 \quad (1) $$
$$y^*_1 = z_1 \beta_1 + y_2 \beta_2 + u_1 \quad (2) $$
$$y_1= 1(y^*_1>0)$$
</p>
<p> where the errors \( (u_1,u_2) \) follow a bivariate normal distribution:
$$
\begin{pmatrix} u_1 \\ u_2 \end{pmatrix} \sim Normal  \begin{pmatrix}  \begin{pmatrix} 0 \\ 0 \end{pmatrix},
\begin{pmatrix} 1 & \rho \sigma_2\\ \rho \sigma_2 & \sigma^2_2 \end{pmatrix} \end{pmatrix} 
$$
In this model \( z_1 \) is the set of exogenous variables that affect \( y^*_1 \), 
and \( z_2 \) are the instruments.
</p>

<p>
It is also easy to see that \( y_2 \) is endogenous, because:
 $$ if \quad corr(u_1,u_2) \neq 0 \quad and \quad  corr(y_2,u_2) \neq 0 \rightarrow corr(y_2,u_1) \neq 0 $$
</p>

<p>
So how do we estimate this model? by parts!
</p>

<p>
First, equation (1) can be estimated directly, because it is a function of exogenous 
variables only. Thus, we could estimated that equation
using standard OLS (as ivprobit-two step does), or via MLE assuming the normality of the errors \( u_2 \).
</p>

<p>
The one that requires more attention is equation (2). We know that \( corr(y_2,u_1) \) 
is different from zero, which is the cause of the endogeneity of \( y_2 \).  We could, however, "decompose"
\( u_1 \), into two parts. One that contains the endogenous component, and one that is 
exogenous and uncorrelated with all other variables. First, recall that if \( (u_1,u_2) \)
 follows a bivariate normal distribution,
then, conditional on \( u_2 \), \( u_1 \) will have the following distribution:
$$ u_1 \sim N \left( \rho \frac{u_2}{\sigma_2}, \sqrt{1-\rho^2} \right) $$
This means that:
$$ u_1 = \rho \frac{u_2}{\sigma_2} + v_1 \quad (3) $$
so that \( v_1 \) is uncorrelated with \( y_2 \). We can replace equation (3) into (2) which gives us:
$$y^*_1 = z_1 \beta_1 + y_2 \beta_2 + \rho \frac{u_2}{\sigma_2} + v_1  (4) $$
This equation could now be estimated directly, assuming we observe \( u_2 \). However, 
to be estimated using a probit model, we need to rescale the equation so that the error \(v_1 \) has a variance of 1. 
</p>
<p>
This can be done by simply dividing all terms by \( \sqrt(1-\rho^2) \), and estimating the following
model using standard probit model.

	$$\frac{y^*_1}{\sqrt{1-\rho^2})} = 
	z_1 \frac{\beta_1}{\sqrt{1-\rho^2}} + 
	y_2 \frac{\beta_2}{\sqrt{1-\rho^2}} + 
	\frac{\rho}{\sqrt{1-\rho^2}} \frac{u_2}{\sigma_2} +
	\frac{v_1}{\sqrt{1-\rho^2}} \quad (5)
	$$
	or perhaps the simpler:
	$$y^{**}_1 = 
	z_1 \beta^r_1 + y_2 \beta^r_2 + 
	\theta \frac{u_2}{\sigma_2} +
	\nu_1 \quad (6)
	$$
</p>

<p>
So what is the difference between using one equation or the other?. I would argue none,
as long as you know how estimate the standard errors from the system.
</p>

<h2>The actual estimation</h2>
<p>
So, how do we estimate this ivprobit model? I would say that there are at least 
three ways of doing so.
</p>

<p>
The first method is what is typically known as the two-step approach. In the first
step, one estimates equation (1) using, say, OLS. obtain the predicted residuals \( \hat{u}_2 \),
plug them in equation (6), which can be estimated using a simple probit model. Of course,
in doing so, one does not obtain estimates for the structural coefficients, but only the 
"rescaled" ones. 
<br><br>
The main difficulty of using this method is the correct estimation of the standard errors
of all coefficients. 
<br><br>
Many textbooks would say it is a "simple" application of a delta method, or, use of bootstrap, 
but the fact of the matter is that you need some way to account that \( \hat{u}_2 \) is 
a variable that is measured with error, when you estimate equation (5).
</p>

<p>
The second method is to estimate equations (1) and (4) simulatenously using full information maximum likelihood.
This imposes the assumption that the errors follow a bivariate normal distribution, and allows you to 
obtain estimates for the structural parameters in equation (2), in addition to the "linked" parameters
\( \sigma_2 \) and \( \rho \). 
<br>
<br>
Under this strategy, the contribution of a single observation to the likelihood function becomes:
$$ L_i = L^1_i * L^2_i \quad (7a)$$
$$ L^1_i=\phi \left( y_2,z_1 \delta_1 + z_2 \delta_2, \sigma_2 \right) \quad (7b)$$
$$ L^2_i=\Phi \left( \frac{ z_1 \beta_1 + y_2 \beta_2 + \rho \frac{y_2 -z_1 \delta_1 - z_2 \delta_2}{\sigma_2}}
{\sqrt{1-\rho^2}} \right) \quad if \quad y_1 ==1 (7c)$$
$$ L^2_i=1-\Phi \left( \frac{ z_1 \beta_1 + y_2 \beta_2 + \rho \frac{y_2 -z_1 \delta_1 - z_2 \delta_2}{\sigma_2}}
{\sqrt{1-\rho^2}} \right) \quad if \quad y_1 ==0 (7d)$$
Notice that instead of "plugging in" \( \hat{u}_2 \) in the second equation, we plug \( y_2 -z_1 \delta_1 - z_2 \delta_2 \).
This allows to implicitly account for the measurment erros of the first stage.
</p>
<p>
There is a third option, which I call "two-step-mle". I call it two stage, because the ivprobit will be estimated
using equations (1) and (5). However, I call it MLE, because both equations are estimated simultaneously using MLE:
$$ L_i = L^1_i * L^2_i \quad (8a)$$
$$ L^1_i=\phi \left( y_2,z_1 \delta_1 + z_2 \delta_2, \sigma_2 \right) \quad (8b)$$
$$ L^2_i=\Phi \left(  z_1 \beta^r_1 + y_2 \beta^r_2 + \theta \frac {y_2 -z_1 \delta_1 - z_2 \delta_2}{\sigma_2} \right) \quad (8c)$$
The main difference with the standard FIML, is that only rescaled coefficients are are estimated, and that the
link between both equation is not \( \rho \), but \( \theta \). 
<br><br>
However, for this simplified example, both equations identify the exactly the same model. 
<br><br> Compared to the usual two-step approach, however, because the model is estimated simultaneously, the
standard errors of all coefficients can be correctly estimated, without further calculations (no delta method nor bootstrap).
<br><br>
One last thing to notice in this model. First, there is a close relationship between \( \rho \) and \( \theta \):
$$\theta = \frac{\rho}{\sqrt{1-\rho ^2 }} \quad (9a)$$
$$\rho ^2 = \frac{\theta ^2 }{{1+\theta^2 }} \quad (9b)$$
and between the rescaled parameters (we will need this later on):
$$\beta_1 = \beta^r_1 * {\sqrt{1-\rho^2}} = \frac{\beta^r_1}{\sqrt{1+\theta^2}} \quad (10)$$

</p>

<h2>The Log Likelihood function.</h2>

<p> While I have shown that using FIML and two-step-ml will provide virtually the same results, I ll stick with the two-step approach,
as it allows me to derive marginal effects telling the story of what happened to -margins- though different Stata versions. 
<br><br>
The following program defines this the log likelihood function for the IV probit, using the two step approach (equation 8), using the following
walk-through for the specification:
</p>

<ul>
<li> xb will contain all the exogenous variables \( z_1 \) plus the endogenous variable \( y_2 \). </li>
<li> zb will contain all the exogenous variables \( z_1 \) and instruments \( z_2 \). </li>
</ul>
<pre id="stlog-2" class="stlog"><samp><span class="stinp">. capture program drop myivprobit_2sls</span>

<span class="stinp">. program myivprobit_2sls</span>
  1<span class="stinp">.         args lnf xb  theta  zb lnsigma</span>
  2<span class="stinp">.         qui {</span>
  3<span class="stinp">.                 local y1 $ML_y1</span>
  4<span class="stinp">.                 local y2 $ML_y2</span>
  5<span class="stinp">.                 local u2 (`y2'-`zb')</span>
  6<span class="stinp">.  </span>
<span class="stinp">.                 tempvar xb_zb p1 p0</span>
  7<span class="stinp">.                 gen double `xb_zb'= `xb'+`theta'*((`u2')/exp(`lnsigma')) </span>
  8<span class="stinp">.                 gen double `p1'   = normal( `xb_zb')</span>
  9<span class="stinp">.                 gen double `p0'   = normal(-`xb_zb')</span>
 10<span class="stinp">.                 tempvar lnf1 lnf2</span>
 11<span class="stinp">.                 gen double `lnf1'  = log(normalden(`y2', `zb', exp(`lnsigma')))</span>
 12<span class="stinp">.                 gen double `lnf2' = log(`p1') if `y1'==1</span>
 13<span class="stinp">.                 replace    `lnf2' = log(`p0') if `y1'==0</span>
 14<span class="stinp">.                 replace `lnf' = `lnf1' + `lnf2'</span>
 15<span class="stinp">.         }</span>
 16<span class="stinp">.  </span>
<span class="stinp">. end</span>
</samp></pre>
<h2>The predict program</h2>
<p> 
So finally, the part that will be a bit more controversial. The prediction of probability of success!.
<br><br>
The reason why this is controntroversial is because  there are two candidates have been suggestes to identify 
this expression. 
<br><br>
The first candidate relates to the structural equation (2). Basically, if we are able to estimate the unscaled coefficients, 
the predicted outcome could be identified by:
$$ P(y_1=1| z_1 , y_2) = \Phi \left( z_1 \beta_1 + y_2 \beta_2  \right) $$
or if one prefers the version based on rescaled coefficients:
$$ P(y_1=1| z_1 , y_2) = \Phi \left( z_1 \beta^r_1 * {\sqrt{1-\rho^2}} + y_2 \beta^r_2 * {\sqrt{1-\rho^2}} \right) $$
And marginal effects can be obtained by analyzing this equation alone. Standard errors for this expression (at the corresponding
marginal effects) can be identified directly only if we estimate the the IVprobit 
by FIML, or using the rescaled coefficients, making sure standard errors are calculated acounting for the estimation errors of the first stage.
<br><br>
The second option relates to estimate the marginal effects using equation (6):
$$ P(y_1=1| z_1 , y_2, \hat{u}_2 ) = \Phi \left( z_1 \beta^r_1 + y_2 \beta^r_2 + \theta \hat{u}_2 \right) $$
However, because \( \hat{u}_2 \) is never really observed, it is usually recommended to average (but not ignore) the impact of \( \hat{u}_2 \) 
on the equation:
$$ P(y_1=1| z_1 , y_2) = E \left( \Phi \left( z_1 \beta^r_1 + y_2 \beta^r_2 + \theta \hat{u}_2 \right)| z_1, y_2 \right) $$
The bottom line: If one uses the two-step approach, marginal effects could be estimated assuming \( \hat{u}_2 \) is just another 
exogenous variable in the model. The difficulty would be obtaining the correct estimation of standard errors. (specially because they will depend on 
the measurement errors of \( \hat{u}_2 \).
<br><br>
Now, the FIML option is the most efficient, because standard errors of \(  P(y_1=1|.) \) depend on \(  \beta_r \)'s and \( \rho  \). 
However the two-step procedure depend on \(  \beta_r's \), \( \rho  \), and \( \hat{u}_2  \).
<br><br>
So Lets write these two options into a "predict" program.
</p>
<pre id="stlog-3" class="stlog"><samp><span class="stinp">. program myivprobit_p</span>
  1<span class="stinp">.         syntax newvarname [if] [in] , [ pr1 pr2  *]</span>
  2<span class="stinp">.         if "`pr1'`pr2'" =="" {</span>
  3<span class="stinp">.             ml_p `0'</span>
  4<span class="stinp">.         }</span>
  5<span class="stinp">.         tokenize `e(depvar)'</span>
  6<span class="stinp">.         local y1  `1'</span>
  7<span class="stinp">.         local y2  `2'</span>
  8<span class="stinp">.         marksample touse, novarlist</span>
  9<span class="stinp">.         if "`pr1'" !=""  {</span>
 10<span class="stinp">.             tempvar xb zb theta lnsigma</span>
 11<span class="stinp">.             _predict double `xb'   , eq(#1)</span>
 12<span class="stinp">.                 _predict double `theta', eq(#2)</span>
 13<span class="stinp">.                 _predict double `zb'   , eq(#3) </span>
 14<span class="stinp">.                 _predict double `lnsigma', eq(#4)       </span>
 15<span class="stinp">.                 gen `typlist' `varlist' = normal(`xb'+`theta'*(`y2'-`zb')/exp(`lnsigma')) if `touse'</span>
 16<span class="stinp">.                 label var `varlist' "P(y=1|X) two-step"</span>
 17<span class="stinp">.         }       </span>
 18<span class="stinp">.         else if "`pr2'"!="" {</span>
 19<span class="stinp">.                 tempvar xb zb theta lnsigma</span>
 20<span class="stinp">.             _predict double `xb' , eq(#1)</span>
 21<span class="stinp">.                 _predict double `theta'  , eq(#2)</span>
 22<span class="stinp">.                 gen `typlist' `varlist' = normal(`xb'/sqrt(1+`theta'^2)) if `touse'</span>
 23<span class="stinp">.                 label var `varlist' "P(y=1|X) FIML"</span>
 24<span class="stinp">.         }</span>
 25<span class="stinp">. end</span>
</samp></pre>
<p>
so, the first option -pr1- will estimate the predicted probability as if the model were estimated using the two step approach, 
whereas the second will estimate the predicted probability based on the structural equation.
<br><br>
Alright, so lets estimate the model and compare the results with the built-in ivprobit command:
</p>
<pre id="stlog-4" class="stlog"><samp><span class="stinp">. clear  </span>

<span class="stinp">. webuse laborsup</span>

<span class="stinp">. global  y1   fem_work</span>

<span class="stinp">. global  z1   fem_educ   kids  </span>

<span class="stinp">. global  y2   other_inc</span>

<span class="stinp">. global  z2   male_educ   </span>

<span class="stinp">. </span>
<span class="stinp">. <span class="stcmt">*Built in command:</span></span>
<span class="stinp">. ivprobit $y1  $z1 ($y2 = $z2), two</span>
Checking reduced-form model...

Two-step probit with endogenous regressors        Number of obs   = <span class="stres">       500</span>
                                                  Wald chi2(<span class="stres">3</span>)    = <span class="stres">     93.97</span>
                                                  Prob &gt; chi2     = <span class="stres">    0.0000</span>

------------------------------------------------------------------------------
             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   other_inc |<span class="stres">   -.058473   .0093364    -6.26   0.000    -.0767719    -.040174</span>
    fem_educ |<span class="stres">    .227437   .0281628     8.08   0.000     .1722389     .282635</span>
        kids |<span class="stres">  -.1961748   .0496323    -3.95   0.000    -.2934522   -.0988973</span>
       _cons |<span class="stres">   .3956061   .4982649     0.79   0.427    -.5809752    1.372187</span>
------------------------------------------------------------------------------
Instrumented:  other_inc
Instruments:   fem_educ kids male_educ
------------------------------------------------------------------------------
Wald test of exogeneity: chi2(<span class="stres">1</span>) = <span class="stres">6.50</span>                   Prob &gt; chi2 = <span class="stres">0.0108</span>

<span class="stinp">. <span class="stcmt">*my ivprobit two-step</span></span>
<span class="stinp">. ml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , <span class="stcmt">///</span>
&gt;                     technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog</span>

<span class="stinp">. ml display</span>

                                                Number of obs     = <span class="stres">       500</span>
                                                Wald chi2(<span class="stres">3</span>)      = <span class="stres">     94.01</span>
Log likelihood = <span class="stres">-2368.2062</span>                     Prob &gt; chi2       = <span class="stres">    0.0000</span>

------------------------------------------------------------------------------
             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
<span class="stres">eq1          </span>|
    fem_educ |<span class="stres">    .227437   .0281561     8.08   0.000      .172252     .282622</span>
        kids |<span class="stres">  -.1961748   .0496179    -3.95   0.000    -.2934242   -.0989254</span>
   other_inc |<span class="stres">   -.058473   .0093339    -6.26   0.000    -.0767671   -.0401788</span>
       _cons |<span class="stres">   .3956062   .4981099     0.79   0.427    -.5806714    1.371884</span>
-------------+----------------------------------------------------------------
<span class="stres">theta        </span>|
       _cons |<span class="stres">   .4008085   .1626174     2.46   0.014     .0820843    .7195328</span>
-------------+----------------------------------------------------------------
<span class="stres">eq3          </span>|
    fem_educ |<span class="stres">   .3351866   .2825972     1.19   0.236    -.2186937     .889067</span>
        kids |<span class="stres">   .8329055   .5475666     1.52   0.128    -.2403052    1.906116</span>
   male_educ |<span class="stres">   2.845253    .282746    10.06   0.000     2.291081    3.399425</span>
       _cons |<span class="stres">   9.872562   5.029193     1.96   0.050      .015524     19.7296</span>
-------------+----------------------------------------------------------------
<span class="stres">lnsigma      </span>|
       _cons |<span class="stres">   2.813383   .0316228    88.97   0.000     2.751404    2.875363</span>
------------------------------------------------------------------------------
</samp></pre>
<p>
you can see right away that except for differences attributed to rounding errors, and degrees of freedom
the results are virtually the same. 
<br><br>
It is also reasuring to see that the results are also the same when we compared ivprobit-mle and 
the rescaled coefficients:
</p>
<pre id="stlog-5" class="stlog"><samp><span class="stinp">. <span class="stcmt">*FIML</span></span>
<span class="stinp">. ivprobit $y1  $z1 ($y2 = $z2), ml</span>

Fitting exogenous probit model

Iteration 0:   log likelihood = <span class="stres">-344.63508</span>  
Iteration 1:   log likelihood = <span class="stres">-252.10819</span>  
Iteration 2:   log likelihood = <span class="stres">-252.04529</span>  
Iteration 3:   log likelihood = <span class="stres">-252.04529</span>  

Fitting full model

Iteration 0:   log likelihood = <span class="stres">-2368.2142</span>  
Iteration 1:   log likelihood = <span class="stres">-2368.2062</span>  
Iteration 2:   log likelihood = <span class="stres">-2368.2062</span>  

Probit model with endogenous regressors         Number of obs     = <span class="stres">       500</span>
                                                Wald chi2(<span class="stres">3</span>)      = <span class="stres">    163.88</span>
Log likelihood = <span class="stres">-2368.2062</span>                     Prob &gt; chi2       = <span class="stres">    0.0000</span>

----------------------------------------------------------------------------------------------
                             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-----------------------------+----------------------------------------------------------------
                   other_inc |<span class="stres">  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485</span>
                    fem_educ |<span class="stres">    .211111   .0268648     7.86   0.000     .1584569    .2637651</span>
                        kids |<span class="stres">  -.1820929   .0478267    -3.81   0.000    -.2758315   -.0883542</span>
                       _cons |<span class="stres">   .3672086   .4480724     0.82   0.412    -.5109971    1.245414</span>
-----------------------------+----------------------------------------------------------------
 corr(e.other_inc,e.fem_work)|<span class="stres">   .3720375   .1300518                      .0946562    .5958136</span>
              sd(e.other_inc)|<span class="stres">   16.66621   .5270318                      15.66461    17.73186</span>
----------------------------------------------------------------------------------------------
Instrumented:  other_inc
Instruments:   fem_educ kids male_educ
----------------------------------------------------------------------------------------------
Wald test of exogeneity (corr = 0): chi2(<span class="stres">1</span>) = <span class="stres">6.70</span>        Prob &gt; chi2 = <span class="stres">0.0096</span>

<span class="stinp">. est sto ivp</span>

<span class="stinp">. <span class="stcmt">*my ivprobit two-step</span></span>
<span class="stinp">. ml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , <span class="stcmt">///</span>
&gt;                     technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog</span>

<span class="stinp">. adde local predict myivprobit_p                 </span>

<span class="stinp">. est sto myivp                   </span>

<span class="stinp">. <span class="stcmt">*with rescaled coefficients:</span></span>
<span class="stinp">. nlcom   (other_inc: _b[other_inc]/sqrt(1+_b[theta:_cons]^2)) <span class="stcmt">///</span>
&gt;                 (fem_educ: _b[fem_educ]/sqrt(1+_b[theta:_cons]^2)) <span class="stcmt">///</span>
&gt;                 (kids: _b[kids]/sqrt(1+_b[theta:_cons]^2)) <span class="stcmt">///</span>
&gt;                 (cons: _b[_cons]/sqrt(1+_b[theta:_cons]^2)) </span>

   other_inc:  <span class="stres">_b[other_inc]/sqrt(1+_b[theta:_cons]^2)</span>
<span class="stres">    </span>fem_educ:  <span class="stres">_b[fem_educ]/sqrt(1+_b[theta:_cons]^2)</span>
<span class="stres">        </span>kids:  <span class="stres">_b[kids]/sqrt(1+_b[theta:_cons]^2)</span>
<span class="stres">        </span>cons:  <span class="stres">_b[_cons]/sqrt(1+_b[theta:_cons]^2)</span>

------------------------------------------------------------------------------
             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   other_inc |<span class="stres">  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485</span>
    fem_educ |<span class="stres">    .211111   .0268648     7.86   0.000     .1584569    .2637651</span>
        kids |<span class="stres">  -.1820929   .0478267    -3.81   0.000    -.2758316   -.0883543</span>
        cons |<span class="stres">   .3672086   .4480724     0.82   0.412    -.5109971    1.245414</span>
------------------------------------------------------------------------------
</samp></pre>
Where the results are exactly the same.

<h2>A Story of marginal effects </h2>

<p>
Let me now walk you though the Story of marginal effects with ivprobit. 
<br></br>
Back in Stata 13, marginal effects for IV probit were estimated using the structural equation coeffients:
$$ P(y_1=1| z_1 , y_2) = \Phi \left( z_1 \beta_1 + y_2 \beta_2  \right) $$
So that marginal effects were defined as:
$$\frac{\partial P(y_1=1|.)}{\partial z_1} = \phi ( z_1 \beta_1 + y_2 \beta_2  ) \beta_1 $$
$$\frac{\partial P(y_1=1|.)}{\partial y_2} = \phi ( z_1 \beta_1 + y_2 \beta_2  ) \beta_2 $$
While I currently do not have access to Stata13, 
this is what it would produce under version control from Stata 14.2:
</p>
<pre id="stlog-one" class="stlog"><samp><span class="stinp">. <span class="stcmt">//margins, dydx(*) predict(pr)</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//Average marginal effects                        Number of obs     =        500</span></span>
<span class="stinp">. <span class="stcmt">//Model VCE    : OIM</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//Expression   : Prob of positive outcome when rho=0, predict(pr)</span></span>
<span class="stinp">. <span class="stcmt">//dy/dx w.r.t. : other_inc fem_educ kids male_educ</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//------------------------------------------------------------------------------</span></span>
<span class="stinp">. <span class="stcmt">//             |            Delta-method</span></span>
<span class="stinp">. <span class="stcmt">//             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]</span></span>
<span class="stinp">. <span class="stcmt">//-------------+----------------------------------------------------------------</span></span>
<span class="stinp">. <span class="stcmt">//   other_inc |   -.014015   .0009836   -14.25   0.000    -.0159428   -.0120872</span></span>
<span class="stinp">. <span class="stcmt">//    fem_educ |   .0545129   .0066007     8.26   0.000     .0415758      .06745</span></span>
<span class="stinp">. <span class="stcmt">//        kids |  -.0470199   .0123397    -3.81   0.000    -.0712052   -.0228346</span></span>
<span class="stinp">. <span class="stcmt">//   male_educ |          0  (omitted)</span></span>
<span class="stinp">. <span class="stcmt">//------------------------------------------------------------------------------</span></span>
</samp></pre>
<p>This marginal effects are emulated using pr2 after myivprobit:</p>
<pre id="stlog-6" class="stlog"><samp><span class="stinp">. est restore myivp</span>
(results myivp are active now)

<span class="stinp">. margins, dydx(*) predict(pr2) force</span>
(note: prediction is a function of possibly stochastic quantities other than <span class="stres">e(b)</span>)

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
Model VCE    : <span class="stres">OIM</span>

Expression   : <span class="stres">P(y=1|X) FIML, predict(pr2)</span>
dy/dx w.r.t. : <span class="stres">fem_educ kids other_inc male_educ</span>

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .0545129   .0066003     8.26   0.000     .0415766    .0674493</span>
        kids |<span class="stres">  -.0470199   .0123394    -3.81   0.000    -.0712047   -.0228351</span>
   other_inc |<span class="stres">   -.014015   .0009837   -14.25   0.000    -.0159431   -.0120869</span>
   male_educ |<span class="stres">          0</span>  (omitted)
------------------------------------------------------------------------------
</samp></pre>
<p>You will see that the above command also includes "male_educ" in the list of exogenous variables, because 
this is an explanatory variable for at least one equation in the model. However, this not being inlcuded in the
structural equation, has a no effect on \(P(y=1|x) \). 
<br><br>
When we reached Stata 14.1. A change was introduced in how probabilities were calculated after ivprobit. 
As it says in the "whatsnew" material, the new formulation would take into account endogeneity.
<br><br>
Specifically, they use what I call the 2sls predicted probabilities, following equation (5):
	$$P(y^**1 >0|z_1,y_2,u_2) =\Phi \left( 
	z_1 \frac{\beta_1}{\sqrt{1-\rho^2}} + 
	y_2 \frac{\beta_2}{\sqrt{1-\rho^2}} + 
	\frac{\rho}{\sqrt{1-\rho^2}} \frac{u_2}{\sigma_2}
	\right ) \quad (11)
	$$
With the caveat \( u_2 \) was substituted by \( y_2 - z_1 \delta_1 - z_2 \):
	$$P(y^{**}_1 >0|z_1,y_2)= P(y_1=1|z_1,y_2,u_2) =\Phi \left( 
	z_1 \frac{\beta_1}{\sqrt{1-\rho^2}} + 
	y_2 \frac{\beta_2}{\sqrt{1-\rho^2}} + 
	\frac{\rho}{\sqrt{1-\rho^2}} \frac{y_2 - z_1 \delta_1 - z_2 \delta_2}{\sigma_2}
	\right ) \quad (12)
	$$	
While the two equations above are basically the same, they have important differences when marginal 
effects	are estimated by software. 
<br><br>
So let me explain first what Stata 14.1, and 15 did.
<br><br>
To estimate marginal effects, partial derivatives were based on equation 12:
$$ \frac{\partial P(y=1|.)}{\partial z_1} = 
  \phi(.)*\left( \frac{\beta_1}{\sqrt{1-\rho^2}}  
  -\frac{\rho}{\sqrt{1-\rho^2}} * \frac{\delta_1}{\sigma_2} \right)  $$
$$ \frac{\partial P(y=1|.)}{\partial z_2} =
	\phi(.)*\left( 0
  -\frac{\rho}{\sqrt{1-\rho^2}} * \frac{\delta_2}{\sigma_2} \right)  $$
$$ \frac{\partial P(y=1|.)}{\partial y_2} =
	\phi(.)*\left( \frac{\beta_2}{\sqrt{1-\rho^2}}  
	+\frac{1}{\sqrt{1-\rho^2}} * \frac{1}{\sigma_2} \right)  $$
So, from the technical point of view, this partial derivatives are correct, since they are capturing the direct, 
and indirect partial effects. A kind of total derivative, rather than partial derivatives. 	
<br><br>
The problem, however, is that this assumes we could actually observe how the unobserved component \( u_2 \) changes when other variables
change. Standard regression analysis, however, would say that this unobserved components should be considered as fixed, and instead one should
estimate marginal effects averaging over the unobserved factors. Thus, the second term on each on of the above derivatives, should be zero.
<br><br>
Nevertheless, lets replicate this. First, if you are using Stata 16, the following behaivor may no longer be replicable:
</p>
<pre id="stlog-two" class="stlog"><samp><span class="stinp">. <span class="stcmt">//. margins, dydx(*) predict(pr)</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//Average marginal effects                        Number of obs     =        500</span></span>
<span class="stinp">. <span class="stcmt">//Model VCE    : OIM</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//Expression   : Probability of positive outcome, predict(pr)</span></span>
<span class="stinp">. <span class="stcmt">//dy/dx w.r.t. : other_inc fem_educ kids male_educ</span></span>
<span class="stinp">. <span class="stcmt">//</span></span>
<span class="stinp">. <span class="stcmt">//------------------------------------------------------------------------------</span></span>
<span class="stinp">. <span class="stcmt">//             |            Delta-method</span></span>
<span class="stinp">. <span class="stcmt">//             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]</span></span>
<span class="stinp">. <span class="stcmt">//-------------+----------------------------------------------------------------</span></span>
<span class="stinp">. <span class="stcmt">//   other_inc |  -.0097802   .0014994    -6.52   0.000     -.012719   -.0068414</span></span>
<span class="stinp">. <span class="stcmt">//    fem_educ |   .0623273    .007099     8.78   0.000     .0484135     .076241</span></span>
<span class="stinp">. <span class="stcmt">//        kids |  -.0614265   .0139446    -4.41   0.000    -.0887574   -.0340956</span></span>
<span class="stinp">. <span class="stcmt">//   male_educ |  -.0194406   .0022103    -8.80   0.000    -.0237728   -.0151084</span></span>
<span class="stinp">. <span class="stcmt">//------------------------------------------------------------------------------</span></span>
</samp></pre>
<p>
If I would like to replicate this using myivprobit, I would estimate marginal effects using option "pr1", 
and requesting derivates to be estimated without the "chain" option. This makes sure that one takes into account
the effect of all changes in \( y_2 , z_1 \) on the outcome \( P(y=1|.) \). Not using  this option would ignore, 
for example, the effect of \(y_2\) that comes through the error \(u_2\). (again, assuming this derivatives are correct).
</p>
<pre id="stlog-7" class="stlog"><samp><span class="stinp">. est restore myivp</span>
(results myivp are active now)

<span class="stinp">. margins, dydx(*) predict(pr1) force nochain</span>
(note: prediction is a function of possibly stochastic quantities other than <span class="stres">e(b)</span>)

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
Model VCE    : <span class="stres">OIM</span>

Expression   : <span class="stres">P(y=1|X) two-step, predict(pr1)</span>
dy/dx w.r.t. : <span class="stres">fem_educ kids other_inc male_educ</span>

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .0623273   .0060316    10.33   0.000     .0505055     .074149</span>
        kids |<span class="stres">  -.0614265   .0128383    -4.78   0.000    -.0865891   -.0362639</span>
   other_inc |<span class="stres">  -.0097802   .0009828    -9.95   0.000    -.0117065   -.0078539</span>
   male_educ |<span class="stres">  -.0194406   .0074886    -2.60   0.009    -.0341181   -.0047631</span>
------------------------------------------------------------------------------
</samp></pre>
So what do we see here? First, that the Stata 14, 15 and early 16 versions were estimating a partial effect
that inadvertedly accounted for a second order effect (through the first stage regression). This would normally have a small
effect on the exogenous variables, but will have a noticible effect on the endogenous variable of interest.
<br><br>
Some people would even report negative marginal effects even when the estimated coefficient was possitive.
<br><br>
We also see the odd outcome that the instrument, here male_educ, would also appear in the output, capturing only a second order effect
on the outcome of interest.
<br><br>
Just as a side note, you could also try estimating this without the "nochain" option. If you do that, you will find
that effects for the endogenous variable "improve". This will happen because the second order effect would not be estimated for the endogenous variable, but 
only for the exogenous variables.
So finally we arrive to the corrected marginal effects for Stata version 16. 
<br><br>
After a very interesting discussion in Statalist, where Prof Wooldridge intervene, it was said that the way Stata (and margins) was estimating 
marginal effects was incorrect. As I show here, that was the case because partial derivatives were being estimated through the first and second equations.
<br><br>
What Prof. Wooldridge suggested was to take a step back, and estimate marginal effects using a manual two-step approach, obtaining 
standard errors via bootstrap. This implies using equation (11) to estimate the partial effects.
<br><br>
How does this make a difference? 
<br> This makes a difference because we will be making the explicit assumption that \( \hat{u}_2 \) does not change when \( y_2, z_1 \quad or \quad z_2 \) changes.
This will modify the partial effects to the following:
$$ \frac{\partial P(y=1|.)}{\partial z_1} = 
  \phi(.)*\left( \frac{\beta_1}{\sqrt{1-\rho^2}} \right)  $$
$$ \frac{\partial P(y=1|.)}{\partial z_2} =
	\phi(.)* 0 $$
$$ \frac{\partial P(y=1|.)}{\partial y_2} =
	\phi(.)* \left( \frac{\beta_2}{\sqrt{1-\rho^2}}  \right)  $$
The main differences with the "structural" marginal effects are that the evaluation of 
\( \phi(.) \) includes the values predicted values of the errors; and the coefficients used correspond to the two-step 
procedure ones (rescaled)

To show empirically how this works, we can compare the builtin command, with the "two-step" procedure suggested by Prof. Wooldridge:
<pre id="stlog-8" class="stlog"><samp><span class="stinp">. <span class="stcmt">* two step procedure</span></span>
<span class="stinp">. reg $y2 $z1 $z2</span>

      Source |       SS           df       MS      Number of obs   =<span class="stres">       500</span>
-------------+----------------------------------   F(3, 496)       = <span class="stres">    34.36</span>
       Model | <span class="stres"> 28864.2732         3  9621.42439   </span>Prob &gt; F        =<span class="stres">    0.0000</span>
    Residual | <span class="stres"> 138881.269       496  280.002558   </span>R-squared       =<span class="stres">    0.1721</span>
-------------+----------------------------------   Adj R-squared   =<span class="stres">    0.1671</span>
       Total | <span class="stres"> 167745.542       499  336.163411   </span>Root MSE        =   <span class="stres"> 16.733</span>

------------------------------------------------------------------------------
   other_inc |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .3351866   .2837344     1.18   0.238    -.2222829    .8926562</span>
        kids |<span class="stres">   .8329056   .5497701     1.52   0.130    -.2472597    1.913071</span>
   male_educ |<span class="stres">   2.845253   .2838838    10.02   0.000      2.28749    3.403016</span>
       _cons |<span class="stres">   9.872562   5.049432     1.96   0.051    -.0483506    19.79347</span>
------------------------------------------------------------------------------

<span class="stinp">. predict double u2, resid</span>

<span class="stinp">. probit $y1 $z1 $y2 u2</span>

Iteration 0:   log likelihood = <span class="stres">-344.63508</span>  
Iteration 1:   log likelihood = <span class="stres">-252.10819</span>  
Iteration 2:   log likelihood = <span class="stres">-252.04529</span>  
Iteration 3:   log likelihood = <span class="stres">-252.04529</span>  

Probit regression                               Number of obs     = <span class="stres">       500</span>
                                                LR chi2(<span class="stres">4</span>)        = <span class="stres">    185.18</span>
                                                Prob &gt; chi2       = <span class="stres">    0.0000</span>
Log likelihood = <span class="stres">-252.04529</span>                     Pseudo R2         = <span class="stres">    0.2687</span>

------------------------------------------------------------------------------
    fem_work |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">    .227437   .0273171     8.33   0.000     .1738964    .2809775</span>
        kids |<span class="stres">  -.1961748   .0478084    -4.10   0.000    -.2898776    -.102472</span>
   other_inc |<span class="stres">   -.058473   .0090228    -6.48   0.000    -.0761573   -.0407887</span>
          u2 |<span class="stres">   .0240492   .0094295     2.55   0.011     .0055677    .0425306</span>
       _cons |<span class="stres">   .3956061   .4784993     0.83   0.408    -.5422352    1.333448</span>
------------------------------------------------------------------------------

<span class="stinp">. margins, dydx(*) predict(pr)</span>

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
Model VCE    : <span class="stres">OIM</span>

Expression   : <span class="stres">Pr(fem_work), predict(pr)</span>
dy/dx w.r.t. : <span class="stres">fem_educ kids other_inc u2</span>

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .0646175   .0060271    10.72   0.000     .0528045    .0764304</span>
        kids |<span class="stres">  -.0557355   .0129302    -4.31   0.000    -.0810782   -.0303929</span>
   other_inc |<span class="stres">  -.0166128   .0022499    -7.38   0.000    -.0210225   -.0122032</span>
          u2 |<span class="stres">   .0068326    .002632     2.60   0.009     .0016741    .0119912</span>
------------------------------------------------------------------------------
</samp></pre>
Standard errors would not be corrrect, but bootstrap could be applied to obtain corrected standard errors.
<pre id="stlog-9" class="stlog"><samp><span class="stinp">. <span class="stcmt">** built-in command</span></span>
<span class="stinp">. </span>
<span class="stinp">. qui:ivprobit $y1  $z1 ($y2 = $z2), ml</span>

<span class="stinp">. margins, dydx(*) predict(pr)</span>

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
Model VCE    : <span class="stres">OIM</span>

Expression   : <span class="stres">Average structural function probabilities, predict(pr)</span>
dy/dx w.r.t. : <span class="stres">other_inc fem_educ kids</span>

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   other_inc |<span class="stres">  -.0166128   .0012889   -12.89   0.000     -.019139   -.0140867</span>
    fem_educ |<span class="stres">   .0646175   .0073529     8.79   0.000      .050206     .079029</span>
        kids |<span class="stres">  -.0557355   .0144233    -3.86   0.000    -.0840047   -.0274664</span>
------------------------------------------------------------------------------
</samp></pre>
<p>So you can see that the two step approach, and the built-in approach
now provide the same marginal effects. And since the official command 
estimates all coefficients simultaneously, the standard errors can be taken as correct...Let me get back to this point later.
<br><br>
So how can we correct for this with our predict program. Since there is nothing
to prevent margins to obtain numerical derivative across both equations, we need to 
modify slighly how the model is estimated. First, we create clone copies of all 
variables that enter the second stage: z_1 and y_2, and use them for the model estimation:
</p>
<pre id="stlog-10" class="stlog"><samp><span class="stinp">. clonevar c_other_inc = other_inc</span>

<span class="stinp">. clonevar c_fem_educ  = fem_educ</span>

<span class="stinp">. clonevar c_kids      = kids</span>

<span class="stinp">. global  y2b c_other_inc</span>

<span class="stinp">. global  z1b c_fem_educ c_kids </span>

<span class="stinp">. </span>
<span class="stinp">. ml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2b = $z1b $z2  ) (lnsigma:) , <span class="stcmt">///</span>
&gt;                     technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog</span>

<span class="stinp">. adde local predict myivprobit_p                 </span>

<span class="stinp">. est sto myivp   </span>
</samp></pre>
<p> The idea of using "clones" of the exogenous variables \( z_1 \) and endogenous \( y_2 \)
is to have access to the same information as the original data, 
but making sure \( \hat{u}_2 \) does not change when the original data changes. 
<br><br>
Marginal effects can be calculated a I did before, except that I now make it explicit to request marginal effects 
with respect to \(z_1 \quad \&  \quad y_2 \) only.
<pre id="stlog-11" class="stlog"><samp><span class="stinp">. est restore myivp       </span>
(results myivp are active now)

<span class="stinp">. margins, dydx($z1 $y2) predict(pr1) force </span>

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
Model VCE    : <span class="stres">OIM</span>

Expression   : <span class="stres">P(y=1|X) two-step, predict(pr1)</span>
dy/dx w.r.t. : <span class="stres">fem_educ kids other_inc</span>

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .0646175    .006331    10.21   0.000     .0522089    .0770261</span>
        kids |<span class="stres">  -.0557355   .0134693    -4.14   0.000    -.0821349   -.0293362</span>
   other_inc |<span class="stres">  -.0166128   .0023499    -7.07   0.000    -.0212185   -.0120072</span>
------------------------------------------------------------------------------
</samp></pre>
<p>
And done. we have been able to reproduced the second version, two-step, marginal effects
for the instrumental variable probit model, that follows the two-step approach advocated by Prof. Wooldridge.
Furthermore, this also reproduces what ivprobit currently does!
<br><br>
There is only one last perky detail. If you look at the marginal effect 
standard errors I produce with the myivprobit command, and compare it with the marginal effects
the ivprobit command produces, you will notice they are different. 
<br><br>
Im still uncertain why this may be happening, and Im currently discussing with people at Stata about the "correct"
way to obtain standard errors, but here are the clift notes:
</p>
<p>
 Recall that IVprobit uses the following formula to estimate marginal effects with respect to \(y_2 \) is: </li>
$$ \frac{\partial P(y=1|.)}{\partial y_2} =
	\phi \left(z_1 \frac{\beta_1}{\sqrt{1-\rho^2}} + 
	y_2 \frac{\beta_2}{\sqrt{1-\rho^2}} + 
	\frac{\rho}{\sqrt{1-\rho^2}} \frac{\hat{u}_2}{\sigma_2} \right)* \left( \frac{\beta_2}{\sqrt{1-\rho^2}}  \right)  $$
My own version of predict uses the same formula, but modified for the two-step approach. 
<br>
If one needs to estimate standard errors for this partial effects, it is necessary to account for the uncertainty of all 
estimated coefficients in the model, including the uncertainly of the errors \( \hat{u}_2 \).
Unofficial words (still under discussion)  indictates that the current formulation assumes \( \rho \) and \( \sigma_2 \) to be constant, 
when standard errors are calculated. 
<br><br>
While this may seem incorrect, I understand the intuition behind this idea. 
<br>
If you recall the estimation of marginal effects from the structural equation:
$$\frac{\partial P(y_1=1|.)}{\partial y_2} = \phi ( z_1 \beta_1 + y_2 \beta_2  ) \beta_2 $$
you will see that this are not affected by \( \rho \) or \( \sigma_2 \). Perhaps this is one of the reasons why the currently
estimated marginal effects standard errors are so similar to the ones based on the "old" structural marginal effects.  
<br><br>
My own command, however, accounts for the uncertainty in \( \rho \) and \( \sigma_2 \). This also seems correct 
since two-step marginal procedures are expected to be less efficient than the Fullinformation counterparts. 
<br> Of course, if you prefer to have a tie-breaker on which one is correct, I can use a Bootstrap procedure to produce 
the elusive standard errors. 
<br> Basically, I'll use the manual two-step procedure, alomg with a 250 bootstrap repetitions, to report the results:
</p>
<pre id="stlog-12" class="stlog"><samp><span class="stinp">. program bs_ivprobit, eclass</span>
  1<span class="stinp">.         reg $y2 $z1 $z2</span>
  2<span class="stinp">.         capture drop u2</span>
  3<span class="stinp">.         predict double u2, resid</span>
  4<span class="stinp">.         probit $y1 $z1 $y2 u2</span>
  5<span class="stinp">.         margins, dydx(*) predict(pr) nose post</span>
  6<span class="stinp">. end</span>

<span class="stinp">. bootstrap , reps(250) seed(1) nodots:bs_ivprobit</span>

Average marginal effects                        Number of obs     = <span class="stres">       500</span>
                                                Replications      = <span class="stres">       250</span>

------------------------------------------------------------------------------
             |   Observed   Bootstrap                         Normal-based
             |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    fem_educ |<span class="stres">   .0646175   .0058983    10.96   0.000      .053057     .076178</span>
        kids |<span class="stres">  -.0557355   .0138358    -4.03   0.000    -.0828532   -.0286179</span>
   other_inc |<span class="stres">  -.0166128   .0023778    -6.99   0.000    -.0212732   -.0119525</span>
          u2 |<span class="stres">   .0068326   .0027569     2.48   0.013     .0014292    .0122361</span>
------------------------------------------------------------------------------
</samp></pre>
<p>
In this case, it seems that the bootstrap estimates seem to favor my version of marginal effects and standard errors.
However, you are free to take this as limited evidence, and use either strategy as your own.
</p>
<h2> Conclusion </h2>
<p> The command -ml- is a powerful tool that can be used to estimate single or multiple equation models, as long as
the loglikelihood functions (and their inter-relations) can be properly defined.
<br><br>
-margins- is also a very flexible command that can be easily combined with -ml- to expand 
the estimation marginal effects for properly defined outcomes. While the command is flexible, and relatively easy to use, 
these properties can also be double+edge swords, if one is not aware of the mechanics behind the actual estimation of partial effects.
<br><br>
In my view, the original estimation of marginal effects after iv-probit was correct, but the changes
it recieved in Stata 14.1 introduced what we could call a bug, that was based on solid Math. However, unless you dig dipper into what
ivprobit tries to estimate, it would be difficult to say why that change produced undesirable results.
<br><br>
While the last update on Stata 16 has made a correction that now produces correct partial effects (two-step like), 
I still believe there is a remaining bug in the program that may be affecting the estimation of standard errors. 
And of course, margins could also go back to the beggining, and provide users the option for the estimation of MLE-structural marginal effects.
<br><br>
Who knows, this may be going to the Statalist wishlist....
</p>
</body>
</html>
